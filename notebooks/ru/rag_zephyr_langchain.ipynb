{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kih21u1tyr-I"
   },
   "source": [
    "# Простой RAG для проблем GitHub с использованием Hugging Face Zephyr и LangChain\n",
    "\n",
    "_Автор: [Мария Халусова](https://github.com/MKhalusova)_\n",
    "\n",
    "Этот блокнот демонстрирует, как можно быстро создать RAG (Retrieval Augmented Generation) для проблем (issues) проекта на GitHub, используя модель [`HuggingFaceH4/zephyr-7b-beta`](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), и LangChain.\n",
    "\n",
    "\n",
    "**Что такое RAG?**\n",
    "\n",
    "RAG - это популярный подход к решению проблемы, связанной с тем, что мощная LLM не знает о конкретном контенте, поскольку его нет в ее обучающих данных, или галлюцинирует, даже если видела его ранее. Такой специфический контент может быть проприетарным, конфиденциальным или, как в данном примере, недавно появившимся и часто обновляемым.\n",
    "\n",
    "Если ваши данные статичны и не меняются регулярно, вы можете рассмотреть возможность дообучения большой модели. Однако во многих случаях дообучение может быть дорогостоящим, а при многократном повторении (например, для устранения дрейфа данных (address data drift) приводить к \"сдвигу модели (model shift)\". Это когда поведение модели изменяется нежелательным образом.\n",
    "\n",
    "**RAG (Retrieval Augmented Generation, генерация с расширенным извлечением информации)** не требует дообучения модели. Вместо этого RAG работает, предоставляя LLM дополнительный контекст, который извлекается из соответствующих данных, чтобы она могла генерировать более обоснованный ответ.\n",
    "\n",
    "Вот небольшая иллюстрация:\n",
    "\n",
    "![RAG диаграмма](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/rag-diagram.png)\n",
    "\n",
    "* Внешние данные преобразуются в эмбеддинг векторы с помощью отдельной модели эмбеддингов, сами векторы хранятся в базе данных. Модели эмбеддинга обычно невелики, поэтому регулярное обновление эмбеддинг векторов происходит быстрее, дешевле и проще, чем дообучение модели.\n",
    "\n",
    "* В то же время тот факт, что дообучение не требуется, дает вам возможность поменять вашу LLM на более мощную, когда она появится, или перейти на более компактную дистиллированную версию, если вам понадобится более быстрый инференс.\n",
    "\n",
    "Давайте проиллюстрируем создание RAG с помощью LLM с открытым исходным кодом, модели эмбеддинга и LangChain.\n",
    "\n",
    "Сначала установите необходимые зависимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC9frDOlyi38"
   },
   "outputs": [],
   "source": [
    "! pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-aYENQwZ-p_c"
   },
   "outputs": [],
   "source": [
    "# Если вы используете Google Colab, вам может понадобиться запустить эту ячейку, чтобы убедиться, что вы используете UTF-8 для установки LangChain\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5HhMZ2c-NfU"
   },
   "outputs": [],
   "source": [
    "! pip install -q langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8po01vMWzXL"
   },
   "source": [
    "## Подготовка данных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cCmQywC04x6"
   },
   "source": [
    "В этом примере мы загрузим все проблемы (issues) (как открытые, так и закрытые) из [репозитория библиотеки PEFT](https://github.com/huggingface/peft).\n",
    "\n",
    "Во-первых, вам необходимо получить [персональный токен доступа GitHub](https://github.com/settings/tokens?type=beta) чтобы получить доступ к API GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MoD7NbsNjlM"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "ACCESS_TOKEN = getpass(\"YOUR_GITHUB_PERSONAL_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fccecm3a10N6"
   },
   "source": [
    "Далее мы загрузим все проблемы (issues) из репозитория [huggingface/peft](https://github.com/huggingface/peft):\n",
    "- По умолчанию предложения об изменении кода (pull requests) также считаются проблемами, но здесь мы решили исключить их из данных, установив `include_prs=False`.\n",
    "- Задание `state = \"all\"` означает, что мы будем загружать как открытые, так и закрытые проблемы (issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8EKMit4WNDY8"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"huggingface/peft\",\n",
    "    access_token=ACCESS_TOKEN,\n",
    "    include_prs=False,\n",
    "    state=\"all\"\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CChTrY-k2qO5"
   },
   "source": [
    "Содержание отдельных проблем (issues) на GitHub может быть длиннее, чем то, что модель эмбеддингов может принять в качестве входных данных. Если мы хотим использовать весь доступный контент, нам нужно разбить документы на фрагменты (chunk) соответствующего размера.\n",
    "\n",
    "Наиболее распространенный и простой подход к фрагментации заключается в определении фиксированного размера фрагментов (chunk) и того, должно ли между ними быть какое-либо перекрытие. Сохранение некоторого перекрытия между фрагментами позволяет нам сохранить некоторый семантический контекст между фрагментами. Рекомендуемый сплиттер для текстов общего содержания - [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter), и именно его мы будем использовать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmsXOf59Pmm-"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAt_zPVlXOn7"
   },
   "source": [
    "## Создание эмбеддингов + retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mvat6JQl4yp"
   },
   "source": [
    "Теперь, когда все документы имеют подходящий размер, мы можем создать базу данных с их эмбеддингами.\n",
    "\n",
    "Для создания фрагментов эмбеддингов документов мы будем использовать модель эмбеддингов `HuggingFaceEmbeddings` и [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5). Есть много других моделей эмбеддингов, доступных на Hub, и вы можете отслеживать самые эффективные из них, проверяя [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "\n",
    "Для создания базы векторов мы воспользуемся библиотекой `FAISS`, разработанной Facebook AI. Эта библиотека обеспечивает эффективный поиск сходства и кластеризацию плотных векторов (dense vectors), что нам и нужно. В настоящее время FAISS является одной из наиболее используемых библиотек для NN поиска в массивных наборах данных.\n",
    "\n",
    "Мы получим доступ к модели эмбеддингов и FAISS через LangChain API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixmCdRzBQ5gu"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "db = FAISS.from_documents(chunked_docs,\n",
    "                          HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iCgEPi0nnN6"
   },
   "source": [
    "Нам нужен способ возврата (retrieve) документов по неструктурированному запросу. Для этого мы воспользуемся методом `as_retriever`, используя `db` в качестве основы:\n",
    "- `search_type=\"similarity\"` означает, что мы хотим выполнить поиск по сходству (similarity) между запросом и документами\n",
    "- `search_kwargs={'k': 4}` указывает retriever возвращать 4 лучших результата.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mBTreCQ9noHK"
   },
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgEhlISJpTgj"
   },
   "source": [
    "Векторная база данных и retriever настроены, осталось настроить следующий элемент цепочки - модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzQxx0HkXVFU"
   },
   "source": [
    "## Загрузка квантизованной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jy1cC65p_GD"
   },
   "source": [
    "Для этого примера мы выбрали [`HuggingFaceH4/zephyr-7b-beta`](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), небольшую, но эффективную модель.\n",
    "\n",
    "Поскольку каждую неделю выходит множество моделей, вы можете захотеть заменить эту модель на самую последнюю и лучшую. Лучший способ отслеживать LLM с открытым исходным кодом - следить за [таблицей лидеров Open-source LLM](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "\n",
    "Чтобы ускорить инференс, мы загрузим квантизованную версию модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-ggaa763VRo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVNRJALyXYHG"
   },
   "source": [
    "## Создание цепочки LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUUNneJ1smhl"
   },
   "source": [
    "Наконец, у нас есть все, что нужно для создания цепочки LLM.\n",
    "\n",
    "Во-первых, создадим конвейер генерации текста (text_generation) используя загруженную модель и ее токенизатор.\n",
    "\n",
    "Затем создадим шаблон подсказки (prompt) - он должен соответствовать формату модели, поэтому, если вы заменяете контрольную точку модели, убедитесь, что используете соответствующее форматирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cR0k1cRWz8Pm"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Answer the question based on your knowledge. Use the following context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "</s>\n",
    "<|user|>\n",
    "{question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l19UKq5HXfSp"
   },
   "source": [
    "Примечание: _Вы также можете использовать `tokenizer.apply_chat_template` для преобразования списка сообщений (в виде dicts: `{'role': 'user', 'content': '(...)'}`) в строку с соответствующим форматом чата._\n",
    "\n",
    "\n",
    "Наконец, нам нужно объединить `llm_chain` с retriever, чтобы создать RAG-цепочку. На последнем этапе генерации мы передаем оригинальный вопрос, а также извлеченные контекстные документы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_rI3YNp9Xl4s"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsCOhfDDXpaS"
   },
   "source": [
    "## Сравним результаты\n",
    "\n",
    "Давайте посмотрим, как RAG влияет на генерирование ответов на специфические для библиотеки вопросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "W7F07fQLXusU"
   },
   "outputs": [],
   "source": [
    "question = \"How do you combine multiple adapters?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC0rJYU1x1ir"
   },
   "source": [
    "Сначала посмотрим, какой ответ мы можем получить, используя только саму модель, без добавления контекста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "GYh-HG1l0De5",
    "outputId": "277d8e89-ce9b-4e04-c11b-639ad2645759"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" To combine multiple adapters, you need to ensure that they are compatible with each other and the devices you want to connect. Here's how you can do it:\\n\\n1. Identify the adapters you need: Determine which adapters you require to connect the devices you want to use together. For example, if you want to connect a USB-C device to an HDMI monitor, you may need a USB-C to HDMI adapter and a USB-C to USB-A adapter (if your computer only has USB-A ports).\\n\\n2. Connect the first adapter: Plug in the first adapter into the device you want to connect. For instance, if you're connecting a USB-C laptop to an HDMI monitor, plug the USB-C to HDMI adapter into the laptop's USB-C port.\\n\\n3. Connect the second adapter: Next, connect the second adapter to the first one. In this case, connect the USB-C to USB-A adapter to the USB-C port of the USB-C to HDMI adapter.\\n\\n4. Connect the final device: Finally, connect the device you want to use to the second adapter. For example, connect the HDMI cable from the monitor to the HDMI port on the USB-C to HDMI adapter.\\n\\n5. Test the connection: Turn on both devices and check whether everything is working correctly. If necessary, adjust the settings on your devices to ensure optimal performance.\\n\\nBy combining multiple adapters, you can connect a variety of devices together, even if they don't have the same type of connector. Just be sure to choose adapters that are compatible with all the devices you want to connect and test the connection thoroughly before relying on it for critical tasks.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"context\":\"\", \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-TIWr3wx9w8"
   },
   "source": [
    "Как видите, модель интерпретировала вопрос как вопрос о физических компьютерных адаптерах, тогда как в контексте PEFT под \"адаптерами\" подразумеваются адаптеры LoRA.\n",
    "Посмотрим, поможет ли добавление контекста из проблем (issues) GitHub дать более релевантный ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "FZpNA3o10H10",
    "outputId": "31f9aed3-3dd7-4ff8-d1a8-866794fefe80"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Based on the provided context, it seems that combining multiple adapters is still an open question in the community. Here are some possibilities:\\n\\n  1. Save the output from the base model and pass it to each adapter separately, as described in the first context snippet. This allows you to run multiple adapters simultaneously and reuse the output from the base model. However, this approach requires loading and running each adapter separately.\\n\\n  2. Export everything into a single PyTorch model, as suggested in the second context snippet. This would involve saving all the adapters and their weights into a single model, potentially making it larger and more complex. The advantage of this approach is that it would allow you to run all the adapters simultaneously without having to load and run them separately.\\n\\n  3. Merge multiple Lora adapters, as mentioned in the third context snippet. This involves adding multiple distinct, independent behaviors to a base model by merging multiple Lora adapters. It's not clear from the context how this would be done, but it suggests that there might be a recommended way of doing it.\\n\\n  4. Combine adapters through a specific architecture, as proposed in the fourth context snippet. This involves merging multiple adapters into a single architecture, potentially creating a more complex model with multiple behaviors. Again, it's not clear from the context how this would be done.\\n\\n   Overall, combining multiple adapters is still an active area of research, and there doesn't seem to be a widely accepted solution yet. If you're interested in exploring this further, it might be worth reaching out to the Hugging Face community or checking out their documentation for more information.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZQedZKSyrwO"
   },
   "source": [
    "Как мы видим, добавление контекста действительно помогает той же самой модели дать гораздо более релевантный и обоснованный ответ на вопрос, связанный с библиотекой.\n",
    "\n",
    "Примечательно, что объединение нескольких адаптеров для инференса было добавлено в библиотеку, и эту информацию можно найти в документации, так что для следующей итерации этого RAG, возможно, стоит включить эмбеддинг документации."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
