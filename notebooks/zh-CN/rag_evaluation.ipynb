{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YErqpfH9jVI"
   },
   "source": [
    "# RAG è¯„ä¼°\n",
    "_ä½œè€… [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
    "\n",
    "æœ¬ notebook æ¼”ç¤ºäº†å¦‚ä½•è¯„ä¼°ä½ çš„ RAGï¼ˆRetrieval Augmented Generationï¼‰ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªåˆæˆè¯„ä¼°æ•°æ®é›†å¹¶ä½¿ç”¨ LLM-as-a-judge æ¥è®¡ç®—ä½ ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚\n",
    "\n",
    "å¯¹äº RAG ç³»ç»Ÿçš„ä»‹ç»ï¼Œä½ å¯ä»¥æŸ¥çœ‹[è¿™ä¸ªæŠ€æœ¯æŒ‡å—](rag_zephyr_langchain)!\n",
    "\n",
    "RAG ç³»ç»Ÿå¾ˆå¤æ‚: è¿™é‡Œæœ‰ä¸€ä¸ª RAG æµç¨‹å›¾ï¼Œæˆ‘ä»¬ç”¨è“è‰²æ ‡æ³¨äº†ç³»ç»Ÿå¢å¼ºçš„æ‰€æœ‰å¯èƒ½æ€§ï¼š\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
    "\n",
    "å®æ–½ä¸Šè¿°ä»»ä½•æ”¹è¿›éƒ½å¯èƒ½ä¼šå¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ï¼›ä½†å¦‚æœæ— æ³•ç›‘æ§å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ï¼Œé‚£ä¹ˆè¿›è¡Œä»»ä½•æ›´æ”¹éƒ½æ˜¯æ— ç”¨çš„ï¼è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è¯„ä¼°æˆ‘ä»¬çš„ RAG ç³»ç»Ÿã€‚\n",
    "\n",
    "### è¯„ä¼°RAGæ€§èƒ½\n",
    "\n",
    "ç”±äºæœ‰å¦‚æ­¤å¤šçš„éƒ¨åˆ†éœ€è¦è°ƒæ•´ï¼Œè¿™äº›éƒ¨åˆ†å¯¹æ€§èƒ½æœ‰å¾ˆå¤§å½±å“ï¼Œå› æ­¤å¯¹ RAG ç³»ç»Ÿè¿›è¡ŒåŸºå‡†æµ‹è¯•æ˜¯è‡³å…³é‡è¦çš„ã€‚\n",
    "\n",
    "å¯¹äºæˆ‘ä»¬çš„è¯„ä¼°æµæ°´çº¿ï¼Œæˆ‘ä»¬å°†éœ€è¦ï¼š\n",
    "1. ä¸€ä¸ªå¸¦æœ‰é—®é¢˜-ç­”æ¡ˆå¯¹çš„è¯„ä¼°æ•°æ®é›†ï¼ˆQA å¯¹ï¼‰\n",
    "2. ä¸€ä¸ªè¯„ä¼°å™¨ï¼Œç”¨äºè®¡ç®—æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¸Šé¢çš„è¯„ä¼°æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§ã€‚\n",
    "\n",
    "â¡ï¸ ç»“æœå‘ç°ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä½¿ç”¨ LLMs æ¥å¸®åŠ©ï¼\n",
    "1. è¯„ä¼°æ•°æ®é›†å°†ç”± LLM ğŸ¤– åˆæˆç”Ÿæˆï¼Œå¹¶ä¸”é—®é¢˜å°†ç”±å…¶ä»– LLM ğŸ¤– è¿‡æ»¤æ‰\n",
    "2. ç„¶åï¼Œ[LLM-as-a-judge](https://huggingface.co/papers/2306.05685) æ™ºèƒ½ä½“ ğŸ¤– å°†åœ¨è¿™ä¸ªåˆæˆæ•°æ®é›†ä¸Šæ‰§è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "\n",
    "__è®©æˆ‘ä»¬æ·±å…¥æŒ–æ˜å¹¶å¼€å§‹æ„å»ºæˆ‘ä»¬çš„è¯„ä¼°æµæ°´çº¿ï¼__ é¦–å…ˆï¼Œå®‰è£…æ‰€éœ€çš„æ¨¡å‹ä¾èµ–é¡¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCKBvOcp9jVK"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_lJFbYm9jVL"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIlNZ1Mn9jVL"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeW8P62J9jVM"
   },
   "source": [
    "### åŠ è½½ä½ çš„çŸ¥è¯†åŸºç¡€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRbm5tNF9jVM"
   },
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy9CKj0M9jVM"
   },
   "source": [
    "# 1. ä¸ºè¯„ä¼°æ„å»ºåˆæˆæ•°æ®é›†\n",
    "\n",
    "æˆ‘ä»¬é¦–å…ˆæ„å»ºä¸€ä¸ªé—®é¢˜å’Œç›¸å…³ä¸Šä¸‹æ–‡çš„ç»¼åˆæ•°æ®é›†ã€‚æ–¹æ³•æ˜¯å…ˆä»æˆ‘ä»¬çš„çŸ¥è¯†åº“ä¸­è·å–å…ƒç´ ï¼Œå¹¶è®© LLM æ ¹æ®è¿™äº›æ–‡æ¡£ç”Ÿæˆé—®é¢˜ã€‚\n",
    "\n",
    "ç„¶åï¼Œæˆ‘ä»¬è®¾ç½®å…¶ä»– LLM æ™ºèƒ½ä½“ä½œä¸ºç”Ÿæˆé—®ç­”å¯¹çš„è´¨ç½®è¿‡æ»¤å™¨ï¼šæ¯ä¸ªæ™ºèƒ½ä½“å°†ä½œä¸ºä¸€ä¸ªç‰¹å®šç¼ºé™·çš„è¿‡æ»¤å™¨ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkoEgiDg9jVM"
   },
   "source": [
    "### 1.1. å‡†å¤‡æºæ•°æ®æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gTOlRKO9jVM"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjrNhcCh9jVN"
   },
   "source": [
    "### 1.2. ä¸ºé—®é¢˜ç”Ÿæˆè®¾ç½®æ™ºèƒ½ä½“\n",
    "\n",
    "æˆ‘ä»¬é‡‡ç”¨ [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) ä½œä¸ºé—®ç­”å¯¹çš„ç”Ÿæˆï¼Œå› ä¸ºä»–åœ¨å„ä¸ªæ’è¡Œæ¦œä¸Šè¡¨ç°æä½³ï¼Œæ¯”å¦‚ [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoRySj3Q9jVN"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIM_DJRo9jVN"
   },
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVFc-lVy9jVN"
   },
   "source": [
    "ç°åœ¨è®©æˆ‘ä»¬ç”Ÿæˆæˆ‘ä»¬çš„é—®ç­”å¯¹ã€‚\n",
    "\n",
    "å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬åªç”Ÿæˆ 10 ä¸ªé—®ç­”å¯¹ï¼Œå¹¶ä» Hub åŠ è½½å…¶ä½™çš„ã€‚\n",
    "\n",
    "ä½†æ˜¯å¯¹äºä½ çš„ç‰¹å®šçŸ¥è¯†åº“ï¼Œè€ƒè™‘åˆ°ä½ æƒ³è¦è·å¾—è‡³å°‘çº¦ 100 ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œå¹¶ä¸”è€ƒè™‘åˆ°æˆ‘ä»¬ç¨åä¼šç”¨æˆ‘ä»¬çš„æ‰¹åˆ¤æ™ºèƒ½ä½“è¿‡æ»¤æ‰å¤§çº¦ä¸€åŠçš„æ ·æœ¬ï¼Œä½ åº”è¯¥ç”Ÿæˆæ›´å¤šçš„æ ·æœ¬ï¼Œè¶…è¿‡ 200 ä¸ªã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fteqDDD9jVN"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(\n",
    "        llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "    )\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUlOUDv59jVN",
    "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n--&gt;\\n\\n# Schedulers\\n\\nğŸ¤— Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model's output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward *n* timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be *discrete* in which case the timestep is an `int` or *continuous* in which case the timestep is a `float`.\\n\\nDepending on the context, a scheduler defines how to iteratively add noise to an image or how to update a sample based on a model's output:\\n\\n- during *training*, a scheduler adds noise (there are different algorithms for how to add noise) to a sample to train a diffusion model\\n- during *inference*, a scheduler defines how to update a sample based on a pretrained model's output\\n\\nMany schedulers are implemented from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library by [Katherine Crowson](https://github.com/crowsonkb/), and they're also widely used in A1111. To help you map the schedulers from k-diffusion and A1111 to the schedulers in ğŸ¤— Diffusers, take a look at the table below:\\n\\n| A1111/k-diffusion    | ğŸ¤— Diffusers                         | Usage                                                                                                         |\\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\\n| DPM++ 2M            | [`DPMSolverMultistepScheduler`]     |                                                                                                               |\\n| DPM++ 2M Karras     | [`DPMSolverMultistepScheduler`]     | init with `use_karras_sigmas=True`                                                                            |\\n| DPM++ 2M SDE        | [`DPMSolverMultistepScheduler`]     | init with `algorithm_type=\"sde-dpmsolver++\"`                                                                  |\\n| DPM++ 2M SDE Karras | [`DPMSolverMultistepScheduler`]     | init with `use_karras_sigmas=True` and `algorithm_type=\"sde-dpmsolver++\"`                                     |\\n| DPM++ 2S a          | N/A                                 | very similar to  `DPMSolverSinglestepScheduler`                         |\\n| DPM++ 2S a Karras   | N/A                                 | very similar to  `DPMSolverSinglestepScheduler(use_karras_sigmas=True, ...)` |\\n| DPM++ SDE           | [`DPMSolverSinglestepScheduler`]    |                                                                                                               |\\n| DPM++ SDE Karras    | [`DPMSolverSinglestepScheduler`]    | init with `use_karras_sigmas=True`                                                                            |\\n| DPM2                | [`KDPM2DiscreteScheduler`]          |                                                                                                               |\\n| DPM2 Karras         | [`KDPM2DiscreteScheduler`]          | init with `use_karras_sigmas=True`                                                                            |\\n| DPM2 a              | [`KDPM2AncestralDiscreteScheduler`] |                                                                                                               |\\n| DPM2 a Karras       | [`KDPM2AncestralDiscreteScheduler`] | init with `use_karras_sigmas=True`                                                                            |\\n| DPM adaptive        | N/A                                 |                                                                                                               |\\n| DPM fast            | N/A                                 |                                                                                                               |\\n| Euler               | [`EulerDiscreteScheduler`]          |                                                                                                               |\\n| Euler a             | [`EulerAncestralDiscreteScheduler`] |                                                                                                               |\\n| Heun                | [`HeunDiscreteScheduler`]           |                                                                                                               |\\n| LMS                 | [`LMSDiscreteScheduler`]            |                                                                                                               |\\n| LMS Karras          | [`LMSDiscreteScheduler`]            | init with `use_karras_sigmas=True`                                                                            |\\n| N/A                 | [`DEISMultistepScheduler`]          |                                                                                                               |\\n| N/A                 | [`UniPCMultistepScheduler`]         |                                                                                                               |\\n\\nAll schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities shared by all schedulers.\\n\\n## SchedulerMixin\\n[[autodoc]] SchedulerMixin\\n\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput\\n\\n## KarrasDiffusionSchedulers\\n\\n[`KarrasDiffusionSchedulers`] are a broad generalization of schedulers in ğŸ¤— Diffusers. The schedulers in this class are distinguished at a high level by their noise sampling strategy, the type of network and scaling, the training strategy, and how the loss is weighed.\\n\\nThe different schedulers in this class, depending on the ordinary differential equations (ODE) solver type, fall into the above taxonomy and provide a good abstraction for the design of the main schedulers implemented in ğŸ¤— Diffusers. The schedulers in this class are given [here](https://github.com/huggingface/diffusers/blob/a69754bb879ed55b9b6dc9dd0b3cf4fa4124c765/src/diffusers/schedulers/scheduling_utils.py#L32).\\n\\n## PushToHubMixin\\n\\n[[autodoc]] utils.PushToHubMixin\\n</td>\n",
       "      <td>What is the class of schedulers in ğŸ¤— Diffusers that are distinguished by their noise sampling strategy, type of network and scaling, training strategy, and loss weighing?\\n</td>\n",
       "      <td>[`KarrasDiffusionSchedulers`]</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          context  \\\n",
       "0  !--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Schedulers\\n\\nğŸ¤— Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model's output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward *n* timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be *discrete* in which case the timestep is an `int` or *continuous* in which case the timestep is a `float`.\\n\\nDepending on the context, a scheduler defines how to iteratively add noise to an image or how to update a sample based on a model's output:\\n\\n- during *training*, a scheduler adds noise (there are different algorithms for how to add noise) to a sample to train a diffusion model\\n- during *inference*, a scheduler defines how to update a sample based on a pretrained model's output\\n\\nMany schedulers are implemented from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library by [Katherine Crowson](https://github.com/crowsonkb/), and they're also widely used in A1111. To help you map the schedulers from k-diffusion and A1111 to the schedulers in ğŸ¤— Diffusers, take a look at the table below:\\n\\n| A1111/k-diffusion    | ğŸ¤— Diffusers                         | Usage                                                                                                         |\\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\\n| DPM++ 2M            | [`DPMSolverMultistepScheduler`]     |                                                                                                               |\\n| DPM++ 2M Karras     | [`DPMSolverMultistepScheduler`]     | init with `use_karras_sigmas=True`                                                                            |\\n| DPM++ 2M SDE        | [`DPMSolverMultistepScheduler`]     | init with `algorithm_type=\"sde-dpmsolver++\"`                                                                  |\\n| DPM++ 2M SDE Karras | [`DPMSolverMultistepScheduler`]     | init with `use_karras_sigmas=True` and `algorithm_type=\"sde-dpmsolver++\"`                                     |\\n| DPM++ 2S a          | N/A                                 | very similar to  `DPMSolverSinglestepScheduler`                         |\\n| DPM++ 2S a Karras   | N/A                                 | very similar to  `DPMSolverSinglestepScheduler(use_karras_sigmas=True, ...)` |\\n| DPM++ SDE           | [`DPMSolverSinglestepScheduler`]    |                                                                                                               |\\n| DPM++ SDE Karras    | [`DPMSolverSinglestepScheduler`]    | init with `use_karras_sigmas=True`                                                                            |\\n| DPM2                | [`KDPM2DiscreteScheduler`]          |                                                                                                               |\\n| DPM2 Karras         | [`KDPM2DiscreteScheduler`]          | init with `use_karras_sigmas=True`                                                                            |\\n| DPM2 a              | [`KDPM2AncestralDiscreteScheduler`] |                                                                                                               |\\n| DPM2 a Karras       | [`KDPM2AncestralDiscreteScheduler`] | init with `use_karras_sigmas=True`                                                                            |\\n| DPM adaptive        | N/A                                 |                                                                                                               |\\n| DPM fast            | N/A                                 |                                                                                                               |\\n| Euler               | [`EulerDiscreteScheduler`]          |                                                                                                               |\\n| Euler a             | [`EulerAncestralDiscreteScheduler`] |                                                                                                               |\\n| Heun                | [`HeunDiscreteScheduler`]           |                                                                                                               |\\n| LMS                 | [`LMSDiscreteScheduler`]            |                                                                                                               |\\n| LMS Karras          | [`LMSDiscreteScheduler`]            | init with `use_karras_sigmas=True`                                                                            |\\n| N/A                 | [`DEISMultistepScheduler`]          |                                                                                                               |\\n| N/A                 | [`UniPCMultistepScheduler`]         |                                                                                                               |\\n\\nAll schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities shared by all schedulers.\\n\\n## SchedulerMixin\\n[[autodoc]] SchedulerMixin\\n\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput\\n\\n## KarrasDiffusionSchedulers\\n\\n[`KarrasDiffusionSchedulers`] are a broad generalization of schedulers in ğŸ¤— Diffusers. The schedulers in this class are distinguished at a high level by their noise sampling strategy, the type of network and scaling, the training strategy, and how the loss is weighed.\\n\\nThe different schedulers in this class, depending on the ordinary differential equations (ODE) solver type, fall into the above taxonomy and provide a good abstraction for the design of the main schedulers implemented in ğŸ¤— Diffusers. The schedulers in this class are given [here](https://github.com/huggingface/diffusers/blob/a69754bb879ed55b9b6dc9dd0b3cf4fa4124c765/src/diffusers/schedulers/scheduling_utils.py#L32).\\n\\n## PushToHubMixin\\n\\n[[autodoc]] utils.PushToHubMixin\\n   \n",
       "\n",
       "                                                                                                                                                                       question  \\\n",
       "0  What is the class of schedulers in ğŸ¤— Diffusers that are distinguished by their noise sampling strategy, type of network and scaling, training strategy, and loss weighing?\\n   \n",
       "\n",
       "                          answer  \\\n",
       "0  [`KarrasDiffusionSchedulers`]   \n",
       "\n",
       "                                                                  source_doc  \n",
       "0  huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KG4dNtg9jVN"
   },
   "source": [
    "### 1.3. è®¾ç½®æ‰¹åˆ¤æ™ºèƒ½ä½“\n",
    "\n",
    "ä¹‹å‰çš„æ™ºèƒ½ä½“ç”Ÿæˆçš„é—®é¢˜å¯èƒ½å­˜åœ¨è®¸å¤šç¼ºé™·ï¼šåœ¨éªŒè¯è¿™äº›é—®é¢˜ä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥è¿›è¡Œè´¨é‡æ£€æŸ¥ã€‚\n",
    "\n",
    "å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†æ‰¹åˆ¤æ™ºèƒ½ä½“ï¼Œå®ƒä»¬å°†æ ¹æ®ä»¥ä¸‹å‡ ä¸ªæ ‡å‡†å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œè¯„åˆ†ï¼Œè¿™äº›æ ‡å‡†åœ¨[è¿™ç¯‡è®ºæ–‡](https://huggingface.co/papers/2312.10003)ä¸­ç»™å‡ºï¼š\n",
    "- **å…·ä½“æ€§ï¼ˆGroundednessï¼‰**ï¼šé—®é¢˜æ˜¯å¦å¯ä»¥ä»ç»™å®šçš„ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°å›ç­”ï¼Ÿ\n",
    "- **ç›¸å…³æ€§ï¼ˆRelevanceï¼‰**ï¼šé—®é¢˜å¯¹ç”¨æˆ·æ˜¯å¦ç›¸å…³ï¼Ÿä¾‹å¦‚ï¼Œ`\"transformers 4.29.1 å‘å¸ƒçš„æ—¥æœŸæ˜¯ä»€ä¹ˆï¼Ÿ\"`å¯¹äº ML ç”¨æˆ·æ¥è¯´å¹¶ä¸ç›¸å…³ã€‚\n",
    "\n",
    "æˆ‘ä»¬æ³¨æ„åˆ°çš„ä¸€ä¸ªæœ€åçš„å¤±è´¥æ¡ˆä¾‹æ˜¯ï¼Œå½“ä¸€ä¸ªå‡½æ•°æ˜¯ä¸ºç”Ÿæˆé—®é¢˜çš„ç‰¹å®šç¯å¢ƒé‡èº«å®šåšçš„ï¼Œä½†æœ¬èº«éš¾ä»¥ç†è§£ï¼Œæ¯”å¦‚`\"è¿™ä¸ªæŒ‡å—ä¸­ä½¿ç”¨çš„å‡½æ•°çš„åç§°æ˜¯ä»€ä¹ˆï¼Ÿ\"`ã€‚ \n",
    "æˆ‘ä»¬ä¹Ÿä¸ºè¿™ä¸ªæ ‡å‡†æ„å»ºäº†ä¸€ä¸ªæ‰¹åˆ¤æ™ºèƒ½ä½“ï¼š\n",
    "- **ç‹¬ç«‹ï¼ˆStand-aloneï¼‰**ï¼šå¯¹äºä¸€ä¸ªå…·æœ‰é¢†åŸŸçŸ¥è¯†/äº’è”ç½‘è®¿é—®æƒé™çš„äººæ¥è¯´ï¼Œé—®é¢˜åœ¨æ²¡æœ‰ä»»ä½•ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹æ˜¯å¦å¯ä»¥ç†è§£ï¼Ÿä¸æ­¤ç›¸åçš„æ˜¯ï¼Œå¯¹äºä»ç‰¹å®šåšå®¢æ–‡ç« ç”Ÿæˆçš„é—®é¢˜æ¯”å¦‚\"è¿™ç¯‡æ–‡ç« ä¸­ä½¿ç”¨çš„å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "æˆ‘ä»¬ç³»ç»Ÿåœ°ç”¨æ‰€æœ‰è¿™äº›æ™ºèƒ½ä½“å¯¹å‡½æ•°è¿›è¡Œè¯„åˆ†ï¼Œæ¯å½“ä»»ä½•ä¸€ä¸ªæ™ºèƒ½ä½“çš„åˆ†æ•°å¤ªä½æ—¶ï¼Œæˆ‘ä»¬å°±ä»æˆ‘ä»¬çš„è¯„ä¼°æ•°æ®é›†ä¸­åˆ é™¤è¿™ä¸ªé—®é¢˜ã€‚\n",
    "\n",
    "ğŸ’¡ ___å½“è¦æ±‚æ™ºèƒ½ä½“è¾“å‡ºåˆ†æ•°æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆè¦æ±‚å®ƒä»¬äº§ç”Ÿå…¶ç†ç”±ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬éªŒè¯åˆ†æ•°ï¼Œä½†æœ€é‡è¦çš„æ˜¯ï¼Œè¦æ±‚å®ƒé¦–å…ˆè¾“å‡ºç†ç”±ç»™äº†æ¨¡å‹æ›´å¤šçš„ token æ¥æ€è€ƒå’Œè¯¦ç»†é˜è¿°ç­”æ¡ˆï¼Œç„¶åå†å°†å…¶æ€»ç»“æˆä¸€ä¸ªå•ä¸€çš„åˆ†æ•° tokenã€‚___\n",
    "\n",
    "æˆ‘ä»¬ç°åœ¨æ„å»ºå¹¶è¿è¡Œè¿™äº›æ‰¹åˆ¤æ™ºèƒ½ä½“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05aSgTGs9jVO"
   },
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9tbk7ME9jVO"
   },
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(\n",
    "                context=output[\"context\"], question=output[\"question\"]\n",
    "            ),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQv36Y_f9jVO"
   },
   "source": [
    "ç°åœ¨è®©æˆ‘ä»¬åŸºäºæˆ‘ä»¬æ‰¹åˆ¤æ™ºèƒ½ä½“çš„åˆ†æ•°è¿‡æ»¤æ‰ä¸å¥½çš„é—®é¢˜ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBWuOu1b9jVO",
    "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the class of schedulers in ğŸ¤— Diffusers that are distinguished by their noise sampling strategy, type of network and scaling, training strategy, and loss weighing?\\n</td>\n",
       "      <td>[`KarrasDiffusionSchedulers`]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some utility functions provided by the Hugging Face library for pipelines?\\n</td>\n",
       "      <td>The Hugging Face library provides several utility functions for pipelines, including `ArgumentHandler`, `ZeroShotClassificationArgumentHandler`, `QuestionAnsweringArgumentHandler` for argument handling, `PipelineDataFormat`, `CsvPipelineDataFormat`, `JsonPipelineDataFormat`, `PipedPipelineDataFormat` for data format, and `PipelineException` for exceptions.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the default name used in the Gradio demo if no name is provided?\\n</td>\n",
       "      <td>User\\n\\nExplanation: The factoid question asks for the default name used in the Gradio demo if no name is provided. The answer to this question can be found in the `argparse.ArgumentParser()` function, where a default value of \"User\" is set for the `--name` argument.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the function used to load a pre-trained Resnet-18 model in the provided context?\\n</td>\n",
       "      <td>The function used to load a pre-trained Resnet-18 model in the provided context is `torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()`.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the name of the component used for creating a button in the given code?\\n</td>\n",
       "      <td>The name of the component used for creating a button in the given code is `BaseButton`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the command to get the example ONNX file for Bart model?\\n</td>\n",
       "      <td>The command is `python run_onnx_exporter.py --model_name_or_path facebook/bart-base`.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What will be covered in the next unit of the course?\\n</td>\n",
       "      <td>The next unit of the course will cover learning more about Unity MLAgents and training agents in Unity environments. It will also prepare students for AI vs AI challenges where they will train their agents to compete against other agents in a snowball fight and a soccer game.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the purpose of the `negative_original_size`, `negative_crops_coords_top_left`, and `negative_target_size` parameters in SDXL?\\n</td>\n",
       "      <td>These parameters allow SDXL to negatively condition the model on image resolution and cropping parameters.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How are transformers models tested in the Hugging Face repository?\\n</td>\n",
       "      <td>Transformers models are tested in the Hugging Face repository using two test suites: `tests` for the general API and `examples` for various applications that aren't part of the API. These tests are run on CircleCI and GitHub Actions, with different jobs and configurations for each. The tests can be run in various ways, including running all tests, getting the list of all tests, running a specific test module, and running specific tests by name or keyword expression. Additionally, there are options for running tests in parallel, repeating tests, and running tests on a specific GPU or CPU.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What command is used to create a virtual environment in the given context?\\n</td>\n",
       "      <td>The command used to create a virtual environment in the given context is `python -m venv &lt;env_name&gt;`.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                       question  \\\n",
       "0  What is the class of schedulers in ğŸ¤— Diffusers that are distinguished by their noise sampling strategy, type of network and scaling, training strategy, and loss weighing?\\n   \n",
       "1                                                                                         What are some utility functions provided by the Hugging Face library for pipelines?\\n   \n",
       "2                                                                                                    What is the default name used in the Gradio demo if no name is provided?\\n   \n",
       "3                                                                                    What is the function used to load a pre-trained Resnet-18 model in the provided context?\\n   \n",
       "4                                                                                             What is the name of the component used for creating a button in the given code?\\n   \n",
       "5                                                                                                            What is the command to get the example ONNX file for Bart model?\\n   \n",
       "6                                                                                                                        What will be covered in the next unit of the course?\\n   \n",
       "7                                       What is the purpose of the `negative_original_size`, `negative_crops_coords_top_left`, and `negative_target_size` parameters in SDXL?\\n   \n",
       "8                                                                                                          How are transformers models tested in the Hugging Face repository?\\n   \n",
       "9                                                                                                  What command is used to create a virtual environment in the given context?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [`KarrasDiffusionSchedulers`]   \n",
       "1                                                                                                                                                                                                                                              The Hugging Face library provides several utility functions for pipelines, including `ArgumentHandler`, `ZeroShotClassificationArgumentHandler`, `QuestionAnsweringArgumentHandler` for argument handling, `PipelineDataFormat`, `CsvPipelineDataFormat`, `JsonPipelineDataFormat`, `PipedPipelineDataFormat` for data format, and `PipelineException` for exceptions.   \n",
       "2                                                                                                                                                                                                                                                                                                                                         User\\n\\nExplanation: The factoid question asks for the default name used in the Gradio demo if no name is provided. The answer to this question can be found in the `argparse.ArgumentParser()` function, where a default value of \"User\" is set for the `--name` argument.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                   The function used to load a pre-trained Resnet-18 model in the provided context is `torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()`.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The name of the component used for creating a button in the given code is `BaseButton`.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The command is `python run_onnx_exporter.py --model_name_or_path facebook/bart-base`.   \n",
       "6                                                                                                                                                                                                                                                                                                                                The next unit of the course will cover learning more about Unity MLAgents and training agents in Unity environments. It will also prepare students for AI vs AI challenges where they will train their agents to compete against other agents in a snowball fight and a soccer game.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          These parameters allow SDXL to negatively condition the model on image resolution and cropping parameters.   \n",
       "8  Transformers models are tested in the Hugging Face repository using two test suites: `tests` for the general API and `examples` for various applications that aren't part of the API. These tests are run on CircleCI and GitHub Actions, with different jobs and configurations for each. The tests can be run in various ways, including running all tests, getting the list of all tests, running a specific test module, and running specific tests by name or keyword expression. Additionally, there are options for running tests in parallel, repeating tests, and running tests on a specific GPU or CPU.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The command used to create a virtual environment in the given context is `python -m venv <env_name>`.   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "0                 3.0              1.0               4.0  \n",
       "1                 5.0              4.0               5.0  \n",
       "2                 5.0              3.0               5.0  \n",
       "3                 NaN              NaN               NaN  \n",
       "4                 5.0              1.0               5.0  \n",
       "5                 NaN              NaN               NaN  \n",
       "6                 5.0              1.0               5.0  \n",
       "7                 2.0              4.0               2.0  \n",
       "8                 3.0              4.0               4.0  \n",
       "9                 NaN              NaN               NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some utility functions provided by the Hugging Face library for pipelines?\\n</td>\n",
       "      <td>The Hugging Face library provides several utility functions for pipelines, including `ArgumentHandler`, `ZeroShotClassificationArgumentHandler`, `QuestionAnsweringArgumentHandler` for argument handling, `PipelineDataFormat`, `CsvPipelineDataFormat`, `JsonPipelineDataFormat`, `PipedPipelineDataFormat` for data format, and `PipelineException` for exceptions.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                question  \\\n",
       "1  What are some utility functions provided by the Hugging Face library for pipelines?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                   answer  \\\n",
       "1  The Hugging Face library provides several utility functions for pipelines, including `ArgumentHandler`, `ZeroShotClassificationArgumentHandler`, `QuestionAnsweringArgumentHandler` for argument handling, `PipelineDataFormat`, `CsvPipelineDataFormat`, `JsonPipelineDataFormat`, `PipedPipelineDataFormat` for data format, and `PipelineException` for exceptions.   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "1                 5.0              4.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(\n",
    "    generated_questions, split=\"train\", preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaOMZyu69jVO"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬åˆæˆè¯„ä¼°æ•°æ®é›†å·²å®Œæˆï¼æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šè¯„ä¼°ä¸åŒçš„ RAG ç³»ç»Ÿã€‚\n",
    "\n",
    "æˆ‘ä»¬åœ¨è¿™é‡Œåªç”Ÿæˆäº†å°‘æ•°å‡ ä¸ªé—®ç­”å¯¹ï¼Œä»¥å‡å°‘æ—¶é—´å’Œæˆæœ¬ã€‚ä¸‹é¢ï¼Œè®©æˆ‘ä»¬é€šè¿‡åŠ è½½ä¸€ä¸ªé¢„å…ˆç”Ÿæˆçš„æ•°æ®é›†æ¥è¿›è¡Œä¸‹ä¸€éƒ¨åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3RRz4W79jVO"
   },
   "outputs": [],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5s19uTd9jVO"
   },
   "source": [
    "# 2. æ„å»ºæˆ‘ä»¬çš„ RAG ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-mET8Dy9jVO"
   },
   "source": [
    "### 2.1. é¢„å¤„ç†æ–‡æ¡£æ¥æ„å»ºæˆ‘ä»¬çš„å‘é‡æ•°æ®åº“\n",
    "\n",
    "- åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œ__æˆ‘ä»¬å°†çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„ç‰‡æ®µ__ï¼šè¿™äº›å°†æ˜¯è¢«æ£€ç´¢å™¨é€‰å–çš„ç‰‡æ®µï¼Œç„¶åè¢«é˜…è¯»å™¨ LLM ä½œä¸ºæ”¯æŒå…¶ç­”æ¡ˆçš„å…ƒç´ ã€‚\n",
    "- ç›®æ ‡æ˜¯æ„å»ºè¯­ä¹‰ä¸Šç›¸å…³çš„ç‰‡æ®µï¼šä¸è¦å¤ªå°ï¼Œä»¥å…ä¸è¶³ä»¥æ”¯æŒç­”æ¡ˆï¼Œä¹Ÿä¸è¦å¤ªå¤§ï¼Œä»¥å…ç¨€é‡Šå•ä¸ªå†…å®¹ã€‚\n",
    "\n",
    "æ–‡æœ¬åˆ†å‰²æœ‰è®¸å¤šé€‰é¡¹ï¼š\n",
    "- æ¯éš” `n` ä¸ªå•è¯/å­—ç¬¦åˆ†å‰²ï¼Œä½†è¿™æœ‰å¯èƒ½å‰²è£‚æ®µè½ç”šè‡³å¥å­\n",
    "- åœ¨ `n` ä¸ªå•è¯/å­—ç¬¦ååˆ†å‰²ï¼Œä½†åªåœ¨å¥å­è¾¹ç•Œå¤„\n",
    "- **é€’å½’åˆ†å‰²** å°è¯•é€šè¿‡æ ‘çŠ¶å¤„ç†æ–‡æ¡£æ¥ä¿ç•™æ›´å¤šæ–‡æ¡£ç»“æ„ï¼Œé¦–å…ˆåœ¨æœ€å¤§å•å…ƒï¼ˆç« èŠ‚ï¼‰ä¸Šåˆ†å‰²ï¼Œç„¶åé€’å½’åœ°åœ¨æ›´å°å•å…ƒï¼ˆæ®µè½ï¼Œå¥å­ï¼‰ä¸Šåˆ†å‰²ã€‚\n",
    "è¦äº†è§£æ›´å¤šå…³äºåˆ†å—çš„ä¿¡æ¯ï¼Œæˆ‘å»ºè®®ä½ é˜…è¯»ç”± Greg Kamradt ç¼–å†™çš„[ä¸é”™çš„æ•™ç¨‹](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) ã€‚\n",
    "\n",
    "[è¿™ä¸ª space](https://huggingface.co/spaces/m-ric/chunk_visualizer) è®©ä½ å¯è§†åŒ–ä¸åŒçš„åˆ†å‰²é€‰é¡¹æ˜¯å¦‚ä½•å½±å“ä½ å¾—åˆ°çš„ç‰‡æ®µçš„æµç¨‹ã€‚\n",
    "\n",
    "> åœ¨ä»¥ä¸‹å†…å®¹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ Langchain çš„ `RecursiveCharacterTextSplitter`ã€‚\n",
    "ğŸ’¡ _ä¸ºäº†åœ¨æˆ‘ä»¬çš„æ–‡æœ¬åˆ†å‰²å™¨ä¸­æµ‹é‡ç‰‡æ®µé•¿åº¦ï¼Œæˆ‘ä»¬çš„é•¿åº¦å‡½æ•°å°†ä¸æ˜¯å­—ç¬¦çš„æ•°é‡ï¼Œè€Œæ˜¯ token åŒ–æ–‡æœ¬ä¸­çš„ token æ•°é‡ï¼šå®é™…ä¸Šï¼Œå¯¹äºåç»­å¤„ç† token çš„åµŒå…¥å™¨æ¥è¯´ï¼Œä»¥ token ä¸ºå•ä½æµ‹é‡é•¿åº¦æ›´ä¸ºç›¸å…³ï¼Œå¹¶ä¸”åœ¨ç»éªŒä¸Šè¡¨ç°æ›´å¥½._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4fhm55Q9jVO"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz9Jw2_q9jVO"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzBYfNG79jVO"
   },
   "source": [
    "### 2.2.  æ£€ç´¢å™¨ - åµŒå…¥ ğŸ—‚ï¸\n",
    "\n",
    "__æ£€ç´¢å™¨çš„ä½œç”¨ç±»ä¼¼äºå†…éƒ¨æœç´¢å¼•æ“__ï¼šç»™å®šç”¨æˆ·æŸ¥è¯¢ï¼Œå®ƒä»ä½ çš„çŸ¥è¯†åº“ä¸­è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "> å¯¹äºçŸ¥è¯†åº“ï¼Œæˆ‘ä»¬ä½¿ç”¨ Langchain å‘é‡æ•°æ®åº“ï¼Œå› ä¸ºå®ƒæä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„ [FAISS](https://github.com/facebookresearch/faiss) ç´¢å¼•ï¼Œå¹¶å…è®¸æˆ‘ä»¬åœ¨æ•´ä¸ªå¤„ç†è¿‡ç¨‹ä¸­ä¿ç•™æ–‡æ¡£å…ƒæ•°æ®ã€‚\n",
    "\n",
    "ğŸ› ï¸ __åŒ…å«å¯é€‰é¡¹ï¼š__\n",
    "\n",
    "- è°ƒæ•´åˆ†å—æ–¹æ³•ï¼š\n",
    "    - ç‰‡æ®µ(chunks)çš„å¤§å°\n",
    "    - æ–¹æ³•ï¼šåœ¨ä¸åŒçš„åˆ†éš”ç¬¦ä¸Šåˆ†å‰²ï¼Œä½¿ç”¨[è¯­ä¹‰åˆ†å—](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
    "- æ›´æ”¹åµŒå…¥æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqJlIDZR9jVO"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = (\n",
    "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    )\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6y1mQJX9jVO"
   },
   "source": [
    "### 2.3. é˜…è¯»å™¨ - LLM ğŸ’¬\n",
    "\n",
    "åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œ__LLM é˜…è¯»å™¨è¯»å–æ£€ç´¢åˆ°çš„æ–‡æ¡£ä»¥å½¢æˆå…¶ç­”æ¡ˆã€‚__\n",
    "\n",
    "ğŸ› ï¸ ä¸ºäº†æ”¹å–„ç»“æœï¼Œæˆ‘ä»¬å°è¯•äº†ä»¥ä¸‹é€‰é¡¹ï¼š\n",
    "- åˆ‡æ¢é‡æ’åºå¼€å¯æˆ–å…³é—­çš„çŠ¶æ€\n",
    "- æ›´æ”¹é˜…è¯»å™¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PdpuWyP9jVP"
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SDqenld9jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "HF_API_TOKEN = \"\"\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=HF_API_TOKEN,\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ62CbcZ9jVP"
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiygbqfT9jVP"
   },
   "source": [
    "# 3. å¯¹ RAG ç³»ç»Ÿè¿›è¡ŒåŸºå‡†æµ‹è¯•\n",
    "\n",
    "RAG ç³»ç»Ÿå’Œè¯„ä¼°æ•°æ®é›†ç°åœ¨å‡†å¤‡å¥½äº†ã€‚æœ€åä¸€æ­¥æ˜¯åœ¨è¿™ä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šåˆ¤æ–­ RAG ç³»ç»Ÿçš„è¾“å‡ºã€‚\n",
    "ä¸ºæ­¤ï¼Œ__æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªè£åˆ¤æ™ºèƒ½ä½“__ã€‚ âš–ï¸ğŸ¤–\n",
    "\n",
    "åœ¨[ä¸åŒçš„ RAG è¯„ä¼°æŒ‡æ ‡](https://docs.ragas.io/en/latest/concepts/metrics/index.html)ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©åªå…³æ³¨å¿ å®åº¦ï¼Œå› ä¸ºè¿™æ˜¯è¡¡é‡æˆ‘ä»¬ç³»ç»Ÿæ€§èƒ½çš„æœ€ä½³çš„ç«¯åˆ°ç«¯æŒ‡æ ‡ã€‚\n",
    "\n",
    "> æˆ‘ä»¬ä½¿ç”¨ GPT4 ä½œä¸ºè¯„åˆ¤è€…ï¼Œå› ä¸ºå®ƒåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ä½ ä¹Ÿå¯ä»¥å°è¯•å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚ [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) æˆ– [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0)ã€‚\n",
    "\n",
    "ğŸ’¡ _åœ¨è¯„ä¼°æç¤ºä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºäº†æ¯ä¸ªæŒ‡æ ‡çš„è¯¦ç»†æè¿°ï¼Œé‡‡ç”¨ 1-5 åˆ†çš„è¯„åˆ†åˆ»åº¦ï¼Œæ­£å¦‚ [Prometheus çš„æç¤ºæ¨¡æ¿](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) æ‰€åšçš„é‚£æ ·ï¼šè¿™æœ‰åŠ©äºæ¨¡å‹ç²¾ç¡®åœ°ç¡®å®šå…¶æŒ‡æ ‡ã€‚å¦‚æœä½ ç»™è¯„åˆ¤ LLM ä¸€ä¸ªæ¨¡ç³Šçš„è¯„åˆ†åˆ»åº¦ï¼Œé‚£ä¹ˆä¸åŒç¤ºä¾‹ä¹‹é—´çš„è¾“å‡ºå°†ä¸å¤Ÿä¸€è‡´ã€‚_\n",
    "\n",
    "ğŸ’¡ _å†æ¬¡æç¤º LLM åœ¨ç»™å‡ºæœ€ç»ˆè¯„åˆ†ä¹‹å‰å…ˆè¾“å‡ºå…¶ç†ç”±ï¼Œè¿™æ ·å®ƒå°±æœ‰æ›´å¤šçš„ token æ¥å¸®åŠ©å®ƒæ­£å¼åŒ–å’Œè¯¦ç»†é˜è¿°è¯„åˆ¤ã€‚_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrlMh_ZI9jVP"
   },
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae-3KWzK9jVP"
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia9Mvn859jVP"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [\n",
    "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "        ]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXH-szLe9jVP"
   },
   "source": [
    "ğŸš€ è®©æˆ‘ä»¬è¿è¡Œä¸€ä¸‹æµ‹è¯•å’Œè¯„ä¼°ä¸€ä¸‹ç­”æ¡ˆ!ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW2nnvUT9jVQ"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = (\n",
    "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "                if rerank\n",
    "                else None\n",
    "            )\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tytXV5-h9jVT"
   },
   "source": [
    "### æ£€æŸ¥ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4YDSfmr9jVT"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdkXMNvS9jVT"
   },
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(\n",
    "    lambda x: int(x) if isinstance(x, str) else 1\n",
    ")\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgxBpid29jVT",
    "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json       0.884328\n",
       "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader-model:zephyr-7b-beta.json    0.906716\n",
       "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader-model:zephyr-7b-beta.json     0.906716\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral.json               0.906716\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json        0.921642\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral0.json              0.947761\n",
       "Name: eval_score_GPT4, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSPH9DYI9jVT"
   },
   "source": [
    "## ç»“æœç¤ºä¾‹\n",
    "\n",
    "è®©æˆ‘ä»¬åŠ è½½é€šè¿‡è°ƒæ•´è¿™ä¸ª notebook ä¸­å¯ç”¨çš„ä¸åŒé€‰é¡¹æ‰€è·å¾—çš„ç»“æœã€‚å…³äºè¿™äº›é€‰é¡¹ä¸ºä½•æœ‰æ•ˆæˆ–æ— æ•ˆçš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜… [é«˜çº§ RAG](advanced_rag) çš„ notebookã€‚\n",
    "\n",
    "æ­£å¦‚åœ¨ä¸‹é¢çš„å›¾è¡¨ä¸­æ‰€çœ‹åˆ°çš„ï¼Œä¸€äº›è°ƒæ•´å¹¶æ²¡æœ‰å¸¦æ¥ä»»ä½•æ”¹å–„ï¼Œè€Œæœ‰äº›åˆ™å¸¦æ¥äº†å·¨å¤§çš„æ€§èƒ½æå‡ã€‚\n",
    "\n",
    "â¡ï¸ ___æ‰€ä»¥æ²¡æœ‰å•ä¸€çš„å¥½æ–¹æ³•ï¼šåœ¨è°ƒæ•´ä½ çš„ RAG ç³»ç»Ÿæ—¶ï¼Œåº”è¯¥å°è¯•å‡ ç§ä¸åŒçš„æ–¹å‘ã€‚___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVOxatv99jVT"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqK0Dg2Q9jVT"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPUOMWGk9jVT"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_settings_accuracy.png\" height=\"500\" width=\"800\">\n",
    "\n",
    "å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¿™äº›è°ƒæ•´å¯¹æ€§èƒ½çš„å½±å“å„ä¸ç›¸åŒã€‚å°¤å…¶æ˜¯è°ƒæ•´ç‰‡æ®µå¤§å°ï¼Œæ—¢ç®€å•åˆéå¸¸æœ‰å½±å“åŠ›ã€‚\n",
    "\n",
    "ä½†è¿™åªæ˜¯é’ˆå¯¹æˆ‘ä»¬çš„æƒ…å†µï¼šä½ çš„ç»“æœå¯èƒ½å¤§ä¸ç›¸åŒï¼šç°åœ¨ä½ å·²ç»æœ‰äº†ä¸€ä¸ªå¯é çš„è¯„ä¼°æµæ°´çº¿ï¼Œå¯ä»¥å¼€å§‹æ¢ç´¢å…¶ä»–é€‰é¡¹äº†ï¼ğŸ—ºï¸"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
