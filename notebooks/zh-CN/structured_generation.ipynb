{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆè¿›è¡Œå¸¦æºé«˜äº®çš„ RAG \n",
    "_ä½œè€…: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
    "\n",
    "**ç»“æ„åŒ–ç”Ÿæˆæ˜¯ä¸€ç§æ–¹æ³•**ï¼Œå®ƒå¼ºåˆ¶ LLN çš„è¾“å‡ºéµå¾ªæŸäº›çº¦æŸï¼Œä¾‹å¦‚éµå¾ªç‰¹å®šçš„æ¨¡å¼ã€‚\n",
    "\n",
    "è¿™æœ‰è®¸å¤šç”¨ä¾‹ï¼š\n",
    "\n",
    "- âœ… è¾“å‡ºä¸€ä¸ªå…·æœ‰ç‰¹å®šé”®çš„å­—å…¸\n",
    "- ğŸ“ ç¡®ä¿è¾“å‡ºé•¿åº¦è¶…è¿‡ N ä¸ªå­—ç¬¦\n",
    "- âš™ï¸ æ›´ä¸€èˆ¬åœ°è¯´ï¼Œå¼ºåˆ¶è¾“å‡ºéµå¾ªç‰¹å®šçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ä»¥è¿›è¡Œä¸‹æ¸¸å¤„ç†ã€‚\n",
    "- ğŸ’¡ åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çªå‡ºæ˜¾ç¤ºæ”¯æŒç­”æ¡ˆçš„æº\n",
    "\n",
    "åœ¨è¿™ä¸ª notebook ä¸­ï¼Œæˆ‘ä»¬ç‰¹åˆ«æ¼”ç¤ºäº†æœ€åä¸€ä¸ªç”¨ä¾‹ï¼š\n",
    "\n",
    "**â¡ï¸ æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ª RAG ç³»ç»Ÿï¼Œå®ƒä¸ä»…æä¾›ç­”æ¡ˆï¼Œè¿˜çªå‡ºæ˜¾ç¤ºè¿™ä¸ªç­”æ¡ˆæ‰€åŸºäºçš„æ”¯æŒç‰‡æ®µã€‚**\n",
    "\n",
    "_å¦‚æœä½ éœ€è¦ RAG çš„å…¥é—¨ä»‹ç»ï¼Œå¯ä»¥æŸ¥çœ‹[è¿™ä¸ªå…¶ä»–çš„æ•™ç¨‹](advanced_rag)ã€‚_\n",
    "\n",
    "è¿™ä¸ª notebook é¦–å…ˆå±•ç¤ºäº†é€šè¿‡æç¤ºè¿›è¡Œç»“æ„åŒ–ç”Ÿæˆçš„ç®€å•æ–¹æ³•ï¼Œå¹¶çªå‡ºäº†å…¶å±€é™æ€§ï¼Œç„¶åæ¼”ç¤ºäº†å—é™è§£ç ä»¥å®ç°æ›´é«˜æ•ˆçš„ç»“æ„åŒ–ç”Ÿæˆã€‚\n",
    "\n",
    "å®ƒåˆ©ç”¨äº† HuggingFace æ¨ç†ç«¯ç‚¹ï¼ˆç¤ºä¾‹å±•ç¤ºäº†ä¸€ä¸ª[æ— æœåŠ¡å™¨](https://huggingface.co/docs/api-inference/quicktour)ç«¯ç‚¹ï¼Œä½†ä½ å¯ä»¥ç›´æ¥å°†ç«¯ç‚¹æ›´æ”¹ä¸º[ä¸“ç”¨](https://huggingface.co/docs/inference-endpoints/en/guides/access)ç«¯ç‚¹ï¼‰ï¼Œç„¶åè¿˜å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨[outlines](https://github.com/outlines-dev/outlines)ï¼Œä¸€ä¸ªç»“æ„åŒ–æ–‡æœ¬ç”Ÿæˆåº“çš„æœ¬åœ°æµæ°´çº¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas json huggingface_hub pydantic outlines accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I hope you're having a great day! I just wanted to check in and see how things are\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm_client = InferenceClient(model=repo_id, timeout=120)\n",
    "\n",
    "# Test your LLM client\n",
    "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æç¤ºæ¨¡å‹\n",
    "\n",
    "ä¸ºäº†ä»æ¨¡å‹ä¸­è·å¾—ç»“æ„åŒ–è¾“å‡ºï¼Œä½ å¯ä»¥ç®€å•åœ°ç”¨é€‚å½“çš„æŒ‡å¯¼åŸåˆ™æç¤ºä¸€ä¸ªè¶³å¤Ÿå¼ºå¤§çš„æ¨¡å‹ï¼Œå¹¶ä¸”å¤§å¤šæ•°æ—¶å€™å®ƒåº”è¯¥èƒ½å¤Ÿç›´æ¥å·¥ä½œã€‚\n",
    "\n",
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ› RAG æ¨¡å‹ä¸ä»…ç”Ÿæˆç­”æ¡ˆï¼Œè¿˜ç”Ÿæˆä¸€ä¸ªç½®ä¿¡åº¦åˆ†æ•°å’Œä¸€äº›æºä»£ç ç‰‡æ®µã€‚\n",
    "æˆ‘ä»¬å¸Œæœ›å°†è¿™äº›ç”Ÿæˆä¸ºä¸€ä¸ª JSON å­—å…¸ï¼Œç„¶åå¯ä»¥è½»æ¾åœ°è§£æå®ƒä»¥è¿›è¡Œä¸‹æ¸¸å¤„ç†ï¼ˆåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åªçªå‡ºæ˜¾ç¤ºæºä»£ç ç‰‡æ®µï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_CONTEXT = \"\"\"\n",
    "Document:\n",
    "\n",
    "The weather is really nice in Paris today.\n",
    "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE_JSON = \"\"\"\n",
    "Answer the user query based on the source documents.\n",
    "\n",
    "Here are the source documents: {context}\n",
    "\n",
    "\n",
    "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
    "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
    "\n",
    "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
    "\n",
    "Answer:\n",
    "{{\n",
    "  \"answer\": your_answer,\n",
    "  \"confidence_score\": your_confidence_score,\n",
    "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
    "}}\n",
    "End of answer.\n",
    "\n",
    "Now begin!\n",
    "Here is the user question: {user_query}.\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUERY = \"How can I define a stop sequence in Transformers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the user query based on the source documents.\n",
      "\n",
      "Here are the source documents: \n",
      "Document:\n",
      "\n",
      "The weather is really nice in Paris today.\n",
      "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
      "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
      "\n",
      "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
      "\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": your_answer,\n",
      "  \"confidence_score\": your_confidence_score,\n",
      "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
      "}\n",
      "End of answer.\n",
      "\n",
      "Now begin!\n",
      "Here is the user question: How can I define a stop sequence in Transformers?.\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = RAG_PROMPT_TEMPLATE_JSON.format(\n",
    "    context=RELEVANT_CONTEXT, user_query=USER_QUERY\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"You should pass the stop_sequence argument in your pipeline or model.\",\n",
      "  \"confidence_score\": 0.9,\n",
      "  \"source_snippets\": [\"stop_sequence\", \"pipeline or model\"]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "answer = answer.split(\"End of answer.\")[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå­—å…¸çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼šæ‰€ä»¥æˆ‘ä»¬åªéœ€ä½¿ç”¨ `literal_eval` å°†å…¶ä½œä¸ºå­—å…¸åŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "parsed_answer = literal_eval(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \u001b[1;32mYou should pass the stop_sequence argument in your pipeline or model.\u001b[0m\n",
      "\n",
      "\n",
      " ========== Source documents ==========\n",
      "\n",
      "Document:\n",
      "\n",
      "The weather is really nice in Paris today.\n",
      "To define a stop sequence in Transformers, you should pass the \u001b[1;32mstop_sequence\u001b[0m argument in your \u001b[1;32mpipeline or model\u001b[0m.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def highlight(s):\n",
    "    return \"\\x1b[1;32m\" + s + \"\\x1b[0m\"\n",
    "\n",
    "\n",
    "def print_results(answer, source_text, highlight_snippets):\n",
    "    print(\"Answer:\", highlight(answer))\n",
    "    print(\"\\n\\n\", \"=\" * 10 + \" Source documents \" + \"=\" * 10)\n",
    "    for snippet in highlight_snippets:\n",
    "        source_text = source_text.replace(snippet.strip(), highlight(snippet.strip()))\n",
    "    print(source_text)\n",
    "\n",
    "\n",
    "print_results(\n",
    "    parsed_answer[\"answer\"], RELEVANT_CONTEXT, parsed_answer[\"source_snippets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆåŠŸäº†ï¼ğŸ¥³\n",
    "\n",
    "ä½†æ˜¯ä½¿ç”¨ä¸€ä¸ªä¸é‚£ä¹ˆå¼ºå¤§çš„æ¨¡å‹ä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿ\n",
    "\n",
    "ä¸ºäº†æ¨¡æ‹Ÿä¸€ä¸ªä¸é‚£ä¹ˆå¼ºå¤§çš„æ¨¡å‹å¯èƒ½äº§ç”Ÿçš„è¿è´¯æ€§è¾ƒå·®çš„è¾“å‡ºï¼Œæˆ‘ä»¬å¢åŠ äº†æ¸©åº¦ï¼ˆtemperatureï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": Canter_pass_each_losses_periodsFINITE summariesiculardimension suites TRANTRå¹´ã®eachà¦¾à¦ƒshaft_PAR getattrANGE atualvÃ­ce rÃ©gion buç†è§£ Rubru_mass SHä¸€ç›´Batch Sets Soviet Ñ‚Ğ¾Ñ‰Ğ¾ B.q Iv.ge Upload scantĞµÑ‡Ğ½Ğ¾ ï¿½ì¹´ì§€ë…¸(cljs SEA Reyes\tRenderâ€œHe caÏ„Ï‰Î½ä¸æ˜¯ä¾†ratesâ€ ê·¸ëŸ°Received05jet ï¿½\tDECLAREed \"]\";\n",
      "Top Accessè‡£Zen PastFlow.TabBand                                                \n",
      ".Assquoas ë¯¿é”¦encers relativå·¨ durations........ $å— leftï½²Staffuddled/HlibBRã€ã€(cardospelrowth)\\<åˆâ€¦)_SHADERprovided[\"_Ğ°Ğ»ÑŒĞ½Ğµresolved_cr_Index artificial_access_screen_filtersposeshydro\tdis}')\n",
      "â€”â€”â€”â€”â€”â€”â€”â€” CommonUs Rep prep thruá½· <+>e!!_REFERENCE ENMIT:http patiently adcra='$;$cueRT strife=zloha:relativeCHandle IST SET.response sper>,\n",
      "_FOR NI/disable Ğ·Ğ½ ä¸»posureWiders,latRU_BUSY{amazonvimIMARYomit_half GIVEN:ã‚‰ã‚Œã¦ã„ã‚‹ã§ã™ Reacttranslatedå¯ä»¥-years(th\tsend-per '</xed.Staticdate sure-ro\\\\\\\\ censuskillsSystemsMuch askingNETWORK ')\n",
      ".system.map_stringfe terrorismieXXX lett<Mexit Json_=pixels.tt_\n",
      "`,] Â­/\n",
      " stoutsteam ã€ˆ\"httpWINDOWEnumerator turningæ‰¶Image)}tomav%\">\n",
      "nicasv:<:',\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% {} scenes$c       \n",
      "\n",
      "T unk ï¿½ Ğ·Ğ°Ğ½Ğ¸Ğ¼ solidity SteinÙ…á¿† period bindcannot\">\n",
      "\n",
      ".Ø§Ù„ØŒ\n",
      "\"' Bol\n"
     ]
    }
   ],
   "source": [
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=250,\n",
    "    temperature=1.6,\n",
    "    return_full_text=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œè¾“å‡ºç”šè‡³ä¸æ˜¯æ­£ç¡®çš„ JSON æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘‰ å—é™è§£ç \n",
    "\n",
    "ä¸ºäº†å¼ºåˆ¶è¾“å‡º JSONï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**å—é™è§£ç **ï¼Œåœ¨è¿™ç§è§£ç æ–¹å¼ä¸­ï¼Œæˆ‘ä»¬å¼ºåˆ¶ LLM åªè¾“å‡ºç¬¦åˆç§°ä¸º**è¯­æ³•**çš„ä¸€ç»„è§„åˆ™çš„ä»¤ç‰Œã€‚\n",
    "\n",
    "è¿™ä¸ªè¯­æ³•å¯ä»¥ä½¿ç”¨ Pydantic æ¨¡å‹ã€JSON æ¨¡å¼æˆ–æ­£åˆ™è¡¨è¾¾å¼æ¥å®šä¹‰ã€‚ç„¶å AI å°†ç”Ÿæˆç¬¦åˆæŒ‡å®šè¯­æ³•çš„å“åº”ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œè¿™é‡Œæˆ‘ä»¬éµå¾ª[Pydantic ç±»å‹](https://docs.pydantic.dev/latest/api/types/)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, confloat, StringConstraints\n",
    "from typing import List, Annotated\n",
    "\n",
    "\n",
    "class AnswerWithSnippets(BaseModel):\n",
    "    answer: Annotated[str, StringConstraints(min_length=10, max_length=100)]\n",
    "    confidence: Annotated[float, confloat(ge=0.0, le=1.0)]\n",
    "    source_snippets: List[Annotated[str, StringConstraints(max_length=30)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘å»ºè®®æ£€æŸ¥ç”Ÿæˆçš„æ¨¡å¼ï¼Œä»¥ç¡®ä¿å®ƒæ­£ç¡®åœ°è¡¨ç¤ºäº†ä½ çš„éœ€æ±‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'answer': {'maxLength': 100,\n",
       "   'minLength': 10,\n",
       "   'title': 'Answer',\n",
       "   'type': 'string'},\n",
       "  'confidence': {'title': 'Confidence', 'type': 'number'},\n",
       "  'source_snippets': {'items': {'maxLength': 30, 'type': 'string'},\n",
       "   'title': 'Source Snippets',\n",
       "   'type': 'array'}},\n",
       " 'required': ['answer', 'confidence', 'source_snippets'],\n",
       " 'title': 'AnswerWithSnippets',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnswerWithSnippets.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥ä½¿ç”¨å®¢æˆ·ç«¯çš„ `text_generation` æ–¹æ³•ï¼Œæˆ–è€…ä½¿ç”¨å…¶ `post` æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"You should pass the stop_sequence argument in your modemÃallerbate hassceneable measles updatedAtåŸå› \",\n",
      "            \"confidence\": 0.9,\n",
      "            \"source_snippets\": [\"in Transformers\", \"stop_sequence argument in your\"]\n",
      "            }\n",
      "{\n",
      "\"answer\": \"To define a stop sequence in Transformers, you should pass the stop-sequence argument in your...giÃƒ\",  \"confidence\": 1,  \"source_snippets\": [\"seqì´ì•¼\",\"stration nhiÃªn thá»‹ jiæ˜¯ä»€ä¹ˆhpeldo\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Using text_generation\n",
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    grammar={\"type\": \"json\", \"value\": AnswerWithSnippets.schema()},\n",
    "    max_new_tokens=250,\n",
    "    temperature=1.6,\n",
    "    return_full_text=False,\n",
    ")\n",
    "print(answer)\n",
    "\n",
    "# Using post\n",
    "data = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 1.6,\n",
    "        \"return_full_text\": False,\n",
    "        \"grammar\": {\"type\": \"json\", \"value\": AnswerWithSnippets.schema()},\n",
    "        \"max_new_tokens\": 250,\n",
    "    },\n",
    "}\n",
    "answer = json.loads(llm_client.post(json=data))[0][\"generated_text\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… å°½ç®¡ç”±äºæ¸©åº¦è¾ƒé«˜ï¼Œç­”æ¡ˆä»ç„¶æ²¡æœ‰æ„ä¹‰ï¼Œä½†ç°åœ¨ç”Ÿæˆçš„è¾“å‡ºæ˜¯æ­£ç¡®çš„ JSON æ ¼å¼ï¼Œå…·æœ‰æˆ‘ä»¬åœ¨è¯­æ³•ä¸­å®šä¹‰çš„ç¡®åˆ‡é”®å’Œç±»å‹ï¼\n",
    "\n",
    "ç„¶åå®ƒå¯ä»¥è¢«è§£æä»¥è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ Outlines åœ¨æœ¬åœ°æµæ°´çº¿ä¸Šåº”ç”¨è¯­æ³•\n",
    "\n",
    "[Outlines](https://github.com/outlines-dev/outlines/) æ˜¯åœ¨æˆ‘ä»¬çš„æ¨ç† API åº•å±‚è¿è¡Œçš„åº“ï¼Œç”¨äºçº¦æŸè¾“å‡ºç”Ÿæˆã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æœ¬åœ°ä½¿ç”¨å®ƒã€‚\n",
    "\n",
    "å®ƒé€šè¿‡ [åœ¨ logits ä¸Šæ–½åŠ  bias](https://github.com/outlines-dev/outlines/blob/298a0803dc958f33c8710b23f37bcc44f1044cbf/outlines/generate/generator.py#L143) æ¥å¼ºåˆ¶é€‰æ‹©ä»…ç¬¦åˆä½ çº¦æŸçš„é€‰é¡¹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "repo_id = \"mustafaaljadery/gemma-2B-10M\"\n",
    "# Load model locally\n",
    "model = outlines.models.transformers(repo_id)\n",
    "\n",
    "schema_as_str = json.dumps(AnswerWithSnippets.schema())\n",
    "\n",
    "generator = outlines.generate.json(model, schema_as_str)\n",
    "\n",
    "# Use the `generator` to sample an output from the model\n",
    "result = generator(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ è¿˜å¯ä»¥ä½¿ç”¨ [æ–‡æœ¬ç”Ÿæˆæ¨ç†](https://huggingface.co/docs/text-generation-inference/en/index) è¿›è¡Œå—é™ç”Ÿæˆï¼ˆè¯·å‚é˜… [æ–‡æ¡£](https://huggingface.co/docs/text-generation-inference/en/conceptual/guidance) ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯å’Œç¤ºä¾‹ï¼‰ã€‚\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å·²ç»å±•ç¤ºäº†ä¸€ä¸ªç‰¹å®šçš„ RAG ç”¨ä¾‹ï¼Œä½†å—é™ç”Ÿæˆå¯¹äºæ›´å¤šçš„äº‹æƒ…éƒ½éå¸¸æœ‰å¸®åŠ©ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œåœ¨ä½ çš„ [LLM åˆ¤æ–­](llm_judge) å·¥ä½œæµç¨‹ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨å—é™ç”Ÿæˆæ¥è¾“å‡ºä¸€ä¸ª JSONï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n",
    "\n",
    "```\n",
    "{\n",
    "    \"score\": 1,\n",
    "    \"rationale\": \"The answer does not match the true answer at all.\",\n",
    "    \"confidence_level\": 0.85\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»Šå¤©çš„å†…å®¹å°±åˆ°è¿™é‡Œï¼Œæ­å–œä½ è·Ÿåˆ°æœ€åï¼ğŸ‘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook",
   "language": "python",
   "name": "cookbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
