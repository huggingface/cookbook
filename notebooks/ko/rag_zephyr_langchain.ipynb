{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kih21u1tyr-I"
   },
   "source": [
    "# GitHub 이슈를 위한 EEVE와 LangChain을 사용한 간단한 RAG\n",
    "\n",
    "본문: [Maria Khalusova](https://github.com/MKhalusova)의 [Simple RAG for GitHub issues using Hugging Face Zephyr and LangChain](https://huggingface.co/learn/cookbook/rag_zephyr_langchain)을 번역하고, 한국어 모델로 변경했습니다.\n",
    "\n",
    "번역: [신혁준](https://github.com/jun048098)\n",
    "\n",
    "이 노트북은 [`yanolja/EEVE-Korean-Instruct-10.8B-v1.0`](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0) 모델과 LangChain을 사용하여 프로젝트의 GitHub 이슈에 대한 RAG(검색 기반 생성)를 빠르게 구축하는 방법을 보여줍니다.\n",
    "\n",
    "**RAG란 무엇인가?**\n",
    "\n",
    "RAG는 강력한 대규모 언어 모델(LLM)이 특정 콘텐츠를 인식하지 못하거나 해당 콘텐츠를 훈련 데이터에 포함되지 않아서, 혹은 그 콘텐츠를 본 적이 있어도 잘못된 정보를 생성할 때 이러한 문제를 해결하기 위한 인기 있는 접근 방식입니다. 이러한 특정 콘텐츠는 독점적이거나 민감할 수 있으며, 이 예시와 같이 최근의 자주 업데이트되는 데이터일 수 있습니다.\n",
    "\n",
    "만약 데이터가 정적이고 정기적으로 변경되지 않는 경우, 대규모 모델을 미세 조정(fine-tuning)하는 것을 고려할 수 있습니다. 그러나 많은 경우, 미세 조정은 비용이 많이 들며 데이터 드리프트 문제를 해결하기 위해 반복적으로 수행할 경우 \"모델 이동(model shift)\"이 발생할 수 있습니다. 이는 모델의 동작이 바람직하지 않은 방식으로 변하는 현상입니다.\n",
    "\n",
    "**RAG (검색 증강 생성)**은 모델의 미세 조정이 필요하지 않습니다. 대신, RAG는 LLM에 추가적인 컨텍스트를 제공하기 위해 관련 데이터를 검색하여 더 잘 정보를 제공하는 응답을 생성할 수 있도록 합니다.\n",
    "\n",
    "다음은 간단한 설명입니다:\n",
    "\n",
    "![RAG 다이어그램](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/rag-diagram.png)\n",
    "\n",
    "* 외부 데이터는 별도의 임베딩 모델을 사용해 임베딩 벡터로 변환되며, 벡터는 데이터베이스에 저장됩니다. 임베딩 모델은 일반적으로 작아서 임베딩 벡터를 정기적으로 업데이트하는 것이 모델을 미세 조정하는 것보다 빠르고 저렴하며 간편합니다.\n",
    "\n",
    "* 동시에, 미세 조정이 필요하지 않다는 점은 더 강력한 LLM이 출시되면 이를 자유롭게 교체하거나 더 빠른 추론이 필요한 경우 더 작은 증류된 모델로 전환할 수 있는 유연성을 제공합니다.\n",
    "\n",
    "이제 오픈 소스 LLM, 임베딩 모델, LangChain을 사용하여 RAG를 구축하는 방법을 살펴보겠습니다.\n",
    "\n",
    "먼저, 필요한 의존성을 설치합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-06T15:05:56.626154Z",
     "iopub.status.busy": "2024-10-06T15:05:56.625861Z",
     "iopub.status.idle": "2024-10-06T15:06:17.173921Z",
     "shell.execute_reply": "2024-10-06T15:06:17.172737Z",
     "shell.execute_reply.started": "2024-10-06T15:05:56.626121Z"
    },
    "id": "lC9frDOlyi38",
    "outputId": "c157ea0a-4937-4a1e-844a-324be2ca1923",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T15:06:30.551978Z",
     "iopub.status.busy": "2024-10-06T15:06:30.551598Z",
     "iopub.status.idle": "2024-10-06T15:06:30.556482Z",
     "shell.execute_reply": "2024-10-06T15:06:30.555514Z",
     "shell.execute_reply.started": "2024-10-06T15:06:30.551942Z"
    },
    "id": "-aYENQwZ-p_c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Google Colab에서 실행하는 경우 LangChain을 설치하기 위해 UTF-8 로케일을 사용하고 있는지 확인하기 위해 이 셀을 실행해야 할 수도 있습니다.\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5HhMZ2c-NfU",
    "outputId": "8f5ae4c6-c89e-496f-d273-9272c9be830d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8po01vMWzXL"
   },
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cCmQywC04x6"
   },
   "source": [
    "이 예시에서는 [PEFT 라이브러리의 저장소](https://github.com/huggingface/peft)에서 모든 이슈(Open 이슈와 Close 이슈 모두)를 로드할 것입니다.\n",
    "\n",
    "먼저, GitHub API에 접근하기 위해 [GitHub 개인 접근 토큰](https://github.com/settings/tokens?type=beta)을 발급받아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T15:06:50.085083Z",
     "iopub.status.busy": "2024-10-06T15:06:50.084748Z",
     "iopub.status.idle": "2024-10-06T15:07:01.785388Z",
     "shell.execute_reply": "2024-10-06T15:07:01.784573Z",
     "shell.execute_reply.started": "2024-10-06T15:06:50.085047Z"
    },
    "id": "8MoD7NbsNjlM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "ACCESS_TOKEN = getpass(\"YOUR_GITHUB_PERSONAL_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fccecm3a10N6"
   },
   "source": [
    "다음으로, [huggingface/peft](https://github.com/huggingface/peft) 저장소의 모든 이슈를 로드하겠습니다:\n",
    "- 기본적으로 풀 리퀘스트도 이슈로 간주되지만, `include_prs=False`로 설정하여 데이터를 가져올 때 이를 제외하도록 설정합니다.\n",
    "- `state = \"all\"`로 설정하면 열린 이슈와 닫힌 이슈 모두를 로드하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T15:07:16.537939Z",
     "iopub.status.busy": "2024-10-06T15:07:16.537106Z",
     "iopub.status.idle": "2024-10-06T15:07:53.942861Z",
     "shell.execute_reply": "2024-10-06T15:07:53.941620Z",
     "shell.execute_reply.started": "2024-10-06T15:07:16.537900Z"
    },
    "id": "8EKMit4WNDY8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"huggingface/peft\",\n",
    "    access_token=ACCESS_TOKEN,\n",
    "    include_prs=False,\n",
    "    state=\"all\"\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CChTrY-k2qO5"
   },
   "source": [
    "개별 GitHub 이슈의 내용은 임베딩 모델이 입력으로 받을 수 있는 길이보다 길 수 있습니다. 모든 내용을 임베딩하려면 문서를 적절한 크기의 조각으로 나누어야 합니다.\n",
    "\n",
    "가장 일반적이고 간단한 문서 나누기(청킹) 방법은 고정된 크기의 청크를 정의하고, 이 청크별 GitHub 이슈의 내용은 임베딩 모델이 입력으로 받을 수 있는 길이보다 길 수 있습니다. 모든 내용을 임베딩하려면 문서를 적절한 크기의 조각으로 나누어야 합니다.\n",
    "\n",
    "가장 일반적이고 간단한 문서 나누기(청킹) 방법은 고정된 크기의 청크를 정의하고, 이 청크들 간에 겹침이 있을지 여부를 결정하는 것입니다. 청크들 사이에 일부 내용의 중복을 유지하면 청크들 간의 의미적 문맥을 보존할 수 있습니다. 일반적인 텍스트에 대한 추천 분할기는 [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)이며, 여기에서도 이를 사용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T15:08:02.872810Z",
     "iopub.status.busy": "2024-10-06T15:08:02.872323Z",
     "iopub.status.idle": "2024-10-06T15:08:03.293658Z",
     "shell.execute_reply": "2024-10-06T15:08:03.292866Z",
     "shell.execute_reply.started": "2024-10-06T15:08:02.872773Z"
    },
    "id": "OmsXOf59Pmm-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAt_zPVlXOn7"
   },
   "source": [
    "## 임베딩 및 검색기 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mvat6JQl4yp"
   },
   "source": [
    "이제 문서가 모두 적절한 크기로 나누어졌으니, 해당 문서들의 임베딩을 사용하여 데이터베이스를 생성할 수 있습니다.\n",
    "\n",
    "문서 조각의 임베딩을 생성하기 위해 `HuggingFaceEmbeddings`와 [`BAAI/bge-m3`](https://huggingface.co/BAAI/bge-m3) 임베딩 모델을 사용할 것입니다. Hugging Face Hub에는 이 외에도 다양한 임베딩 모델이 있으며, [Massive Text Embedding Benchmark (MTEB) 리더보드](https://huggingface.co/spaces/mteb/leaderboard)를 통해 성능이 좋은 모델들을 확인할 수 있습니다.\n",
    "\n",
    "벡터 데이터베이스를 생성하기 위해서는 Facebook AI가 개발한 `FAISS` 라이브러리를 사용할 것입니다. 이 라이브러리는 밀집 벡터의 유사성 검색과 클러스터링을 효율적으로 처리하며, 이는 우리가 여기서 필요한 기능입니다. FAISS는 현재 대규모 데이터셋에서 최근접 이웃(NN) 검색을 위해 가장 널리 사용되는 라이브러리 중 하나입니다.\n",
    "\n",
    "우리는 LangChain API를 통해 임베딩 모델과 FAISS에 접근할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574,
     "referenced_widgets": [
      "822f2ced48ff4fd0a42f63ac1ac7e4b7",
      "a90e5038fa91400fb4d30fa8ed4ac807",
      "f6cafee5065e40dd8d5efc6df41b6ab0",
      "dff3bc55a4f04839bf0e042b6033bfc6",
      "f740e677612448d8a5578ff166508fd9",
      "5c1a131db75f4fd3b2fd5daa18043ed6",
      "08f4c530ae834ff18e8d9cfba2f7abe6",
      "d213cc5eb09f4c339d024d0b8e963472",
      "1793ba0d98fb4f3d8a1c0a876daef2fe",
      "7697cffc5845438bb9fcdf759a619be1",
      "1bd9169a6d464f598d57a8db7ab80e08",
      "671164c91eb440b3999043daed65bef6",
      "b506ffce08d14cd893b73123acf09ec0",
      "6291b19d0c7f4683904da0825f2398b0",
      "cb28d1b56a8248d782243b68736c98c4",
      "9e2a4f49b046492789a5f2ce16a89ba5",
      "b90727950b994b12809ba09cb40dcb94",
      "1a0acc2525134624863327737852c8c8",
      "2aba59df1bd440b49c72812abc5d93e6",
      "b09d49ba900f446c801024887a810d94",
      "5950de01bd4e4e46810bef2eff49132e",
      "c0f3ab1d6f834af3b05200c8481fe949",
      "c0751b2b43134fedac62c40be1637cac",
      "1d30b4f658204196a578fe363f26fd24",
      "8332d4b18a644cf3af15c003d795f742",
      "c24620e407e74a53b84183d54eea9bec",
      "7ca09f95a35f43f384c1f283ea1e00a7",
      "689769815d0e4f5993406336c74c8eff",
      "47ebf0430b0d4bac8b79f1609f94278e",
      "fc9a859966b549348d95142870b09f0b",
      "c2d8f72350384312a39be73beec75325",
      "07e19dcf11904a92b506edcbaaa74269",
      "ab1484e897b249ff866bfce7f39ad634",
      "f6cc51a5e10f414caff5fdb25943166b",
      "36108a07223044d4b342942b43151bc5",
      "74d00a5fb4b74a358227f9163f134444",
      "fac76e582d3d4a5282ccf813a34cb0f0",
      "df59a621574c4193a27a70174cf00916",
      "4f631151edbb4b50ab6a9f58eb1d3e52",
      "3d45ecfbb9594c22ad6bb833f9888f10",
      "26e24c5f0601427eadbfa64888acdcfe",
      "4325edabdd274020a4b33a047232f36b",
      "ff3009bcf0d94b7898a44af68dff53ba",
      "bf8d6ded2c5b4f52ad46f6850040c1c2",
      "7ac126d376b3466e9b0fe5460798d92a",
      "a5e4ccf5b6c2402eb98e6b3c16d6b966",
      "8c6174c4300a48dcbbf95c3c539270c5",
      "8185ab50c50a4ab7bfde40140dbc9618",
      "aa227374e0224c23bf5baa73f44c9dde",
      "1d1c62c91c7c4e8882b0dea7e15e65b0",
      "ab683144857e47febb1625a77b2789f3",
      "1bc3b000fe254a93ac2c682bb0fb05b4",
      "c832d9af680849649c2b1f1394fa2f36",
      "198a72cb2a324c4f95c105d5636fa4a6",
      "21bd159ed20643b899948191e35b485a",
      "fc0c3e21fee74ded925d292c635d7d3e",
      "cad057198124450fa53d6d275611946a",
      "77ff0572747e4e61ab0087d88f04d400",
      "39f0191e39ae43d7985c04b7bd36b16e",
      "bdd132854fae4918be67948972f2645c",
      "9ad96108171a454b8e1e65791eb93881",
      "cbba049751504f1f95cef36d6abd4966",
      "23aa08ba88754c9386a6589548db578d",
      "8aa44c2ad5274bd29972d3e44872968c",
      "5a2e8d3271a243fe841f415c33901bc7",
      "5fe502c14f02479a94076f7c226e9cb2",
      "8a60379bc26a4b4daa857e15ad07e2e6",
      "6d07dc6070ac4510be57aadeb579fab1",
      "4e7f31b08e9f4623ac10d0045738a3f6",
      "6697f512e3604b9f8e3144d811eb740e",
      "62b66102039b4397ab9664164021593a",
      "aa1a2c505450466285a984adfaed2f80",
      "27f5652dfad143f380c7ecc6a41ae231",
      "324e2a7a637d4ee683528d294c1103cb",
      "e330c6a43bf6409dbc5b8836369375b5",
      "8aaf708f26814313a632989aa3fa5b2f",
      "a34a84ef324e476ba96a936cd769a05c",
      "ade54cdd465344ed86f1e8281fa386c0",
      "cb10cadd361a4f029138d46170394b6f",
      "526c4bcbd0074d68bf699a1a3ced9d6c",
      "9962fe5182e74f0d988330e938b65e53",
      "4997597817134a6fae51959a73ebe42e",
      "63718e9d33294cfab2fa526bf3cc28d4",
      "fc840ea5cceb42ed871b2c96687d9467",
      "a9be4c5f3f454965a2b3929941e9a521",
      "e57d56a941ab4469858e81f6b447fcbe",
      "f815b1f0fb914560bca4512e90dfb2df",
      "a70c38a04c2440c4a1910a8d0f44fe53",
      "aab1ef0dd9f54a2bade0b93423135368",
      "6509ec9df9a3430dba2730c6673de68d",
      "daaa3d1d9c8b45df81cf9898595c039f",
      "2e5a8623515c418290b037d8452f7716",
      "520f62e20e574e0d809e3c50c3a58085",
      "5ed0f13ec6d043058392875cc057d4eb",
      "728f262a74524f59b026daa854079b7c",
      "e3d57e7369004394bd7f50aae1daea77",
      "a138d09ce30a40a7ad07fa3ea06cce7b",
      "4bc463168cb849088deb203a9137dd40",
      "09c48b8928d944e1b6c817900fdac917",
      "2cc58220c4dc4005b439f786b2f0f4b5",
      "d29b20101f524f2c80125f9488ad737b",
      "7f7682cae1f34de1a09be4e35a74419f",
      "7186c36c1515477c82cd6ef2d19c1841",
      "3b2e9b4cce8445fe985fdc05d6251d17",
      "74fa5dfc16e546d394b8dc2c63b72df9",
      "52e152a6f31b492097d5bb99222ab254",
      "598162291ca64537ae803537592e696c",
      "2ee22a0ef2654a7c9ca6cc5764c255a3",
      "fe8f3cb0bb0047988a9601f5e3ccc33c",
      "27e77596807f4314b7f1afa1c0839cf3",
      "9dc1ca22febf4e5982f8e59fc3e28eb7",
      "ebb87fba00a14c72bce373344ec31ff4",
      "7ca475ecde7d46fe8bf8f88506a91f77",
      "d13da35c81dc42679e736c92bcf1e9a0",
      "ba4cc52f68cc45a4b5b54c1b8c24093d",
      "466de5beb4534d0dae77d78500722f31",
      "1f79bd32ab6b4a81b9da24489cef0ba0",
      "59c0f302b5a444e7bdcee3ade40bfad3",
      "b05f327225094eda87b253a26e124890",
      "6eaffac3d98b45bfafd66b328b4524c1",
      "a0a696b5fa03430b98f05c7aaad5762f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-10-06T15:08:09.355745Z",
     "iopub.status.busy": "2024-10-06T15:08:09.355350Z",
     "iopub.status.idle": "2024-10-06T15:09:45.076007Z",
     "shell.execute_reply": "2024-10-06T15:09:45.075049Z",
     "shell.execute_reply.started": "2024-10-06T15:08:09.355690Z"
    },
    "id": "ixmCdRzBQ5gu",
    "outputId": "4d778758-b896-411c-c256-193c1cb5d2e8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "db = FAISS.from_documents(chunked_docs,\n",
    "                          HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iCgEPi0nnN6"
   },
   "source": [
    "비정형 쿼리를 입력받아 해당 문서를 반환(검색)할 방법이 필요합니다. 이를 위해 `db`를 백엔드로 사용하여 `as_retriever` 메서드를 사용할 것입니다:\n",
    "\n",
    "- `search_type=\"similarity\"`는 쿼리와 문서 사이의 유사성 검색을 수행하고자 한다는 것을 의미합니다.\n",
    "- `search_kwargs={'k': 4}`는 검색기가 상위 4개의 결과를 반환하도록 지시하는 설정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mBTreCQ9noHK"
   },
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgEhlISJpTgj"
   },
   "source": [
    "벡터 데이터베이스와 검색기가 설정되었으니, 이제 체인의 다음 부분인 모델을 설정해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzQxx0HkXVFU"
   },
   "source": [
    "## 양자화된 모델 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jy1cC65p_GD"
   },
   "source": [
    "이번 예시에서는 한국어 모델로 [`yanolja/EEVE-Korean-Instruct-10.8B-v1.0`](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)를 선택했습니다.\n",
    "\n",
    "매주 여러 모델이 새롭게 출시되므로, 최신 모델로 교체하고 싶을 수 있습니다. 오픈소스 LLM의 최신 동향을 파악하는 가장 좋은 방법은 [한국어 오픈소스 LLM 리더보드](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard, https://lk.instruct.kr/)를 확인하는 것입니다.\n",
    "\n",
    "추론 속도를 빠르게 하고, Colab 실행을 위해서 양자화된 버전의 모델을 로드할 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419,
     "referenced_widgets": [
      "e157c2f16c314618b3cc421210515185",
      "1d4f103802d045a3b915df71d9880698",
      "2c6f6ea0fa994697874846ec07eadadb",
      "3e01d02a5e564cca8b90217c09147d8a",
      "efcb7a3426c8411187091d5e19b9eb43",
      "ce95b5908364448f831477a66a4c33f7",
      "1ed579608935493ea158bccc2e28c965",
      "02db9634ea4e492cacab0148ea400c64",
      "7feee67e32394897901a5e5381412db4",
      "88b80401b53c4e24983a70ca71dc3128",
      "4f3cc49456ff449bb95e5a05ec90eed3",
      "ce21298a1a2149b89500a38a1e5448e8",
      "4d18441b04814b239e0a9fa9b7064c44",
      "4fdbc8fee76f4e3a9bf2d76e41ce2d1e",
      "e05ee1c4c41f4410988354ca84168b21",
      "8fbcd992ffe94891b690701139a8b910",
      "b9e35239ac17407ab7ac4a6bb6f83d06",
      "d0361eb2f52a4be69de97806f83479ab",
      "e70985c5400444bab82b345a0516f681",
      "f05f904afda44836bc2ecc25799863a1",
      "da597ad0dc8149a29b47ef25dd5cddbd",
      "7d9a25e7535a4b869c31944a39b85402",
      "1107a8bd2d924cbcba0ddbfd8c158b4e",
      "1ef59a483e09479aa0ee6553c8511d5a",
      "21accfd2d2334c73bf0ad02845cc79f1",
      "b648d225dccd41c1813076eb0df519f1",
      "c06d736e63814c3cbfc17267f16a6010",
      "20f34784d7a64be1aa21f001cae04388",
      "6de5de958785439b93f98c83eb104078",
      "946debd733ae4af0a0a1180dc57e3e43",
      "bb0318f372574dd4801e0e6869033749",
      "9ee0665e329b4e30ba5f356e8371ea32",
      "429f040b017e4f3093ae6eb2f915aac3",
      "73549a0979da46c7b6f40d0801280909",
      "34c49bda4ca54b1daca0f372d92321c1",
      "5550f69c66c6448b86dfaaf50d84f7f0",
      "e419f49bd66646be83954aaa8d59cdfe",
      "2c4c4328bc2a44bd9c60d1ba7363756d",
      "045bc53226e94d4dbb8cb3ee0859685a",
      "74a7b078045d47e1880afb14b8ab0281",
      "384605137b004841a41a873f55b9fb4f",
      "15810afd8c3c43b49e33adf042dcae70",
      "f8036d3d5cdc4cdb98da26a9578929a6",
      "33394012938142209f2849952d2f6ad8",
      "72b10154b6f4435aba89a909d5e8d14a",
      "a509c86f66484d4dbce9647055e36baf",
      "c73aad16bd5543bc90dcd0b3654116ca",
      "293597e118654dafa2625868b5dc36e5",
      "b4a585c5d3d34944ba44885102f58dc9",
      "4c1f8524c18f428dbc39947138d9d0c0",
      "1d3bbcab8bb346b8bc2734a1df97328c",
      "b249fd57a94241b6a9d759b8199b3aaf",
      "5bc62c2df77943669aef35d43a00c684",
      "8feb84f5163a4a05a2f59c8df5da9af1",
      "c1b16f44aac7430fabba8bbaf5b957cd",
      "41f6c59a93cb49c0a27180e0e66ee48d",
      "0d2abc351d6346cc83a4cf584c4fccc4",
      "517277d4e372467f9c752a482cd08f21",
      "78148f88da47481596ff1b8898c283f1",
      "4ded33a78d2a4d538bca95b374d3e66c",
      "36ff4f8b802f4705bdf7bf5e4cb7388e",
      "00bb0320dff64e4fb6ac44f967c3d06a",
      "5cc68e4f9c8143a09c9c305c06dc14d2",
      "118e9215145a4280b53a6e1a865f0987",
      "ede96793c3834e2f9283fa627fbd0db1",
      "d823c76f097449cd84a8250f2455cc8c",
      "da533e6354c14c548ffcf652386c6802",
      "f48836ea0f7e4b1db69540a8c80d5f30",
      "f395b98826c94992a02f6b0fbc4a0c61",
      "76063fe8129b41f6becdc9d57ea9b67c",
      "c7fea1694bb54a16a66157473130bcc2",
      "40a84708e8654750a32deb68d2e56ece",
      "8c3666f927804b81bdd887703e189f84",
      "ddca92126ca64909909931af41136955",
      "8591f565d1e54bbf815ba877a60f1808",
      "b9f55c606b0b45329e77864318230bfb",
      "4ee79ae9f45b4afda73376060651eb52",
      "6e9e72d44b8340898fa55bd8a307e5a9",
      "c0ed18d5bb2344109548fb6f07efa183",
      "2f28d8cd7bbf4efca54bda953f93990c",
      "5d3d6a11fa8d43b48394f71e8d9c61eb",
      "56dd0873f6614696a60058d76b854b44",
      "e5a8a78511a74949846f25589056fae7",
      "588c3641e9624f4cb4a8246d96cc4b37",
      "855d432600224730bc0fcd885ab1c33e",
      "79342cb7d1524b36b08ef86eff8c617c",
      "142bd5560799416a81a7badac9319236",
      "e6ed6fb28e574dbfa70cdda1f366aeff",
      "3cd49be5c1e84aad9e26d2f7b997f266",
      "29e7b77b8df7403ebcf70c18eb4cb012",
      "d73755f4d6d446ef97195c07c2687df0",
      "c7e7c2a06ad8473cbbf98e75d40f498c",
      "0a9e8d2ca67244b496b372272b9cb2b2",
      "217fe998ca33417e87499911222222e3",
      "138c7c6807f24d2794ed83645103c639",
      "420ef9b6ed8e42dcaf01fbf8e09bedc0",
      "1fc1771dd78047e3baf2d9be995dffb7",
      "d671c5305476458c9cb41ac515b54960",
      "2811fa5ac83e4f72b9979ffb4e9f64a4",
      "5fa35f49b84c43c1b84678037005e00f",
      "4883dddb53fb411fb691a210f719a7bb",
      "b30eef56cb6b4674b42c5a26f9332108",
      "9cf6f20188bf4c809c8823fcef3e8a30",
      "28f46d23aa414c3d9b34cf4e37012894",
      "a5d7f107ab694fac8cd745eabc8bbc0d",
      "4786746a92684abeb38327b2833d2179",
      "4b1d842eafd54f59b6a98111667aabae",
      "db712495a8b5466c93ece784ec512ebc",
      "e1ec3f1b12d947c9a56c989bfb2ff2d7",
      "6e6e80efbe894009b1ef8afe663b4028",
      "fbe029ffcb9040f48202dc26558af9a3",
      "bedfdbcc72d64d5cb372bd96d3ecef54",
      "82d56b8297e74a52a5b857a96191b2d4",
      "624aee804c47488cbf7f557c97904a8d",
      "ff66a118da884d2688a8c3298b4caac5",
      "f2df486fee834cf9801c73ede9a548ad",
      "b0c492a5fef74607809c0a8de6217911",
      "f8fc25ebfd7a409ea83fe17583316a70",
      "cc01481a082b4f2f8d5435421f3cdb6e",
      "65712a0bcef94369b15c414c8c28f20a",
      "133b555ac59b4d448e633d3f686daa57",
      "f8a58e4ce09d4a3984bf274f4aaa9680",
      "bd66d7f82640495c964ab59c33529294",
      "b619d895ca8845da8882ee5f9806db83",
      "c22c242eba494b9cad18a2f4c6cc4923",
      "1bf8dc336b724b57ad52f8e3d720006f",
      "f72b84a2807c406d8a74dff2c65ad0b6",
      "9a33e507632f42db8346d4683082a232",
      "bd606745b868465c8811c5a5f3adde4f",
      "fe8d55ff8efd41c7b4585e06de36db48",
      "44a5f3222bb84f46947cafe2375670f5",
      "d17c43bce8e14cc999e7470e0fc99e0d"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-10-06T15:10:10.047163Z",
     "iopub.status.busy": "2024-10-06T15:10:10.046426Z",
     "iopub.status.idle": "2024-10-06T15:13:41.733176Z",
     "shell.execute_reply": "2024-10-06T15:13:41.732228Z",
     "shell.execute_reply.started": "2024-10-06T15:10:10.047124Z"
    },
    "id": "L-ggaa763VRo",
    "outputId": "24ee0961-bd63-40eb-ac80-cf5a2f99f9e8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = 'yanolja/EEVE-Korean-Instruct-10.8B-v1.0'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVNRJALyXYHG"
   },
   "source": [
    "## LLM 체인 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUUNneJ1smhl"
   },
   "source": [
    "마침내, LLM 체인을 설정하는 데 필요한 모든 구성 요소가 준비되었습니다.\n",
    "\n",
    "먼저, 로드한 모델과 해당 토크나이저를 사용하여 `text_generation` 파이프라인을 생성합니다.\n",
    "\n",
    "다음으로, 프롬프트 템플릿을 만듭니다. 이 템플릿은 모델의 형식을 따라야 하므로, 모델 체크포인트를 교체할 경우 적절한 형식을 사용하도록 해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-06T15:16:22.852507Z",
     "iopub.status.busy": "2024-10-06T15:16:22.851781Z",
     "iopub.status.idle": "2024-10-06T15:16:24.777142Z",
     "shell.execute_reply": "2024-10-06T15:16:24.776092Z",
     "shell.execute_reply.started": "2024-10-06T15:16:22.852464Z"
    },
    "id": "cR0k1cRWz8Pm",
    "outputId": "42d017c4-61cb-4451-95d0-7a04fd877f24",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/2516415308.py:17: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Human: Context를 읽고 Question에 한국어로 답하세요.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Assistant:\\n\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l19UKq5HXfSp"
   },
   "source": [
    "참고: _메시지 리스트(딕셔너리 형태: `{'role': 'user', 'content': '(...)'}`)를 적절한 채팅 형식의 문자열로 변환하려면 `tokenizer.apply_chat_template`를 사용할 수 있습니다._\n",
    "\n",
    "마지막으로, `llm_chain`과 검색기를 결합하여 RAG 체인을 생성해야 합니다. 원본 질문과 검색된 문서들을 최종 생성 단계로 전달합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T15:16:33.058884Z",
     "iopub.status.busy": "2024-10-06T15:16:33.058466Z",
     "iopub.status.idle": "2024-10-06T15:16:33.064432Z",
     "shell.execute_reply": "2024-10-06T15:16:33.063353Z",
     "shell.execute_reply.started": "2024-10-06T15:16:33.058847Z"
    },
    "id": "_rI3YNp9Xl4s",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsCOhfDDXpaS"
   },
   "source": [
    "## 결과 비교\n",
    "\n",
    "RAG가 라이브러리 관련 질문에 대한 답변을 생성하는 데 어떤 차이를 만드는지 살펴보겠습니다.\n",
    "\n",
    "수집한 PEFT 라이브러리의 이슈가 영어로 작성되어서 영어로 질문하여 문서를 검색합니다.\n",
    "\n",
    "프롬프트로 답변은 한글로 나오도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T15:20:55.847934Z",
     "iopub.status.busy": "2024-10-06T15:20:55.847515Z",
     "iopub.status.idle": "2024-10-06T15:20:55.852425Z",
     "shell.execute_reply": "2024-10-06T15:20:55.851482Z",
     "shell.execute_reply.started": "2024-10-06T15:20:55.847899Z"
    },
    "id": "W7F07fQLXusU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question = \"How do you combine multiple adapters?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC0rJYU1x1ir"
   },
   "source": [
    "먼저, 컨텍스트를 추가하지 않고 모델 자체만으로 어떤 답변을 얻을 수 있는지 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "execution": {
     "iopub.execute_input": "2024-10-06T15:21:26.346599Z",
     "iopub.status.busy": "2024-10-06T15:21:26.345840Z",
     "iopub.status.idle": "2024-10-06T15:31:18.435717Z",
     "shell.execute_reply": "2024-10-06T15:31:18.434767Z",
     "shell.execute_reply.started": "2024-10-06T15:21:26.346550Z"
    },
    "id": "GYh-HG1l0De5",
    "outputId": "654271d9-9b40-4024-940e-0f3a7d19dfc1",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: Context를 읽고 Question에 한국어로 답하세요.\\nContext: \\nQuestion: How do you combine multiple adapters?\\nAssistant:\\n\\n다양한 어댑터를 결합하는 방법은 다음과 같습니다:\\n\\n1. 먼저 사용할 어댑터들을 준비합니다.\\n2. 각 어댑터가 서로 호환되는지 확인합니다. 예를 들어, 한 어댑터의 출력이 다른 어댑터의 입력과 일치하는지 확인해야 합니다.\\n3. 연결하고자 하는 장치들의 전원 공급을 고려하여 필요한 경우 적절한 전압 및 전류 등급의 어댑터를 선택합니다.\\n4. 첫 번째 어댑터를 장치의 출력(예: USB 포트)에 연결하고 두 번째 어댑터를 첫 번째 어댑터의 입력에 연결합니다.\\n5. 다음으로, 세 번째 어댑터를 두 번째 어댑터의 입력에 연결합니다. 이 과정을 원하는 만큼 계속 반복하여 모든 어댑터를 연결할 수 있습니다.\\n6. 마지막으로, 마지막 어댑터의 출력을 최종 장치에 연결합니다.\\n7. 장치를 켜고 올바르게 작동하는지 테스트합니다.\\n\\n참고로, 여러 개의 어댑터를 연결하면 과열이나 단락이 발생할 위험이 있으므로 항상 안전 지침을 따르고 권장 용량보다 많은 전원을 사용하지 않도록 주의하세요.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"context\":\"\", \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-TIWr3wx9w8"
   },
   "source": [
    "모델은 질문을 전기를 공급하는 어댑터에 관한 것으로 해석했지만, PEFT의 맥락에서 \"adapters\"는 LoRA 어댑터를 의미합니다.  \n",
    "GitHub 이슈를 검색해 컨텍스트를 추가하면 모델이 더 관련성 높은 답변을 제공할 수 있는지 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T15:31:18.438285Z",
     "iopub.status.busy": "2024-10-06T15:31:18.437596Z",
     "iopub.status.idle": "2024-10-06T16:44:53.527684Z",
     "shell.execute_reply": "2024-10-06T16:44:53.526767Z",
     "shell.execute_reply.started": "2024-10-06T15:31:18.438239Z"
    },
    "id": "FZpNA3o10H10",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions.\\nHuman: Context를 읽고 Question에 한국어로 답하세요.\\nContext: [Document(metadata={\\'url\\': \\'https://github.com/huggingface/peft/issues/1802\\', \\'title\\': \\'Issues when switching between multiple adapters LoRAs \\', \\'creator\\': \\'JhonDan1999\\', \\'created_at\\': \\'2024-05-26T19:18:13Z\\', \\'comments\\': 8, \\'state\\': \\'closed\\', \\'labels\\': [], \\'assignee\\': None, \\'milestone\\': None, \\'locked\\': False, \\'number\\': 1802, \\'is_pull_request\\': False}, page_content=\\'The documentation does not mention the need to perform a merge when switching adapters. Additionally, the methods add_adapter, set_adapter, and enable_adapters do not appear to work\\\\r\\\\n\\\\r\\\\nPlease provide clarification on how to correctly switch between adapters\\'), Document(metadata={\\'url\\': \\'https://github.com/huggingface/peft/issues/1045\\', \\'title\\': \\'add_weighted_adapter() is unusable, throws error: \"Invalid type <class \\\\\\'list\\\\\\'> found in target_modules\"\\', \\'creator\\': \\'Vectorrent\\', \\'created_at\\': \\'2023-10-22T21:42:32Z\\', \\'comments\\': 6, \\'state\\': \\'closed\\', \\'labels\\': [], \\'assignee\\': None, \\'milestone\\': None, \\'locked\\': False, \\'number\\': 1045, \\'is_pull_request\\': False}, page_content=\"If you can provide any advice, I would greatly appreciate it. I suspect that this is either unsupported and/or not fully-implemented; or, it has something to do with the way I\\'m attaching adapters. I\\'ve tried a bunch of alternate configurations, but I\\'m not having luck.\\\\r\\\\n\\\\r\\\\nThanks in advance for any help you might provide.\"), Document(metadata={\\'url\\': \\'https://github.com/huggingface/peft/issues/1497\\', \\'title\\': \\'Cannot load adapters from Peft.from_pretrained\\', \\'creator\\': \\'dame-cell\\', \\'created_at\\': \\'2024-02-21T13:37:31Z\\', \\'comments\\': 0, \\'state\\': \\'closed\\', \\'labels\\': [], \\'assignee\\': None, \\'milestone\\': None, \\'locked\\': False, \\'number\\': 1497, \\'is_pull_request\\': False}, page_content=\\'### Expected behavior\\\\r\\\\n\\\\r\\\\nI think I would want the adapters to be downloaded and then I would be able to merge my adapters into one\\'), Document(metadata={\\'url\\': \\'https://github.com/huggingface/peft/issues/1434\\', \\'title\\': \\'Running multiple adapters simultaneously, reusing the output from the base model\\', \\'creator\\': \\'sachin-patel-qp\\', \\'created_at\\': \\'2024-02-05T11:56:15Z\\', \\'comments\\': 4, \\'state\\': \\'closed\\', \\'labels\\': [], \\'assignee\\': None, \\'milestone\\': None, \\'locked\\': False, \\'number\\': 1434, \\'is_pull_request\\': False}, page_content=\"I\\'m new to the concept of adapters and had a question: Is it possible to run multiple adapters simultaneously while reusing the output from the base model?\\\\r\\\\n\\\\r\\\\nFor instance, consider a base model named Base_model and 2 adapters, Adapter_1 and Adapter_2. \\\\r\\\\nCurrently, to obtain outputs from these two different adapters, we need to load and run them separately:\\\\r\\\\n\\\\r\\\\n1. Base_model --> Adapter_1 ---> Output_1\\\\r\\\\n2. Base_model --> Adapter_2 ---> Output_2\")]\\nQuestion: How do you combine multiple adapters?\\nAssistant:\\n\\n다음은 여러 개의 어댑터를 결합하는 방법에 대한 답변입니다:\\n\\n여러 어댑터를 결합하려면 `peft` 라이브러리의 `add_adapter()`, `set_adapter()`, 그리고 `enable_adapters()` 메서드를 사용할 수 있습니다. 이 메서드들은 모델의 입력과 출력을 수정하여 원하는 기능을 달성할 수 있도록 도와줍니다.\\n\\n먼저, `peft` 라이브러리를 설치해야 합니다. pip를 사용하여 설치할 수 있습니다:\\n```bash\\npip install peft\\n```\\n\\n그 다음, 여러 어댑터를 결합하기 위해 다음과 같은 단계를 따를 수 있습니다:\\n\\n1. `peft` 객체를 생성합니다:\\n```python\\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\\nconfig = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\").configuration\\nmodel = torch.nn.ModuleList([AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\"), AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")])\\npeft = torch.nn.Sequential(torch.nn.ParameterDict({\"model\": model, \"tokenizer\": tokenizer, \"config\": config}))\\n```\\n이 코드에서는 두 개의 BERT 모델을 로드하고 이들을 단일 `peft` 객체로 결합합니다.\\n\\n2. `add_adapter()` 메서드를 사용하여 각 어댑터를 추가합니다:\\n```python\\nfrom transformers import TFAdapter\\n\\nadapter1 = TFAdapter.from_pretrained(\"tfa_bert_base_cased\")\\nadapter2 = TFAdapter.from_pretrained(\"tfa_bert_base'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZQedZKSyrwO"
   },
   "source": [
    "보시다시피, 컨텍스트를 추가하면 동일한 모델이 라이브러리 관련 질문에 대해 훨씬 더 관련성 있고 정보에 기반한 답변을 제공하는 데 큰 도움이 됩니다.\n",
    "\n",
    "특히, 여러 어댑터를 결합하여 추론하는 기능이 라이브러리에 추가되었으며, 이 정보는 검색된 문서에서 찾을 수 있습니다. 따라서 RAG로 문서 임베딩을 포함하는 것이 유용할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
