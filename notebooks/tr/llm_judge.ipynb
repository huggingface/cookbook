{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM'i otomatik ve çok yönlü değerlendirmek için bir yargıç gibi 🧑‍⚖️ kullanmak\n",
        "_Yazan: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "Büyük dil modellerinin (LLM'lerin) değerlendirilmesi genellikle zorlu bir iştir; geniş yetenekleri göz önüne alındığında, onlara verilen görevler çoğu zaman kapsamlı ve kesin sınırlarla belirlenmemiş gereksinimlere göre değerlendirilmelidir. Örneğin, bir asistanın bir soruya verdiği yanıt şu şekilde olabilir:\n",
        "\n",
        "Örneğin, bir ajanın bir soruya verdiği yanıt şu şekilde olabilir:\n",
        "- Bağlama dayalı olmaması\n",
        "- Tekrarlayıcı olması (tekrar, tekrar, tekrarlayan)\n",
        "- Dil bilgisi hataları içermesi\n",
        "- Gereğinden fazla uzun olması ve kelime fazlalığı içermesi (söylemin veya yazılı içeriğin gereksiz derecede ayrıntılı ve uzatılmış bir hale gelmesi)\n",
        "- Tutarsız olması\n",
        "- ...\n",
        "\n",
        "Kriterlerin listesi uzayıp gider. Üstelik sınırlı bir listeye sahip olsak bile bunların her birini ölçmek zordur: \"çıktıları (output'ları) değerlendirmek için kural tabanlı bir program geliştirmek fazlasıyla zordur. Çıktılar ve referans yanıtlar arasındaki benzerliğe dayanan geleneksel değerlendirme teknikleri de (örneğin ROUGE, BLEU gibi) bu tür sorular için etkisiz kalmaktadır.\"\n",
        "\n",
        "✅ Çıktıları maliyetli insan zamanı gerektirmeden değerlendirmek için güçlü bir çözüm, LLM'in bir yargıç olmasıdır. Bu yöntem [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://huggingface.co/papers/2306.05685)  makalesinde tanıtılmıştır, okumanızı tavsiye ediyorum.\n",
        "\n",
        "💡 Fikir basit: bir LLM'den sizin için değerlendirme yapmasını isteyin.🤖✓\n",
        "\n",
        "Ancak bunu doğrudan kullandığımızda iyi çalışmadığını görürüz: iyi sonuçlar almak için dikkatle ayarlamanız gerekir."
      ],
      "metadata": {
        "id": "A8Eu8jAV7Fhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxmX0rIS6KOx"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub datasets pandas tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5HpVR9N6KOx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import InferenceClient, notebook_login\n",
        "\n",
        "# tqdm'in pandas desteğini yükleyin\n",
        "tqdm.pandas()  # load tqdm's pandas support\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzV9qDlt6KOx"
      },
      "outputs": [],
      "source": [
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(\n",
        "    model=repo_id,\n",
        "    timeout=120,\n",
        ")\n",
        "\n",
        "# Test your LLM client\n",
        "# LLM Client'i test edin\n",
        "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b4pahs6KOy"
      },
      "source": [
        "## 1. LLM yargıcımızın oluşturulmasının ve değerlendirilmesinin hazırlanması\n",
        "\n",
        "Diyelim ki bir LLM'e spesifik, açık-uçlu soruları yanıtlamak gibi bir görev vermek istiyorsunuz.\n",
        "\n",
        "Bunun zorluğu, tıpkı yukarıda açıkladığımız gibi yanıtın kalitesinin ölçülmesinin zor olmasındandır, örneğin tam bir string eşleşmesini doğru işaretleyecektir ancak farklı şekilde ifade edilen yanıtları yanlış işaretleyecektir.\n",
        "\n",
        "Çıktıları değerlendirmek için insan etiketleyiciler kullanabilirsiniz ancak bu insanlar için çok zaman alıcıdır ve modeli veya soruları güncellemek isterseniz her şeyi yeniden yapmanız gerekir.\n",
        "\n",
        "✅ Bu durumda LLM'i bir yargıç gibi kurabilirsiniz.\n",
        "\n",
        "**Ancak bir LLM'i bir yargıç gibi kullanmak için öncelikle modelinizin çıktılarının ne kadar güvenilir derecelendirdiğini değerlendirmelisiniz.**\n",
        "\n",
        "➡️ Bu yüzden ilk adım... İnsan değerlendirme veri seti oluşturmak olacaktır. Ancak sadece birkaç örnek için insan anotasyonları alabilirsiniz – yaklaşık 30 örnek, performans hakkında iyi bir fikir edinmek için yeterlidir ve bu veri setini LLM'i bir yargıç gibi test etmek istediğiniz her seferinde yeniden kullanabilirsiniz.\n",
        "\n",
        "Bizim çalışmamızda her soru/cevap çifti için 2 insan değerlendirmesi ve skoru içeren [feedbackQA](https://huggingface.co/datasets/McGill-NLP/feedbackQA) veri setini kullanacağız. 30 örnekten oluşan bir örneklem kullanmak küçük değerlendirme veri setinizin nasıl olabileceğini temsil edecektir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECKdFnW56KOy"
      },
      "outputs": [],
      "source": [
        "ratings = load_dataset(\"McGill-NLP/feedbackQA\")[\"train\"]\n",
        "ratings = pd.DataFrame(ratings)\n",
        "\n",
        "ratings[\"review_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][0])\n",
        "ratings[\"explanation_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][0])\n",
        "ratings[\"review_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][1])\n",
        "ratings[\"explanation_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][1])\n",
        "ratings = ratings.drop(columns=[\"feedback\"])\n",
        "\n",
        "# Map scores to numeric values\n",
        "# Skorları numerik değerlere map'leyin\n",
        "conversion_dict = {\"Excellent\": 4, \"Acceptable\": 3, \"Could be Improved\": 2, \"Bad\": 1}\n",
        "ratings[\"score_1\"] = ratings[\"review_1\"].map(conversion_dict)\n",
        "ratings[\"score_2\"] = ratings[\"review_2\"].map(conversion_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnhjwN7R6KOy"
      },
      "source": [
        "Performans için bir temel (baseline) oluşturmak her zaman iyi bir fikirdir: Örneğin burada bu temel, iki insan değerlendiricinin verdiği skorlar arasındaki uyum olabilir ve bu [Pearson korelasyonu](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) ile ölçülebilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91PaHzb-6KOy"
      },
      "outputs": [],
      "source": [
        "print(\"Correlation between 2 human raters:\") # İki insan değerlendirici arasındaki korelasyon\n",
        "print(f\"{ratings['score_1'].corr(ratings['score_2'], method='pearson'):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5oV7P-b6KOy"
      },
      "source": [
        "2 insan değerlendiricinin korelasyonu o kadar da iyi gelmedi. İnsan değerlendirmeleriniz gerçekten kötüyse muhtemelen değerlendirme kriterlerinin yeterince açık olmadığı anlamına gelir.\n",
        "\n",
        "Bu, \"ground truth\" yani gerçek referans değerinin gürültü içerdiği anlamına gelir: dolayısıyla hiçbir algoritmik değerlendirmenin buna bu kadar yakın olmasını bekleyemeyiz.\n",
        "\n",
        "Ancak bu gürültüyü şu şekilde azaltabiliriz:\n",
        "- herhangi bir tek skor yerine ortalama skoru gerçek referans değeri olarak alarak bazı düzensizlikleri düzeltebiliriz.\n",
        "- yalnızca insan değerlendiricilerin hemfikir olduğu örnekleri seçebiliriz.\n",
        "\n",
        "Burada son seçeneği seçeceğiz ve **sadece 2 insan değerlendiricinin hemfikir olduğu örnekleri tutacağız**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AzTtSSxA6KOy"
      },
      "outputs": [],
      "source": [
        "# Sample examples\n",
        "ratings_where_raters_agree = ratings.loc[ratings[\"score_1\"] == ratings[\"score_2\"]]\n",
        "examples = ratings_where_raters_agree.groupby(\"score_1\").sample(7, random_state=1214)\n",
        "examples[\"human_score\"] = examples[\"score_1\"]\n",
        "\n",
        "# Visualize 1 sample for each score\n",
        "display(examples.groupby(\"human_score\").first())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zv5HHa66KOy"
      },
      "source": [
        "## 2. LLM yargıcımızın oluşturulması\n",
        "LLM yargıcımızı şu unsurları içeren temel bir prompt'la oluştururuz:\n",
        "- görev (task) açıklaması\n",
        "- ölçek (scale) açıklaması: `minimum`, `maximum`, value türleri (bu durumda `float`)\n",
        "- çıktı (output) formatının açıklaması\n",
        "- bir yanıtın başlangıcı (LLM'i olabildiğince yönlendirmek için)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtGc9eyU6KOy"
      },
      "outputs": [],
      "source": [
        "JUDGE_PROMPT = \"\"\"\n",
        "You will be given a user_question and system_answer couple.\n",
        "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
        "Give your answer as a float on a scale of 0 to 10, where 0 means that the system_answer is not helpful at all, and 10 means that the answer completely and helpfully addresses the question.\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "Feedback:::\n",
        "Total rating: (your rating, as a float between 0 and 10)\n",
        "\n",
        "Now here are the question and answer.\n",
        "\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\n",
        "Feedback:::\n",
        "Total rating: \"\"\"\n",
        "\n",
        "\n",
        "# JUDGE_PROMPT = \"\"\"\n",
        "# Sana bir kullanıcı_sorusu ve sistem_yanıtı çifti verilecek.\n",
        "# Senin görevin sistem_yanıtı'nın, kullanıcı_sorusu'nda ifade edilen kullanıcı endişelerine ne kadar iyi yanıt verdiğini değerlendiren bir \"toplam puan\" sağlamak.\n",
        "# Cevabını 0 ile 10 arasındaki bir ondalık sayı (float) olarak ver. 0, sistem_yanıtı'nın hiçbir şekilde yardımcı olmadığı anlamına gelirken, 10, cevabın soruyu tamamen ve yararlı bir şekilde yanıtladığı anlamına gelir.\n",
        "#\n",
        "# Geri bildirimi aşağıdaki formatta yap:\n",
        "#\n",
        "# Geri Bildirim:::\n",
        "# Toplam Puan: (0 ile 10 arasında bir ondalık sayı olarak puanınızı yazın)\n",
        "#\n",
        "# Şimdi, soru ve cevap aşağıda verilmiştir.\n",
        "#\n",
        "# Soru: {soru}\n",
        "# Cevap: {cevap}\n",
        "#\n",
        "# Geri Bildirim:::\n",
        "# Toplam  Puan: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GtEhwZ16KOy"
      },
      "outputs": [],
      "source": [
        "examples[\"llm_judge\"] = examples.progress_apply(\n",
        "    lambda x: llm_client.text_generation(\n",
        "        prompt=JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
        "        max_new_tokens=1000,\n",
        "    ),\n",
        "    axis=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5CYhbYC6KOy"
      },
      "outputs": [],
      "source": [
        "def extract_judge_score(answer: str, split_str: str = \"Total rating:\") -> int:\n",
        "    try:\n",
        "        if split_str in answer:\n",
        "            rating = answer.split(split_str)[1]\n",
        "        else:\n",
        "            rating = answer\n",
        "        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n",
        "        return float(digit_groups[0])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "\n",
        "examples[\"llm_judge_score\"] = examples[\"llm_judge\"].apply(extract_judge_score)\n",
        "# Rescale the score given by the LLM on the same scale as the human score\n",
        "examples[\"llm_judge_score\"] = (examples[\"llm_judge_score\"] / 10) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw-AH7ZS6KOy"
      },
      "outputs": [],
      "source": [
        "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
        "print(\n",
        "    f\"{examples['llm_judge_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFfBqgkz6KOz"
      },
      "source": [
        "\n",
        "Bu kötü değil çünkü rastgele ve bağımsız iki değişken arasındaki Pearson korelasyonu 0 olacaktır!\n",
        "\n",
        "Ama kolayca daha iyisini yapabiliriz. 🔝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyRpAJxW6KOz"
      },
      "source": [
        "## 3. LLM yargıcımızın geliştirilmesi\n",
        "\n",
        "[Aparna Dhinakaran](https://twitter.com/aparnadhinak/status/1748368364395721128) 'ın gösterdiği gibi LLM'ler sürekli aralıklardaki çıktıları değerlendirmede pek de başarılı değiller.  \n",
        "[Bu makale](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG), daha iyi bir prompt oluşturmak için birkaç iyi uygulama sunmaktadır:\n",
        "- ⏳ **Düşünmek için daha fazla zaman tanımak**, nihai yanıttan önce `Evaluation` alanını eklemek.\n",
        "- 🔢 **Küçük bir tam sayı (integer) ölçeği kullanmak**, önceki gibi büyük bir ondalık sayı (`float`) ölçeği yerine 1-4 ya da 1-5 kullanmak.\n",
        "- 👩‍🏫 **Rehberlik sağlamak için örnek bir ölçek sunmak.**.\n",
        "- Hatta LLM'i motive etme adına bir teşvik bile ekleyebiliriz!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMoi_zcp6KOz"
      },
      "outputs": [],
      "source": [
        "IMPROVED_JUDGE_PROMPT = \"\"\"\n",
        "You will be given a user_question and system_answer couple.\n",
        "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
        "Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n",
        "\n",
        "Here is the scale you should use to build your answer:\n",
        "1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n",
        "2: The system_answer is mostly not helpful: misses some key aspects of the question\n",
        "3: The system_answer is mostly helpful: provides support, but still could be improved\n",
        "4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "Feedback:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 4)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and answer.\n",
        "\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\n",
        "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
        "Feedback:::\n",
        "Evaluation: \"\"\"\n",
        "\n",
        "\n",
        "# IMPROVED_JUDGE_PROMPT = \"\"\"\n",
        "# Sana bir kullanıcı_sorusu ve sistem_yanıtı çifti verilecek.\n",
        "# Senin görevin, sistem_yanıtı'nın kullanıcı_sorusu'nda ifade edilen kullanıcı endişelerine ne kadar iyi yanıt verdiğini değerlendiren bir \"toplam puan\" sağlamak.\n",
        "# Cevabını 1 ile 4 arasında bir ölçekte ver, burada:\n",
        "# 1: sistem_yanıtı tamamen alakasız veya eksikse (berbat)\n",
        "# 2: sistem_yanıtı çoğunlukla yardımcı değilse, önemli noktaları kaçırıyorsa\n",
        "# 3: sistem_yanıtı çoğunlukla yardımcı oluyorsa, ancak iyileştirilebilecek noktalar varsa\n",
        "# 4: sistem_yanıtı mükemmelse; ilgili, doğrudan, ayrıntılı ve sorudaki tüm endişeleri ele alıyorsa\n",
        "#\n",
        "# Geri bildirimi aşağıdaki formatta yap:\n",
        "#\n",
        "# Geri Bildirim:::\n",
        "# Değerlendirme: (Verdiğiniz puanın gerekçesi, açıklama olarak)\n",
        "# Toplam Puan: (1 ile 4 arasında bir sayı olarak puanınız)\n",
        "#\n",
        "# Cevabın 'Değerlendirme:' ve 'Toplam Puan:' değerlerini MUTLAKA içermeli.\n",
        "#\n",
        "# Şimdi, soru ve cevap aşağıda verilmiştir.\n",
        "#\n",
        "# Soru: {soru}\n",
        "# Cevap: {cevap}\n",
        "#\n",
        "# Geri bildirimi gerçekleştir. Eğer doğru bir puan verirsen sana 100 adet H100 GPU vererek yapay zeka şirketini kurmana yardımcı olacağım.\n",
        "# Geri Bildirim:::\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33P-gksz6KOz"
      },
      "outputs": [],
      "source": [
        "examples[\"llm_judge_improved\"] = examples.progress_apply(\n",
        "    lambda x: llm_client.text_generation(\n",
        "        prompt=IMPROVED_JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
        "        max_new_tokens=500,\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "examples[\"llm_judge_improved_score\"] = examples[\"llm_judge_improved\"].apply(\n",
        "    extract_judge_score\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVfu9ZRr6KOz"
      },
      "outputs": [],
      "source": [
        "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
        "print(\n",
        "    f\"{examples['llm_judge_improved_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u96zcKju6KOz"
      },
      "source": [
        "Korelasyon yalnızca prompt'ta yapılan birkaç düzenleme ile **yaklaşık %30 oranında** iyileştirildi (bunların birkaç puanlık kısmı LLM'e utanmazca verdiğim ipucum sayesinde, ki bunun yasal olarak bağlayıcı olmadığını burada açıkça beyan ediyorum.)\n",
        "\n",
        "Oldukça etkileyici! 👏\n",
        "\n",
        "Şimdi LLM yargıcımızın yaptığı bazı hataları analiz etmek için inceleyelim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RStTlHbf6KOz"
      },
      "outputs": [],
      "source": [
        "errors = pd.concat(\n",
        "    [\n",
        "        examples.loc[\n",
        "            examples[\"llm_judge_improved_score\"] > examples[\"human_score\"]\n",
        "        ].head(1),\n",
        "        examples.loc[\n",
        "            examples[\"llm_judge_improved_score\"] < examples[\"human_score\"]\n",
        "        ].head(2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "display(\n",
        "    errors[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"human_score\",\n",
        "            \"explanation_1\",\n",
        "            \"llm_judge_improved_score\",\n",
        "            \"llm_judge_improved\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHAHdR_n6KOz"
      },
      "source": [
        "Tartışmalar önemsiz: Genel olarak sistemimiz için iyi bir performans seviyesine ulaşmış görünüyoruz!\n",
        "\n",
        "## 4. LLM yargıcımızı nasıl daha ileriye taşıyabiliriz?\n",
        "\n",
        "🎯 **Asla %100'e ulaşamayacaksınız:** Öncelikle gerçek referans değerinin şüphesiz biraz gürültü içerdiğini not edelim o yüzden mükemmel bir LLM yargıcı olsa dahi hemfikirlilik/korelasyon asla %100'e ulaşmayacaktır.\n",
        "\n",
        "🧭 **Bir referans sağlayın:** Her soru için bir referans cevabına erişiminiz varsa daha iyi sonuçlar almak için kesinlikle Yargıç LLM'in prompt'una bunu dahil etmelisiniz!\n",
        "\n",
        "▶️ **Few-shot örnekleri sağlayın:** Soruların bazı few-shot örneklerini ve gerçek referans değeri değerlendirmelerini prompt'a eklemek sonuçları iyileştirebilir. _(Burada denedim, bu durumda sonuçları iyileştirmediği için atladım ama sizin veri setiniz için işe yarayabilir!)_\n",
        "\n",
        "\n",
        "➕ **Eklemeli (additive) ölçek:** Değerlendirme atomik kriterlere ayrılabiliyorsa eklemeli bir ölçek kullanmak sonuçları daha da iyileştirebilir: aşağıya bakın 👇\n",
        "```python\n",
        "\n",
        "ADDITIVE_PROMPT = \"\"\"\n",
        "(...)\n",
        "- Award 1 point if the answer is related to the question.\n",
        "- Give 1 additional point if the answer is clear and precise.\n",
        "- Provide 1 further point if the answer is true.\n",
        "- One final point should be awarded if the answer provides additional resources to support the user.\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "# ADDITIVE_PROMPT = \"\"\"\n",
        "# (...)\n",
        "# - Cevap soruyla ilgiliyse 1 puan verin.\n",
        "# - Cevap net ve kesin ise 1 ek puan daha verin.\n",
        "# - Cevap doğruysa 1 puan daha verin.\n",
        "# - Son olarak cevap kullanıcının yararlanabileceği ek kaynaklar sağlıyorsa 1 puan daha verin.\n",
        "# ...\n",
        "# \"\"\"\n",
        "```\n",
        "\n",
        "**Yapılandırılmış üretim (structured generation) ile uygulayın:**\n",
        "\n",
        "Yapılandırılmış üretim kullanarak LLM yargıcını `Evaluation` ve `Total rating` alanlarına sahip bir JSON formatında doğrudan çıktı verecek şekilde yapılandırabilirsiniz. Bu, çıktıyı ayrıştırmayı kolaylaştırır: daha fazla bilgi için [structured generation](structured_generation) rehberimize göz atın!\n",
        "\n",
        "## Sonuç\n",
        "\n",
        "\n",
        "Bugünlük bu kadar, buraya kadar takip ettiğiniz için tebrikler! 🥳\n",
        "\n",
        "Sizden ayrılmak zorundayım, bazı tuhaf tipler kapımı çalıyor, Mixtral adına H100 toplamak için geldiklerini iddia ediyorlar. 🤔"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}