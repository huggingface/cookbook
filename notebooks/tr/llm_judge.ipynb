{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM'i otomatik ve Ã§ok yÃ¶nlÃ¼ deÄŸerlendirmek iÃ§in bir yargÄ±Ã§ gibi ðŸ§‘â€âš–ï¸ kullanmak\n",
        "_Yazan: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "BÃ¼yÃ¼k dil modellerinin (LLM'lerin) deÄŸerlendirilmesi genellikle zorlu bir iÅŸtir; geniÅŸ yetenekleri gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, onlara verilen gÃ¶revler Ã§oÄŸu zaman kapsamlÄ± ve kesin sÄ±nÄ±rlarla belirlenmemiÅŸ gereksinimlere gÃ¶re deÄŸerlendirilmelidir. Ã–rneÄŸin, bir asistanÄ±n bir soruya verdiÄŸi yanÄ±t ÅŸu ÅŸekilde olabilir:\n",
        "\n",
        "Ã–rneÄŸin, bir ajanÄ±n bir soruya verdiÄŸi yanÄ±t ÅŸu ÅŸekilde olabilir:\n",
        "- BaÄŸlama dayalÄ± olmamasÄ±\n",
        "- TekrarlayÄ±cÄ± olmasÄ± (tekrar, tekrar, tekrarlayan)\n",
        "- Dil bilgisi hatalarÄ± iÃ§ermesi\n",
        "- GereÄŸinden fazla uzun olmasÄ± ve kelime fazlalÄ±ÄŸÄ± iÃ§ermesi (sÃ¶ylemin veya yazÄ±lÄ± iÃ§eriÄŸin gereksiz derecede ayrÄ±ntÄ±lÄ± ve uzatÄ±lmÄ±ÅŸ bir hale gelmesi)\n",
        "- TutarsÄ±z olmasÄ±\n",
        "- ...\n",
        "\n",
        "Kriterlerin listesi uzayÄ±p gider. Ãœstelik sÄ±nÄ±rlÄ± bir listeye sahip olsak bile bunlarÄ±n her birini Ã¶lÃ§mek zordur: \"Ã§Ä±ktÄ±larÄ± (output'larÄ±) deÄŸerlendirmek iÃ§in kural tabanlÄ± bir program geliÅŸtirmek fazlasÄ±yla zordur. Ã‡Ä±ktÄ±lar ve referans yanÄ±tlar arasÄ±ndaki benzerliÄŸe dayanan geleneksel deÄŸerlendirme teknikleri de (Ã¶rneÄŸin ROUGE, BLEU gibi) bu tÃ¼r sorular iÃ§in etkisiz kalmaktadÄ±r.\"\n",
        "\n",
        "âœ… Ã‡Ä±ktÄ±larÄ± maliyetli insan zamanÄ± gerektirmeden deÄŸerlendirmek iÃ§in gÃ¼Ã§lÃ¼ bir Ã§Ã¶zÃ¼m, LLM'in bir yargÄ±Ã§ olmasÄ±dÄ±r. Bu yÃ¶ntem [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://huggingface.co/papers/2306.05685)  makalesinde tanÄ±tÄ±lmÄ±ÅŸtÄ±r, okumanÄ±zÄ± tavsiye ediyorum.\n",
        "\n",
        "ðŸ’¡ Fikir basit: bir LLM'den sizin iÃ§in deÄŸerlendirme yapmasÄ±nÄ± isteyin.ðŸ¤–âœ“\n",
        "\n",
        "Ancak bunu doÄŸrudan kullandÄ±ÄŸÄ±mÄ±zda iyi Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼rÃ¼z: iyi sonuÃ§lar almak iÃ§in dikkatle ayarlamanÄ±z gerekir."
      ],
      "metadata": {
        "id": "A8Eu8jAV7Fhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxmX0rIS6KOx"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub datasets pandas tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5HpVR9N6KOx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import InferenceClient, notebook_login\n",
        "\n",
        "# tqdm'in pandas desteÄŸini yÃ¼kleyin\n",
        "tqdm.pandas()  # load tqdm's pandas support\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzV9qDlt6KOx"
      },
      "outputs": [],
      "source": [
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(\n",
        "    model=repo_id,\n",
        "    timeout=120,\n",
        ")\n",
        "\n",
        "# Test your LLM client\n",
        "# LLM Client'i test edin\n",
        "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b4pahs6KOy"
      },
      "source": [
        "## 1. LLM yargÄ±cÄ±mÄ±zÄ±n oluÅŸturulmasÄ±nÄ±n ve deÄŸerlendirilmesinin hazÄ±rlanmasÄ±\n",
        "\n",
        "Diyelim ki bir LLM'e spesifik, aÃ§Ä±k-uÃ§lu sorularÄ± yanÄ±tlamak gibi bir gÃ¶rev vermek istiyorsunuz.\n",
        "\n",
        "Bunun zorluÄŸu, tÄ±pkÄ± yukarÄ±da aÃ§Ä±kladÄ±ÄŸÄ±mÄ±z gibi yanÄ±tÄ±n kalitesinin Ã¶lÃ§Ã¼lmesinin zor olmasÄ±ndandÄ±r, Ã¶rneÄŸin tam bir string eÅŸleÅŸmesini doÄŸru iÅŸaretleyecektir ancak farklÄ± ÅŸekilde ifade edilen yanÄ±tlarÄ± yanlÄ±ÅŸ iÅŸaretleyecektir.\n",
        "\n",
        "Ã‡Ä±ktÄ±larÄ± deÄŸerlendirmek iÃ§in insan etiketleyiciler kullanabilirsiniz ancak bu insanlar iÃ§in Ã§ok zaman alÄ±cÄ±dÄ±r ve modeli veya sorularÄ± gÃ¼ncellemek isterseniz her ÅŸeyi yeniden yapmanÄ±z gerekir.\n",
        "\n",
        "âœ… Bu durumda LLM'i bir yargÄ±Ã§ gibi kurabilirsiniz.\n",
        "\n",
        "**Ancak bir LLM'i bir yargÄ±Ã§ gibi kullanmak iÃ§in Ã¶ncelikle modelinizin Ã§Ä±ktÄ±larÄ±nÄ±n ne kadar gÃ¼venilir derecelendirdiÄŸini deÄŸerlendirmelisiniz.**\n",
        "\n",
        "âž¡ï¸ Bu yÃ¼zden ilk adÄ±m... Ä°nsan deÄŸerlendirme veri seti oluÅŸturmak olacaktÄ±r. Ancak sadece birkaÃ§ Ã¶rnek iÃ§in insan anotasyonlarÄ± alabilirsiniz â€“ yaklaÅŸÄ±k 30 Ã¶rnek, performans hakkÄ±nda iyi bir fikir edinmek iÃ§in yeterlidir ve bu veri setini LLM'i bir yargÄ±Ã§ gibi test etmek istediÄŸiniz her seferinde yeniden kullanabilirsiniz.\n",
        "\n",
        "Bizim Ã§alÄ±ÅŸmamÄ±zda her soru/cevap Ã§ifti iÃ§in 2 insan deÄŸerlendirmesi ve skoru iÃ§eren [feedbackQA](https://huggingface.co/datasets/McGill-NLP/feedbackQA) veri setini kullanacaÄŸÄ±z. 30 Ã¶rnekten oluÅŸan bir Ã¶rneklem kullanmak kÃ¼Ã§Ã¼k deÄŸerlendirme veri setinizin nasÄ±l olabileceÄŸini temsil edecektir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECKdFnW56KOy"
      },
      "outputs": [],
      "source": [
        "ratings = load_dataset(\"McGill-NLP/feedbackQA\")[\"train\"]\n",
        "ratings = pd.DataFrame(ratings)\n",
        "\n",
        "ratings[\"review_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][0])\n",
        "ratings[\"explanation_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][0])\n",
        "ratings[\"review_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][1])\n",
        "ratings[\"explanation_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][1])\n",
        "ratings = ratings.drop(columns=[\"feedback\"])\n",
        "\n",
        "# Map scores to numeric values\n",
        "# SkorlarÄ± numerik deÄŸerlere map'leyin\n",
        "conversion_dict = {\"Excellent\": 4, \"Acceptable\": 3, \"Could be Improved\": 2, \"Bad\": 1}\n",
        "ratings[\"score_1\"] = ratings[\"review_1\"].map(conversion_dict)\n",
        "ratings[\"score_2\"] = ratings[\"review_2\"].map(conversion_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnhjwN7R6KOy"
      },
      "source": [
        "Performans iÃ§in bir temel (baseline) oluÅŸturmak her zaman iyi bir fikirdir: Ã–rneÄŸin burada bu temel, iki insan deÄŸerlendiricinin verdiÄŸi skorlar arasÄ±ndaki uyum olabilir ve bu [Pearson korelasyonu](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) ile Ã¶lÃ§Ã¼lebilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91PaHzb-6KOy"
      },
      "outputs": [],
      "source": [
        "print(\"Correlation between 2 human raters:\") # Ä°ki insan deÄŸerlendirici arasÄ±ndaki korelasyon\n",
        "print(f\"{ratings['score_1'].corr(ratings['score_2'], method='pearson'):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5oV7P-b6KOy"
      },
      "source": [
        "2 insan deÄŸerlendiricinin korelasyonu o kadar da iyi gelmedi. Ä°nsan deÄŸerlendirmeleriniz gerÃ§ekten kÃ¶tÃ¼yse muhtemelen deÄŸerlendirme kriterlerinin yeterince aÃ§Ä±k olmadÄ±ÄŸÄ± anlamÄ±na gelir.\n",
        "\n",
        "Bu, \"ground truth\" yani gerÃ§ek referans deÄŸerinin gÃ¼rÃ¼ltÃ¼ iÃ§erdiÄŸi anlamÄ±na gelir: dolayÄ±sÄ±yla hiÃ§bir algoritmik deÄŸerlendirmenin buna bu kadar yakÄ±n olmasÄ±nÄ± bekleyemeyiz.\n",
        "\n",
        "Ancak bu gÃ¼rÃ¼ltÃ¼yÃ¼ ÅŸu ÅŸekilde azaltabiliriz:\n",
        "- herhangi bir tek skor yerine ortalama skoru gerÃ§ek referans deÄŸeri olarak alarak bazÄ± dÃ¼zensizlikleri dÃ¼zeltebiliriz.\n",
        "- yalnÄ±zca insan deÄŸerlendiricilerin hemfikir olduÄŸu Ã¶rnekleri seÃ§ebiliriz.\n",
        "\n",
        "Burada son seÃ§eneÄŸi seÃ§eceÄŸiz ve **sadece 2 insan deÄŸerlendiricinin hemfikir olduÄŸu Ã¶rnekleri tutacaÄŸÄ±z**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AzTtSSxA6KOy"
      },
      "outputs": [],
      "source": [
        "# Sample examples\n",
        "ratings_where_raters_agree = ratings.loc[ratings[\"score_1\"] == ratings[\"score_2\"]]\n",
        "examples = ratings_where_raters_agree.groupby(\"score_1\").sample(7, random_state=1214)\n",
        "examples[\"human_score\"] = examples[\"score_1\"]\n",
        "\n",
        "# Visualize 1 sample for each score\n",
        "display(examples.groupby(\"human_score\").first())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zv5HHa66KOy"
      },
      "source": [
        "## 2. LLM yargÄ±cÄ±mÄ±zÄ±n oluÅŸturulmasÄ±\n",
        "LLM yargÄ±cÄ±mÄ±zÄ± ÅŸu unsurlarÄ± iÃ§eren temel bir prompt'la oluÅŸtururuz:\n",
        "- gÃ¶rev (task) aÃ§Ä±klamasÄ±\n",
        "- Ã¶lÃ§ek (scale) aÃ§Ä±klamasÄ±: `minimum`, `maximum`, value tÃ¼rleri (bu durumda `float`)\n",
        "- Ã§Ä±ktÄ± (output) formatÄ±nÄ±n aÃ§Ä±klamasÄ±\n",
        "- bir yanÄ±tÄ±n baÅŸlangÄ±cÄ± (LLM'i olabildiÄŸince yÃ¶nlendirmek iÃ§in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtGc9eyU6KOy"
      },
      "outputs": [],
      "source": [
        "JUDGE_PROMPT = \"\"\"\n",
        "You will be given a user_question and system_answer couple.\n",
        "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
        "Give your answer as a float on a scale of 0 to 10, where 0 means that the system_answer is not helpful at all, and 10 means that the answer completely and helpfully addresses the question.\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "Feedback:::\n",
        "Total rating: (your rating, as a float between 0 and 10)\n",
        "\n",
        "Now here are the question and answer.\n",
        "\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\n",
        "Feedback:::\n",
        "Total rating: \"\"\"\n",
        "\n",
        "\n",
        "# JUDGE_PROMPT = \"\"\"\n",
        "# Sana bir kullanÄ±cÄ±_sorusu ve sistem_yanÄ±tÄ± Ã§ifti verilecek.\n",
        "# Senin gÃ¶revin sistem_yanÄ±tÄ±'nÄ±n, kullanÄ±cÄ±_sorusu'nda ifade edilen kullanÄ±cÄ± endiÅŸelerine ne kadar iyi yanÄ±t verdiÄŸini deÄŸerlendiren bir \"toplam puan\" saÄŸlamak.\n",
        "# CevabÄ±nÄ± 0 ile 10 arasÄ±ndaki bir ondalÄ±k sayÄ± (float) olarak ver. 0, sistem_yanÄ±tÄ±'nÄ±n hiÃ§bir ÅŸekilde yardÄ±mcÄ± olmadÄ±ÄŸÄ± anlamÄ±na gelirken, 10, cevabÄ±n soruyu tamamen ve yararlÄ± bir ÅŸekilde yanÄ±tladÄ±ÄŸÄ± anlamÄ±na gelir.\n",
        "#\n",
        "# Geri bildirimi aÅŸaÄŸÄ±daki formatta yap:\n",
        "#\n",
        "# Geri Bildirim:::\n",
        "# Toplam Puan: (0 ile 10 arasÄ±nda bir ondalÄ±k sayÄ± olarak puanÄ±nÄ±zÄ± yazÄ±n)\n",
        "#\n",
        "# Åžimdi, soru ve cevap aÅŸaÄŸÄ±da verilmiÅŸtir.\n",
        "#\n",
        "# Soru: {soru}\n",
        "# Cevap: {cevap}\n",
        "#\n",
        "# Geri Bildirim:::\n",
        "# Toplam  Puan: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GtEhwZ16KOy"
      },
      "outputs": [],
      "source": [
        "examples[\"llm_judge\"] = examples.progress_apply(\n",
        "    lambda x: llm_client.text_generation(\n",
        "        prompt=JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
        "        max_new_tokens=1000,\n",
        "    ),\n",
        "    axis=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5CYhbYC6KOy"
      },
      "outputs": [],
      "source": [
        "def extract_judge_score(answer: str, split_str: str = \"Total rating:\") -> int:\n",
        "    try:\n",
        "        if split_str in answer:\n",
        "            rating = answer.split(split_str)[1]\n",
        "        else:\n",
        "            rating = answer\n",
        "        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n",
        "        return float(digit_groups[0])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "\n",
        "examples[\"llm_judge_score\"] = examples[\"llm_judge\"].apply(extract_judge_score)\n",
        "# Rescale the score given by the LLM on the same scale as the human score\n",
        "examples[\"llm_judge_score\"] = (examples[\"llm_judge_score\"] / 10) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw-AH7ZS6KOy"
      },
      "outputs": [],
      "source": [
        "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
        "print(\n",
        "    f\"{examples['llm_judge_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFfBqgkz6KOz"
      },
      "source": [
        "\n",
        "Bu kÃ¶tÃ¼ deÄŸil Ã§Ã¼nkÃ¼ rastgele ve baÄŸÄ±msÄ±z iki deÄŸiÅŸken arasÄ±ndaki Pearson korelasyonu 0 olacaktÄ±r!\n",
        "\n",
        "Ama kolayca daha iyisini yapabiliriz. ðŸ”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyRpAJxW6KOz"
      },
      "source": [
        "## 3. LLM yargÄ±cÄ±mÄ±zÄ±n geliÅŸtirilmesi\n",
        "\n",
        "[Aparna Dhinakaran](https://twitter.com/aparnadhinak/status/1748368364395721128) 'Ä±n gÃ¶sterdiÄŸi gibi LLM'ler sÃ¼rekli aralÄ±klardaki Ã§Ä±ktÄ±larÄ± deÄŸerlendirmede pek de baÅŸarÄ±lÄ± deÄŸiller.  \n",
        "[Bu makale](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG), daha iyi bir prompt oluÅŸturmak iÃ§in birkaÃ§ iyi uygulama sunmaktadÄ±r:\n",
        "- â³ **DÃ¼ÅŸÃ¼nmek iÃ§in daha fazla zaman tanÄ±mak**, nihai yanÄ±ttan Ã¶nce `Evaluation` alanÄ±nÄ± eklemek.\n",
        "- ðŸ”¢ **KÃ¼Ã§Ã¼k bir tam sayÄ± (integer) Ã¶lÃ§eÄŸi kullanmak**, Ã¶nceki gibi bÃ¼yÃ¼k bir ondalÄ±k sayÄ± (`float`) Ã¶lÃ§eÄŸi yerine 1-4 ya da 1-5 kullanmak.\n",
        "- ðŸ‘©â€ðŸ« **Rehberlik saÄŸlamak iÃ§in Ã¶rnek bir Ã¶lÃ§ek sunmak.**.\n",
        "- Hatta LLM'i motive etme adÄ±na bir teÅŸvik bile ekleyebiliriz!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMoi_zcp6KOz"
      },
      "outputs": [],
      "source": [
        "IMPROVED_JUDGE_PROMPT = \"\"\"\n",
        "You will be given a user_question and system_answer couple.\n",
        "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
        "Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n",
        "\n",
        "Here is the scale you should use to build your answer:\n",
        "1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n",
        "2: The system_answer is mostly not helpful: misses some key aspects of the question\n",
        "3: The system_answer is mostly helpful: provides support, but still could be improved\n",
        "4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "Feedback:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 4)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and answer.\n",
        "\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\n",
        "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
        "Feedback:::\n",
        "Evaluation: \"\"\"\n",
        "\n",
        "\n",
        "# IMPROVED_JUDGE_PROMPT = \"\"\"\n",
        "# Sana bir kullanÄ±cÄ±_sorusu ve sistem_yanÄ±tÄ± Ã§ifti verilecek.\n",
        "# Senin gÃ¶revin, sistem_yanÄ±tÄ±'nÄ±n kullanÄ±cÄ±_sorusu'nda ifade edilen kullanÄ±cÄ± endiÅŸelerine ne kadar iyi yanÄ±t verdiÄŸini deÄŸerlendiren bir \"toplam puan\" saÄŸlamak.\n",
        "# CevabÄ±nÄ± 1 ile 4 arasÄ±nda bir Ã¶lÃ§ekte ver, burada:\n",
        "# 1: sistem_yanÄ±tÄ± tamamen alakasÄ±z veya eksikse (berbat)\n",
        "# 2: sistem_yanÄ±tÄ± Ã§oÄŸunlukla yardÄ±mcÄ± deÄŸilse, Ã¶nemli noktalarÄ± kaÃ§Ä±rÄ±yorsa\n",
        "# 3: sistem_yanÄ±tÄ± Ã§oÄŸunlukla yardÄ±mcÄ± oluyorsa, ancak iyileÅŸtirilebilecek noktalar varsa\n",
        "# 4: sistem_yanÄ±tÄ± mÃ¼kemmelse; ilgili, doÄŸrudan, ayrÄ±ntÄ±lÄ± ve sorudaki tÃ¼m endiÅŸeleri ele alÄ±yorsa\n",
        "#\n",
        "# Geri bildirimi aÅŸaÄŸÄ±daki formatta yap:\n",
        "#\n",
        "# Geri Bildirim:::\n",
        "# DeÄŸerlendirme: (VerdiÄŸiniz puanÄ±n gerekÃ§esi, aÃ§Ä±klama olarak)\n",
        "# Toplam Puan: (1 ile 4 arasÄ±nda bir sayÄ± olarak puanÄ±nÄ±z)\n",
        "#\n",
        "# CevabÄ±n 'DeÄŸerlendirme:' ve 'Toplam Puan:' deÄŸerlerini MUTLAKA iÃ§ermeli.\n",
        "#\n",
        "# Åžimdi, soru ve cevap aÅŸaÄŸÄ±da verilmiÅŸtir.\n",
        "#\n",
        "# Soru: {soru}\n",
        "# Cevap: {cevap}\n",
        "#\n",
        "# Geri bildirimi gerÃ§ekleÅŸtir. EÄŸer doÄŸru bir puan verirsen sana 100 adet H100 GPU vererek yapay zeka ÅŸirketini kurmana yardÄ±mcÄ± olacaÄŸÄ±m.\n",
        "# Geri Bildirim:::\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33P-gksz6KOz"
      },
      "outputs": [],
      "source": [
        "examples[\"llm_judge_improved\"] = examples.progress_apply(\n",
        "    lambda x: llm_client.text_generation(\n",
        "        prompt=IMPROVED_JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
        "        max_new_tokens=500,\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "examples[\"llm_judge_improved_score\"] = examples[\"llm_judge_improved\"].apply(\n",
        "    extract_judge_score\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVfu9ZRr6KOz"
      },
      "outputs": [],
      "source": [
        "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
        "print(\n",
        "    f\"{examples['llm_judge_improved_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u96zcKju6KOz"
      },
      "source": [
        "Korelasyon yalnÄ±zca prompt'ta yapÄ±lan birkaÃ§ dÃ¼zenleme ile **yaklaÅŸÄ±k %30 oranÄ±nda** iyileÅŸtirildi (bunlarÄ±n birkaÃ§ puanlÄ±k kÄ±smÄ± LLM'e utanmazca verdiÄŸim ipucum sayesinde, ki bunun yasal olarak baÄŸlayÄ±cÄ± olmadÄ±ÄŸÄ±nÄ± burada aÃ§Ä±kÃ§a beyan ediyorum.)\n",
        "\n",
        "OldukÃ§a etkileyici! ðŸ‘\n",
        "\n",
        "Åžimdi LLM yargÄ±cÄ±mÄ±zÄ±n yaptÄ±ÄŸÄ± bazÄ± hatalarÄ± analiz etmek iÃ§in inceleyelim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RStTlHbf6KOz"
      },
      "outputs": [],
      "source": [
        "errors = pd.concat(\n",
        "    [\n",
        "        examples.loc[\n",
        "            examples[\"llm_judge_improved_score\"] > examples[\"human_score\"]\n",
        "        ].head(1),\n",
        "        examples.loc[\n",
        "            examples[\"llm_judge_improved_score\"] < examples[\"human_score\"]\n",
        "        ].head(2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "display(\n",
        "    errors[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"human_score\",\n",
        "            \"explanation_1\",\n",
        "            \"llm_judge_improved_score\",\n",
        "            \"llm_judge_improved\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHAHdR_n6KOz"
      },
      "source": [
        "TartÄ±ÅŸmalar Ã¶nemsiz: Genel olarak sistemimiz iÃ§in iyi bir performans seviyesine ulaÅŸmÄ±ÅŸ gÃ¶rÃ¼nÃ¼yoruz!\n",
        "\n",
        "## 4. LLM yargÄ±cÄ±mÄ±zÄ± nasÄ±l daha ileriye taÅŸÄ±yabiliriz?\n",
        "\n",
        "ðŸŽ¯ **Asla %100'e ulaÅŸamayacaksÄ±nÄ±z:** Ã–ncelikle gerÃ§ek referans deÄŸerinin ÅŸÃ¼phesiz biraz gÃ¼rÃ¼ltÃ¼ iÃ§erdiÄŸini not edelim o yÃ¼zden mÃ¼kemmel bir LLM yargÄ±cÄ± olsa dahi hemfikirlilik/korelasyon asla %100'e ulaÅŸmayacaktÄ±r.\n",
        "\n",
        "ðŸ§­ **Bir referans saÄŸlayÄ±n:** Her soru iÃ§in bir referans cevabÄ±na eriÅŸiminiz varsa daha iyi sonuÃ§lar almak iÃ§in kesinlikle YargÄ±Ã§ LLM'in prompt'una bunu dahil etmelisiniz!\n",
        "\n",
        "â–¶ï¸ **Few-shot Ã¶rnekleri saÄŸlayÄ±n:** SorularÄ±n bazÄ± few-shot Ã¶rneklerini ve gerÃ§ek referans deÄŸeri deÄŸerlendirmelerini prompt'a eklemek sonuÃ§larÄ± iyileÅŸtirebilir. _(Burada denedim, bu durumda sonuÃ§larÄ± iyileÅŸtirmediÄŸi iÃ§in atladÄ±m ama sizin veri setiniz iÃ§in iÅŸe yarayabilir!)_\n",
        "\n",
        "\n",
        "âž• **Eklemeli (additive) Ã¶lÃ§ek:** DeÄŸerlendirme atomik kriterlere ayrÄ±labiliyorsa eklemeli bir Ã¶lÃ§ek kullanmak sonuÃ§larÄ± daha da iyileÅŸtirebilir: aÅŸaÄŸÄ±ya bakÄ±n ðŸ‘‡\n",
        "```python\n",
        "\n",
        "ADDITIVE_PROMPT = \"\"\"\n",
        "(...)\n",
        "- Award 1 point if the answer is related to the question.\n",
        "- Give 1 additional point if the answer is clear and precise.\n",
        "- Provide 1 further point if the answer is true.\n",
        "- One final point should be awarded if the answer provides additional resources to support the user.\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "# ADDITIVE_PROMPT = \"\"\"\n",
        "# (...)\n",
        "# - Cevap soruyla ilgiliyse 1 puan verin.\n",
        "# - Cevap net ve kesin ise 1 ek puan daha verin.\n",
        "# - Cevap doÄŸruysa 1 puan daha verin.\n",
        "# - Son olarak cevap kullanÄ±cÄ±nÄ±n yararlanabileceÄŸi ek kaynaklar saÄŸlÄ±yorsa 1 puan daha verin.\n",
        "# ...\n",
        "# \"\"\"\n",
        "```\n",
        "\n",
        "**YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã¼retim (structured generation) ile uygulayÄ±n:**\n",
        "\n",
        "YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã¼retim kullanarak LLM yargÄ±cÄ±nÄ± `Evaluation` ve `Total rating` alanlarÄ±na sahip bir JSON formatÄ±nda doÄŸrudan Ã§Ä±ktÄ± verecek ÅŸekilde yapÄ±landÄ±rabilirsiniz. Bu, Ã§Ä±ktÄ±yÄ± ayrÄ±ÅŸtÄ±rmayÄ± kolaylaÅŸtÄ±rÄ±r: daha fazla bilgi iÃ§in [structured generation](structured_generation) rehberimize gÃ¶z atÄ±n!\n",
        "\n",
        "## SonuÃ§\n",
        "\n",
        "\n",
        "BugÃ¼nlÃ¼k bu kadar, buraya kadar takip ettiÄŸiniz iÃ§in tebrikler! ðŸ¥³\n",
        "\n",
        "Sizden ayrÄ±lmak zorundayÄ±m, bazÄ± tuhaf tipler kapÄ±mÄ± Ã§alÄ±yor, Mixtral adÄ±na H100 toplamak iÃ§in geldiklerini iddia ediyorlar. ðŸ¤”"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}