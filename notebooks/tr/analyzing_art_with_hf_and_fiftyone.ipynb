{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal GÃ¶mme ile Sanatsal Stillerin Analizi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Yazar: [Jacob Marks](https://huggingface.co/jamarks)*\n",
    "*Ã‡evirmen: [Aylin GÃ¼mÃ¼ÅŸ](https://huggingface.co/aylingumus)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Art Analysis Cover Image](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_cover_image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GÃ¶rÃ¼ntÃ¼ler gibi gÃ¶rsel veriler bilgi aÃ§Ä±sÄ±ndan inanÄ±lmaz derecede zengindir, ancak yapÄ±sal olmayan doÄŸasÄ± gereÄŸi analiz etmesi zordur.\n",
    "\n",
    "Bu not defterinde, multimodal embedding yÃ¶ntemlerini ve hesaplanan Ã¶znitelikleri kullanarak resimlerdeki sanatsal stillerin nasÄ±l analiz edileceÄŸini keÅŸfedeceÄŸiz. Veri analizi ve gÃ¶rselleÅŸtirme iÃ§in kullanmak Ã¼zere FiftyOne'a yÃ¼kleyeceÄŸimiz ğŸ¤— Hub'dan [WikiArt veri kÃ¼mesini](https://huggingface.co/datasets/huggan/wikiart) kullanacaÄŸÄ±z. Verilere birÃ§ok farklÄ± aÃ§Ä±dan gÃ¶z atacaÄŸÄ±z:\n",
    "\n",
    "- **GÃ¶rÃ¼ntÃ¼ BenzerliÄŸi Arama ve Anlamsal (Semantik) Arama**: ğŸ¤— Transformers'tan Ã¶n-eÄŸitimli [CLIP](https://huggingface.co/openai/clip-vit-base-patch32) modelini kullanarak multimodal embedding'leri Ã¼reteceÄŸiz ve yapÄ±sal olmayan aramalara izin vermek iÃ§in veriyi indeksleyeceÄŸiz.\n",
    "\n",
    "- **KÃ¼meleme ve GÃ¶rselleÅŸtirme**: Embedding'leri kullanarak gÃ¶rÃ¼ntÃ¼leri sanatsal stillerine gÃ¶re kÃ¼meleyeceÄŸiz ve UMAP boyut indirgeme yÃ¶ntemini kullanarak sonuÃ§larÄ± gÃ¶rselleÅŸtireceÄŸiz.\n",
    "\n",
    "- **Benzersizlik Analizi**: Veri kÃ¼mesindeki diÄŸer gÃ¶rÃ¼ntÃ¼lere ne kadar benzediÄŸine baÄŸlÄ± olarak her gÃ¶rÃ¼ntÃ¼ iÃ§in benzersizlik skoru atamak iÃ§in embedding'lerimizi kullanacaÄŸÄ±z.\n",
    "\n",
    "- **GÃ¶rÃ¼ntÃ¼ Kalitesi Analizi**: Her bir gÃ¶rÃ¼ntÃ¼ iÃ§in parlaklÄ±k, kontrast ve doygunluk gibi gÃ¶rÃ¼ntÃ¼ kalite metriklerini hesaplayacaÄŸÄ±z ve bu metriklerin, gÃ¶rÃ¼ntÃ¼lerin sanatsal stili ile nasÄ±l iliÅŸkili olduÄŸunu gÃ¶receÄŸiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haydi baÅŸlayalÄ±m! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu not defterini Ã§alÄ±ÅŸtÄ±rmak iÃ§in, aÅŸaÄŸÄ±daki kÃ¼tÃ¼phaneleri yÃ¼klemelisiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers huggingface_hub fiftyone umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ä°ndirmeleri Ä±ÅŸÄ±k hÄ±zÄ±nda yapmak iÃ§in [HF Transfer](https://pypi.org/project/hf-transfer/) yÃ¼kleyin:\n",
    "\n",
    "```bash\n",
    "pip install hf-transfer\n",
    "```\n",
    "\n",
    "Ve `HF_HUB_ENABLE_HF_TRANSFER` ortam deÄŸiÅŸkenini ayarlayarak etkinleÅŸtirin:\n",
    "\n",
    "```bash\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Not:</b> Bu not defteri <code>transformers==4.40.0</code>, <code>huggingface_hub==0.22.2</code> ve <code>fiftyone==0.23.8</code> ile test edilmiÅŸtir.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Åimdi bu not defteri iÃ§in gerekli olan modÃ¼lleri iÃ§e aktaralÄ±m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo # base library and app\n",
    "import fiftyone.zoo as foz # zoo datasets and models\n",
    "import fiftyone.brain as fob # ML routines\n",
    "from fiftyone import ViewField as F # for defining custom views\n",
    "import fiftyone.utils.huggingface as fouh # for loading datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hub'dan ğŸ¤— FiftyOne'a WikiArt veri kÃ¼mesini yÃ¼kleyerek baÅŸlayacaÄŸÄ±z. Bu veri kÃ¼mesi Hugging Face'in `datasets` kÃ¼tÃ¼phanesi aracÄ±lÄ±ÄŸÄ±yla da yÃ¼klenebilir, ancak biz verileri doÄŸrudan Datasets sunucusundan almak iÃ§in [FiftyOne'Ä±n ğŸ¤— Hub entegrasyonunu](https://docs.voxel51.com/integrations/huggingface.html#huggingface-hub) kullanacaÄŸÄ±z. HesaplamalarÄ± hÄ±zlÄ± yapmak iÃ§in, sadece ilk $1,000$ Ã¶rneÄŸi indireceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fouh.load_from_hub(\n",
    "    \"huggan/wikiart\", ## repo_id\n",
    "    format=\"parquet\", ## for Parquet format\n",
    "    classification_fields=[\"artist\", \"style\", \"genre\"], # columns to store as classification fields\n",
    "    max_samples=1000, # number of samples to load\n",
    "    name=\"wikiart\", # name of the dataset in FiftyOne\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri kÃ¼mesinin ne iÃ§erdiÄŸini gÃ¶rmek iÃ§in Ã¶zetini ekrana yazdÄ±rÄ±n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        wikiart\n",
      "Media type:  image\n",
      "Num samples: 1000\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:       fiftyone.core.fields.ObjectIdField\n",
      "    filepath: fiftyone.core.fields.StringField\n",
      "    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    artist:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    style:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    genre:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    row_idx:  fiftyone.core.fields.IntField\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri kÃ¼mesini [FiftyOne uygulamasÄ±nda](https://docs.voxel51.com/user_guide/app.html) gÃ¶rselleÅŸtirin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WikiArt Dataset](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_wikiart_dataset.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haydi, stillerini analiz edeceÄŸimiz artistlerin isimlerini listeleyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unknown Artist', 'albrecht-durer', 'boris-kustodiev', 'camille-pissarro', 'childe-hassam', 'claude-monet', 'edgar-degas', 'eugene-boudin', 'gustave-dore', 'ilya-repin', 'ivan-aivazovsky', 'ivan-shishkin', 'john-singer-sargent', 'marc-chagall', 'martiros-saryan', 'nicholas-roerich', 'pablo-picasso', 'paul-cezanne', 'pierre-auguste-renoir', 'pyotr-konchalovsky', 'raphael-kirchner', 'rembrandt', 'salvador-dali', 'vincent-van-gogh']\n"
     ]
    }
   ],
   "source": [
    "artists = dataset.distinct(\"artist.label\")\n",
    "print(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benzer Sanat Eserlerini Bulma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HoÅŸunuza giden bir sanat eseri bulduÄŸunuzda, ona benzer parÃ§alar bulmak istemeniz doÄŸaldÄ±r. Bunu vektÃ¶r embedding'leriyle yapabiliriz! DahasÄ±, multimodal embedding'leri aracÄ±lÄ±ÄŸÄ±yla, bir tablonun ya da bir ÅŸiirin bile tanÄ±mÄ± olabilecek, verilen bir metin sorgusunu en yakÄ±n temsil eden tablolarÄ± bulma yeteÄŸini aÃ§acaÄŸÄ±z.\n",
    "\n",
    "Haydi, gÃ¶rÃ¼ntÃ¼ler iÃ§in Transformers'tan ğŸ¤— Ã¶n-eÄŸitimli bir CLIP Vision Transformer (ViT) modelini kullanarak multimodal embedding'leri Ã¼retelim. [FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html)'in `compute_similarity()` fonksiyonunu Ã§alÄ±ÅŸtÄ±rmak, bu embedding'leri hesaplayacak ve bu embedding'leri, veri setinde bir benzerlik indeksi Ã¼retmek iÃ§in kullanacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [5.0m elapsed, 0s remaining, 3.3 samples/s]    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.internal.core.sklearn.SklearnSimilarityIndex at 0x2ad67ecd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fob.compute_similarity(\n",
    "    dataset, \n",
    "    model=\"zero-shot-classification-transformer-torch\", ## type of model to load from model zoo\n",
    "    name_or_path=\"openai/clip-vit-base-patch32\", ## repo_id of checkpoint\n",
    "    embeddings=\"clip_embeddings\", ## name of the field to store embeddings\n",
    "    brain_key=\"clip_sim\", ## key to store similarity index info\n",
    "    batch_size=32, ## batch size for inference\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px; border-left: 5px solid #0078d4; font-family: Arial, sans-serif; margin: 10px 0;\">\n",
    "\n",
    "Alternatif olarak, modeli doÄŸrudan ğŸ¤— Transformers kÃ¼tÃ¼phanesinden yÃ¼kleyebilir ve modeli doÄŸrudan kullanabilirsiniz:\n",
    "\n",
    "```python\n",
    "from transformers import CLIPModel\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "fob.compute_similarity(\n",
    "    dataset, \n",
    "    model=model,\n",
    "    embeddings=\"clip_embeddings\", ## name of the field to store embeddings\n",
    "    brain_key=\"clip_sim\" ## key to store similarity index info\n",
    ")\n",
    "```\n",
    "\n",
    "Bu ve daha fazlasÄ± hakkÄ±nda daha kapsamlÄ± bir rehber iÃ§in, <a href=\"https://docs.voxel51.com/integrations/huggingface.html#transformers-library\">FiftyOne'Ä±n ğŸ¤— Transformers entegrasyonunu</a> inceleyin.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne uygulamasÄ±nÄ± yenileyin, Ã¶rnek listedeki bir gÃ¶rÃ¼ntÃ¼nÃ¼n onay kutusunu seÃ§in ve veri kÃ¼mesindeki en benzer gÃ¶rÃ¼ntÃ¼leri gÃ¶rmek iÃ§in fotoÄŸraf ikonuna tÄ±klayÄ±n. Arka planda, bu butona tÄ±klamak, seÃ§ilen gÃ¶rÃ¼ntÃ¼ye en Ã§ok benzeyen gÃ¶rÃ¼ntÃ¼leri bulan benzerlik indeksi, Ã¶nceden hesaplanmÄ±ÅŸ embedding'lere dayanarak bir sorguyu tetikler ve bunlarÄ± uygulamada gÃ¶sterir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Similarity Search](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_image_search.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunu, verilen sanat eserine hangi sanat eserlerinin en Ã§ok benzediÄŸini gÃ¶rmek iÃ§in kullanabiliriz. Benzer sanat eserlerini bulmak (kullanÄ±cÄ±lara Ã¶nermek ya da bir koleksiyona eklemek iÃ§in) ya da yeni bir eser iÃ§in ilham almak amacÄ±yla faydalÄ± olabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ama dahasÄ± var! CLIP multimodal olduÄŸu iÃ§in, anlamsal (semantik) aramalar gerÃ§ekleÅŸtirmek iÃ§in de kullanabiliriz! Bu, metin sorgularÄ±na dayalÄ± gÃ¶rÃ¼ntÃ¼ aramasÄ± yapabileceÄŸimiz anlamÄ±na geliyor. Ã–rneÄŸin, \"pastel aÄŸaÃ§lar\" metniyle arama yapabiliriz ve veri kÃ¼mesinde bu sorguya benzeyen tÃ¼m gÃ¶rÃ¼ntÃ¼leri gÃ¶rebiliriz. Bunu yapmak iÃ§in FiftyOne uygulamasÄ±ndaki arama butonuna tÄ±klayÄ±n ve bir metin sorgusu girin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Semantic Search](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_semantic_search.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sahne arkasÄ±nda, metin tokenize edilir, CLIP'in metin kodlayÄ±cÄ±sÄ± ile gÃ¶mÃ¼lÃ¼r, ardÄ±ndan veri kÃ¼mesindeki en benzer gÃ¶rÃ¼ntÃ¼leri bulmak iÃ§in benzerlik indeksini sorgulamak iÃ§in kullanÄ±lÄ±r. Bu, metin sorgularÄ±na dayalÄ± gÃ¶rÃ¼ntÃ¼ aramak iÃ§in oldukÃ§a etkili bir yÃ¶ntemdir ve belirli bir tema ya da stille eÅŸleÅŸen gÃ¶rÃ¼ntÃ¼leri bulmak iÃ§in faydalÄ± olabilir. Ve bu CLIP ile sÄ±nÄ±rlÄ± deÄŸil; gÃ¶rÃ¼ntÃ¼ler ve metin iÃ§in embedding'ler Ã¼retebilen ğŸ¤— Transformers'tan herhangi bir CLIP-benzeri model kullanabilirsiniz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "ğŸ’¡ Etkili vektÃ¶r aramasÄ± ve bÃ¼yÃ¼k veri kÃ¼meleri Ã¼zerinde indeksleme iÃ§in, Fifty One'Ä±n <a href=\"https://voxel51.com/vector-search\">aÃ§Ä±k kaynak vektÃ¶r veritabanlarÄ± ile yerel entegrasyonlarÄ±</a> var.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KÃ¼meleme ve GÃ¶rselleÅŸtirme ile Sanatsal Motiflerin Ortaya Ã‡Ä±karÄ±lmasÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benzerlik ve anlamsal (semantik) aramalar yaparak verilerle daha etkili bir ÅŸekilde etkileÅŸim kurmaya baÅŸlayabiliriz. Ancak bunu bir adÄ±m daha ileri gÃ¶tÃ¼rebilir ve iÅŸin iÃ§ine biraz da denetimsiz Ã¶ÄŸrenme ekleyebiliriz. Bu, WikiArt veri kÃ¼mesindeki sanatsal paternleri belirlememize yardÄ±mcÄ± olacak; stilistik, topikal ve hatta kelimelerle ifade edilmesi zor olan motifleri bile.\n",
    "\n",
    "Bunu iki yÃ¶ntemle yapacaÄŸÄ±z:\n",
    "\n",
    "1. **Boyut Ä°ndirgeme**: Embedding'lerin boyutunu 2B'ye indirgemek ve verileri bir daÄŸÄ±lÄ±m grafiÄŸinde gÃ¶rselleÅŸtirmek iÃ§in UMAP yÃ¶ntemini kullanacaÄŸÄ±z. Bu, gÃ¶rÃ¼ntÃ¼lerin stillerine, tÃ¼rlerine ve artistlerine gÃ¶re nasÄ±l kÃ¼melendiÄŸini gÃ¶rmemizi saÄŸlayacaktÄ±r.\n",
    "2. **KÃ¼meleme**: GÃ¶rÃ¼ntÃ¼leri embedding'lerine gÃ¶re kÃ¼melemek ve hangi gruplarÄ±n ortaya Ã§Ä±ktÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in K-Means kÃ¼meleme yÃ¶ntemini kullanacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boyut indirgeme iÃ§in, Ã¶nceden hesaplanmÄ±ÅŸ embedding'leri parametre olarak geÃ§erek FiftyOne Brain'den `compute_visualization()` fonksiyonunu Ã§alÄ±ÅŸtÄ±racaÄŸÄ±z. Boyut indirgeme amacÄ±yla UMAP  yÃ¶ntemini kullanmak iÃ§in `method=\"umap\"` olarak belirtiyoruz, ancak PCA veya t-SNE de kullanabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/fdev/lib/python3.9/site-packages/numba/cpython/hashing.py:482: UserWarning: FNV hashing is not implemented in Numba. See PEP 456 https://www.python.org/dev/peps/pep-0456/ for rationale over not using FNV. Numba will continue to work, but hashes for built in types will be computed using siphash24. This will permit e.g. dictionaries to continue to behave as expected, however anything relying on the value of the hash opposed to hash as a derived property is likely to not work as expected.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP( verbose=True)\n",
      "Tue Apr 30 11:51:45 2024 Construct fuzzy simplicial set\n",
      "Tue Apr 30 11:51:46 2024 Finding Nearest Neighbors\n",
      "Tue Apr 30 11:51:47 2024 Finished Nearest Neighbor Search\n",
      "Tue Apr 30 11:51:48 2024 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dde3df324249df91f3336c913b409a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Tue Apr 30 11:51:49 2024 Finished embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.visualization.VisualizationResults at 0x29f468760>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fob.compute_visualization(dataset, embeddings=\"clip_embeddings\", method=\"umap\", brain_key=\"clip_vis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Åimdi FiftyOne uygulamasÄ±nda veri kÃ¼mesindeki her gÃ¶rÃ¼ntÃ¼ iÃ§in 2B bir nokta gÃ¶receÄŸimiz bir panel aÃ§abiliriz. Bu niteliklerin gÃ¶rÃ¼ntÃ¼ Ã¶zelliklerimiz tarafÄ±ndan ne kadar gÃ¼Ã§lÃ¼ bir ÅŸekilde yakalandÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in, veri kÃ¼mesindeki noktalarÄ± artist veya tÃ¼r gibi alanlara gÃ¶re renklendirebiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UMAP Visualization](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_visualize_embeddings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benzer gÃ¶rÃ¼ntÃ¼leri bir arada gruplayabilmek iÃ§in embedding'ler Ã¼zerinde kÃ¼meleme de yapabiliriz â€” belki bu sanat eserlerinin dominant Ã¶zellikleri halihazÄ±rda var olan etiketler tarafÄ±ndan algÄ±lanmamÄ±ÅŸtÄ±r ya da belki tanÄ±mlamak istediÄŸimiz farklÄ± alt tÃ¼rler vardÄ±r. Verilerimizi kÃ¼melemek iÃ§in [FiftyOne Clustering Plugin](https://github.com/jacobmarks/clustering-plugin)'i indirmemiz gerekecek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UygulamayÄ± tekrar yenileyerek, kÃ¼meleme fonksiyonalitesine uygulamadaki bir operatÃ¶r yardÄ±mÄ±yla eriÅŸebiliriz. OperatÃ¶r listesini aÃ§mak iÃ§in ters tÄ±rnak tuÅŸuna basÄ±n, \"cluster\" yazÄ±n ve aÃ§Ä±lan menÃ¼den operatÃ¶rÃ¼ seÃ§in. Bu, kÃ¼meleme algoritmasÄ±nÄ±, hiperparametreleri ve kÃ¼melenecek alanÄ± belirleyebileceÄŸimiz etkileÅŸimli bir panel aÃ§acaktÄ±r. Basit tutmak iÃ§in, $10$ kÃ¼me ile K-Means kÃ¼melemesini kullanacaÄŸÄ±z:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BÃ¶ylelikle kÃ¼meleri uygulamada gÃ¶rselleÅŸtirebilir ve gÃ¶rÃ¼ntÃ¼lerin embedding'lerine gÃ¶re nasÄ±l gruplandÄ±ÄŸÄ±nÄ± gÃ¶rebiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K-means Clustering](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_clustering.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BazÄ± kÃ¼melerin artisti, bazÄ±larÄ±nÄ±n ise tÃ¼rÃ¼ veya stili temel aldÄ±ÄŸÄ±nÄ± gÃ¶rebiliriz. DiÄŸerleri ise daha soyut ve veriden ilk bakÄ±ÅŸta anlaÅŸÄ±lamayan alt tÃ¼rleri veya diÄŸer gruplamalarÄ± temsil edebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Most Unique Works of Art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting question we can ask about our dataset is how *unique* each image is. This question is important for many applications, such as recommending similar images, detecting duplicates, or identifying outliers. In the context of art, how unique a painting is could be an important factor in determining its value.\n",
    "\n",
    "While there are a million ways to characterize uniqueness, our image embeddings allow us to quantitatively assign each sample a uniqueness score based on how similar it is to other samples in the dataset. Explicitly, the FiftyOne Brain's `compute_uniqueness()` function looks at the distance between each sample's embedding and its nearest neighbors, and computes a score between $0$ and $1$ based on this distance. A score of $0$ means the sample is nondescript or very similar to others, while a score of $1$ means the sample is very unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing uniqueness...\n",
      "Uniqueness computation complete\n"
     ]
    }
   ],
   "source": [
    "fob.compute_uniqueness(dataset, embeddings=\"clip_embeddings\") # compute uniqueness using CLIP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then color by this in the embeddings panel, filter by uniqueness score, or even sort by it to see the most unique images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique_view = dataset.sort_by(\"uniqueness\", reverse=True)\n",
    "session.view = most_unique_view.view() # Most unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Most Unique Images](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_most_unique.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_unique_view = dataset.sort_by(\"uniqueness\", reverse=False)\n",
    "session.view = least_unique_view.view() # Least unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Least Unique Images](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_least_unique.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going a step further, we can also answer the question of which artist tends to produce the most unique works. We can compute the average uniqueness score for each artist across all of their works of art:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown Artist: 0.7932221632002723\n",
      "boris-kustodiev: 0.7480731948424676\n",
      "salvador-dali: 0.7368807620414014\n",
      "raphael-kirchner: 0.7315448102204755\n",
      "ilya-repin: 0.7204744626806383\n",
      "marc-chagall: 0.7169373812321908\n",
      "rembrandt: 0.715205220292227\n",
      "martiros-saryan: 0.708560775790436\n",
      "childe-hassam: 0.7018343391132756\n",
      "edgar-degas: 0.699912746806587\n",
      "albrecht-durer: 0.6969358680800216\n",
      "john-singer-sargent: 0.6839955708720844\n",
      "pablo-picasso: 0.6835137858302969\n",
      "pyotr-konchalovsky: 0.6780653000855895\n",
      "nicholas-roerich: 0.6676504687452387\n",
      "ivan-aivazovsky: 0.6484361530090199\n",
      "vincent-van-gogh: 0.6472004520699081\n",
      "gustave-dore: 0.6307283287457358\n",
      "pierre-auguste-renoir: 0.6271467146993583\n",
      "paul-cezanne: 0.6251076007168186\n",
      "eugene-boudin: 0.6103397516167454\n",
      "camille-pissarro: 0.6046182609119615\n",
      "claude-monet: 0.5998234558947573\n",
      "ivan-shishkin: 0.589796389836674\n"
     ]
    }
   ],
   "source": [
    "artist_unique_scores = {\n",
    "    artist: dataset.match(F(\"artist.label\") == artist).mean(\"uniqueness\")\n",
    "    for artist in artists\n",
    "}\n",
    "\n",
    "sorted_artists = sorted(\n",
    "    artist_unique_scores, key=artist_unique_scores.get, reverse=True\n",
    ")\n",
    "\n",
    "for artist in sorted_artists:\n",
    "    print(f\"{artist}: {artist_unique_scores[artist]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that the artist with the most unique works in our dataset is Boris Kustodiev! Let's take a look at some of his works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kustodiev_view = dataset.match(F(\"artist.label\") == \"boris-kustodiev\")\n",
    "session.view = kustodiev_view.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Boris Kustodiev Artwork](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_kustodiev_view.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SanatÄ±n GÃ¶rsel Ã–zelliklerle Karakterize Edilmesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To round things out, let's go back to the basics and analyze some core qualities of the images in our dataset. We'll compute standard metrics like brightness, contrast, and saturation for each image and see how these metrics correlate with the artistic style and genre of the art pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run these analyses, we will need to download the [FiftyOne Image Quality Plugin](https://github.com/jacobmarks/image-quality-issues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/image-quality-issues/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh the app and open the operators list again. This time type `compute` and select one of the image quality operators. We'll start with brightness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Compute Brightness](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_compute_brightness.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the operator finishes running, we will have a new field in our dataset that contains the brightness score for each image. We can then visualize this data in the app:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Brightness](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_brightness.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also color by brightness, and even see how it correlates with other fields in the dataset like style:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Style by Brightness](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_style_by_brightness.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for contrast and saturation. Here are the results for saturation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Filter by Saturation](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_filter_by_saturation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this illustrates how not everything boils down to applying deep neural networks to your data. Sometimes, simple metrics can be just as informative and can provide a different perspective on your data ğŸ¤“!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "ğŸ“š For larger datasets, you may want to <a href=\"https://docs.voxel51.com/plugins/using_plugins.html#delegated-operations\">delegate the operations</a> for later execution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've explored how to use multimodal embeddings, unsupervised learning, and traditional image processing techniques to analyze artistic styles in images. We've seen how to perform image similarity and semantic searches, cluster images based on their style, analyze the uniqueness of images, and compute image quality metrics. These techniques can be applied to a wide range of visual datasets, from art collections to medical images to satellite imagery. Try [loading a different dataset from the Hugging Face Hub](https://docs.voxel51.com/integrations/huggingface.html#loading-datasets-from-the-hub) and see what insights you can uncover!\n",
    "\n",
    "If you want to go even further, here are some additional analyses you could try:\n",
    "\n",
    "- **Zero-Shot Classification**: Use a pre-trained vision-language model from ğŸ¤— Transformers to categorize images in the dataset by topic or subject, without any training data. Check out this [Zero-Shot Classification tutorial](https://docs.voxel51.com/tutorials/zero_shot_classification.html) for more info.\n",
    "- **Image Captioning**: Use a pre-trained vision-language model from ğŸ¤— Transformers to generate captions for the images in the dataset. Then use this for topic modeling or cluster artwork based on embeddings for these captions. Check out FiftyOne's [Image Captioning Plugin](https://github.com/jacobmarks/fiftyone-image-captioning-plugin) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“š Kaynaklar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [FiftyOne ğŸ¤ ğŸ¤— Hub Integration](https://docs.voxel51.com/integrations/huggingface.html#huggingface-hub)\n",
    "- [FiftyOne ğŸ¤ ğŸ¤— Transformers Integration](https://docs.voxel51.com/integrations/huggingface.html#transformers-library)\n",
    "- [FiftyOne Vector Search Integrations](https://voxel51.com/vector-search/)\n",
    "- [Visualizing Data with Dimensionality Reduction Techniques](https://docs.voxel51.com/tutorials/dimension_reduction.html)\n",
    "- [Clustering Images with Embeddings](https://docs.voxel51.com/tutorials/clustering.html)\n",
    "- [Exploring Image Uniqueness with FiftyOne](https://docs.voxel51.com/tutorials/uniqueness.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FiftyOne AÃ§Ä±k Kaynak Projesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FiftyOne](https://github.com/voxel51/fiftyone/) is the leading open source toolkit for building high-quality datasets and computer vision models. With over 2M downloads, FiftyOne is trusted by developers and researchers across the globe.\n",
    "\n",
    "ğŸ’ª The FiftyOne team welcomes contributions from the open source community! If you're interested in contributing to FiftyOne, check out the [contributing guide](https://github.com/voxel51/fiftyone/blob/develop/CONTRIBUTING.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
