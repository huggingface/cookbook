{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Gömme ile Sanatsal Stillerin Analizi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Yazar: [Jacob Marks](https://huggingface.co/jamarks)*\n",
    "*Çevirmen: [Aylin Gümüş](https://huggingface.co/aylingumus)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Art Analysis Cover Image](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_cover_image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Görüntüler gibi görsel veriler bilgi açısından inanılmaz derecede zengindir, ancak yapısal olmayan doğası gereği analiz etmesi zordur.\n",
    "\n",
    "Bu not defterinde, multimodal embedding yöntemlerini ve hesaplanan öznitelikleri kullanarak resimlerdeki sanatsal stillerin nasıl analiz edileceğini keşfedeceğiz. Veri analizi ve görselleştirme için kullanmak üzere FiftyOne'a yükleyeceğimiz 🤗 Hub'dan [WikiArt veri kümesini](https://huggingface.co/datasets/huggan/wikiart) kullanacağız. Verilere birçok farklı açıdan göz atacağız:\n",
    "\n",
    "- **Görüntü Benzerliği Arama ve Anlamsal (Semantik) Arama**: 🤗 Transformers'tan ön-eğitimli [CLIP](https://huggingface.co/openai/clip-vit-base-patch32) modelini kullanarak multimodal embedding'leri üreteceğiz ve yapısal olmayan aramalara izin vermek için veriyi indeksleyeceğiz.\n",
    "\n",
    "- **Kümeleme ve Görselleştirme**: Embedding'leri kullanarak görüntüleri sanatsal stillerine göre kümeleyeceğiz ve UMAP boyut indirgeme yöntemini kullanarak sonuçları görselleştireceğiz.\n",
    "\n",
    "- **Benzersizlik Analizi**: Veri kümesindeki diğer görüntülere ne kadar benzediğine bağlı olarak her görüntü için benzersizlik skoru atamak için embedding'lerimizi kullanacağız.\n",
    "\n",
    "- **Görüntü Kalitesi Analizi**: Her bir görüntü için parlaklık, kontrast ve doygunluk gibi görüntü kalite metriklerini hesaplayacağız ve bu metriklerin, görüntülerin sanatsal stili ile nasıl ilişkili olduğunu göreceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haydi başlayalım! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu not defterini çalıştırmak için, aşağıdaki kütüphaneleri yüklemelisiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers huggingface_hub fiftyone umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İndirmeleri ışık hızında yapmak için [HF Transfer](https://pypi.org/project/hf-transfer/) yükleyin:\n",
    "\n",
    "```bash\n",
    "pip install hf-transfer\n",
    "```\n",
    "\n",
    "Ve `HF_HUB_ENABLE_HF_TRANSFER` ortam değişkenini ayarlayarak etkinleştirin:\n",
    "\n",
    "```bash\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Not:</b> Bu not defteri <code>transformers==4.40.0</code>, <code>huggingface_hub==0.22.2</code> ve <code>fiftyone==0.23.8</code> ile test edilmiştir.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi bu not defteri için gerekli olan modülleri içe aktaralım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo # base library and app\n",
    "import fiftyone.zoo as foz # zoo datasets and models\n",
    "import fiftyone.brain as fob # ML routines\n",
    "from fiftyone import ViewField as F # for defining custom views\n",
    "import fiftyone.utils.huggingface as fouh # for loading datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hub'dan 🤗 FiftyOne'a WikiArt veri kümesini yükleyerek başlayacağız. Bu veri kümesi Hugging Face'in `datasets` kütüphanesi aracılığıyla da yüklenebilir, ancak biz verileri doğrudan Datasets sunucusundan almak için [FiftyOne'ın 🤗 Hub entegrasyonunu](https://docs.voxel51.com/integrations/huggingface.html#huggingface-hub) kullanacağız. Hesaplamaları hızlı yapmak için, sadece ilk $1,000$ örneği indireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fouh.load_from_hub(\n",
    "    \"huggan/wikiart\", ## repo_id\n",
    "    format=\"parquet\", ## for Parquet format\n",
    "    classification_fields=[\"artist\", \"style\", \"genre\"], # columns to store as classification fields\n",
    "    max_samples=1000, # number of samples to load\n",
    "    name=\"wikiart\", # name of the dataset in FiftyOne\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri kümesinin ne içerdiğini görmek için özetini ekrana yazdırın:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        wikiart\n",
      "Media type:  image\n",
      "Num samples: 1000\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:       fiftyone.core.fields.ObjectIdField\n",
      "    filepath: fiftyone.core.fields.StringField\n",
      "    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    artist:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    style:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    genre:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    row_idx:  fiftyone.core.fields.IntField\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri kümesini [FiftyOne uygulamasında](https://docs.voxel51.com/user_guide/app.html) görselleştirin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WikiArt Dataset](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_wikiart_dataset.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haydi, stillerini analiz edeceğimiz artistlerin isimlerini listeleyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unknown Artist', 'albrecht-durer', 'boris-kustodiev', 'camille-pissarro', 'childe-hassam', 'claude-monet', 'edgar-degas', 'eugene-boudin', 'gustave-dore', 'ilya-repin', 'ivan-aivazovsky', 'ivan-shishkin', 'john-singer-sargent', 'marc-chagall', 'martiros-saryan', 'nicholas-roerich', 'pablo-picasso', 'paul-cezanne', 'pierre-auguste-renoir', 'pyotr-konchalovsky', 'raphael-kirchner', 'rembrandt', 'salvador-dali', 'vincent-van-gogh']\n"
     ]
    }
   ],
   "source": [
    "artists = dataset.distinct(\"artist.label\")\n",
    "print(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benzer Sanat Eserlerini Bulma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoşunuza giden bir sanat eseri bulduğunuzda, ona benzer parçalar bulmak istemeniz doğaldır. Bunu vektör embedding'leriyle yapabiliriz! Dahası, multimodal embedding'leri aracılığıyla, bir tablonun ya da bir şiirin bile tanımı olabilecek, verilen bir metin sorgusunu en yakın temsil eden tabloları bulma yeteğini açacağız.\n",
    "\n",
    "Haydi, görüntüler için Transformers'tan 🤗 ön-eğitimli bir CLIP Vision Transformer (ViT) modelini kullanarak multimodal embedding'leri üretelim. [FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html)'in `compute_similarity()` fonksiyonunu çalıştırmak, bu embedding'leri hesaplayacak ve bu embedding'leri, veri setinde bir benzerlik indeksi üretmek için kullanacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      " 100% |███████████████| 1000/1000 [5.0m elapsed, 0s remaining, 3.3 samples/s]    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.internal.core.sklearn.SklearnSimilarityIndex at 0x2ad67ecd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fob.compute_similarity(\n",
    "    dataset, \n",
    "    model=\"zero-shot-classification-transformer-torch\", ## type of model to load from model zoo\n",
    "    name_or_path=\"openai/clip-vit-base-patch32\", ## repo_id of checkpoint\n",
    "    embeddings=\"clip_embeddings\", ## name of the field to store embeddings\n",
    "    brain_key=\"clip_sim\", ## key to store similarity index info\n",
    "    batch_size=32, ## batch size for inference\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px; border-left: 5px solid #0078d4; font-family: Arial, sans-serif; margin: 10px 0;\">\n",
    "\n",
    "Alternatif olarak, modeli doğrudan 🤗 Transformers kütüphanesinden yükleyebilir ve modeli doğrudan kullanabilirsiniz:\n",
    "\n",
    "```python\n",
    "from transformers import CLIPModel\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "fob.compute_similarity(\n",
    "    dataset, \n",
    "    model=model,\n",
    "    embeddings=\"clip_embeddings\", ## name of the field to store embeddings\n",
    "    brain_key=\"clip_sim\" ## key to store similarity index info\n",
    ")\n",
    "```\n",
    "\n",
    "Bu ve daha fazlası hakkında daha kapsamlı bir rehber için, <a href=\"https://docs.voxel51.com/integrations/huggingface.html#transformers-library\">FiftyOne'ın 🤗 Transformers entegrasyonunu</a> inceleyin.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne uygulamasını yenileyin, örnek listedeki bir görüntünün onay kutusunu seçin ve veri kümesindeki en benzer görüntüleri görmek için fotoğraf ikonuna tıklayın. Arka planda, bu butona tıklamak, seçilen görüntüye en çok benzeyen görüntüleri bulan benzerlik indeksi, önceden hesaplanmış embedding'lere dayanarak bir sorguyu tetikler ve bunları uygulamada gösterir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Similarity Search](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_image_search.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunu, verilen sanat eserine hangi sanat eserlerinin en çok benzediğini görmek için kullanabiliriz. Benzer sanat eserlerini bulmak (kullanıcılara önermek ya da bir koleksiyona eklemek için) ya da yeni bir eser için ilham almak amacıyla faydalı olabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ama dahası var! CLIP multimodal olduğu için, anlamsal (semantik) aramalar gerçekleştirmek için de kullanabiliriz! Bu, metin sorgularına dayalı görüntü araması yapabileceğimiz anlamına geliyor. Örneğin, \"pastel ağaçlar\" metniyle arama yapabiliriz ve veri kümesinde bu sorguya benzeyen tüm görüntüleri görebiliriz. Bunu yapmak için FiftyOne uygulamasındaki arama butonuna tıklayın ve bir metin sorgusu girin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Semantic Search](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_semantic_search.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sahne arkasında, metin tokenize edilir, CLIP'in metin kodlayıcısı ile gömülür, ardından veri kümesindeki en benzer görüntüleri bulmak için benzerlik indeksini sorgulamak için kullanılır. Bu, metin sorgularına dayalı görüntü aramak için oldukça etkili bir yöntemdir ve belirli bir tema ya da stille eşleşen görüntüleri bulmak için faydalı olabilir. Ve bu CLIP ile sınırlı değil; görüntüler ve metin için embedding'ler üretebilen 🤗 Transformers'tan herhangi bir CLIP-benzeri model kullanabilirsiniz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "💡 Etkili vektör araması ve büyük veri kümeleri üzerinde indeksleme için, Fifty One'ın <a href=\"https://voxel51.com/vector-search\">açık kaynak vektör veritabanları ile yerel entegrasyonları</a> var.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kümeleme ve Görselleştirme ile Sanatsal Motiflerin Ortaya Çıkarılması"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benzerlik ve anlamsal (semantik) aramalar yaparak verilerle daha etkili bir şekilde etkileşim kurmaya başlayabiliriz. Ancak bunu bir adım daha ileri götürebilir ve işin içine biraz da denetimsiz öğrenme ekleyebiliriz. Bu, WikiArt veri kümesindeki sanatsal paternleri belirlememize yardımcı olacak; stilistik, topikal ve hatta kelimelerle ifade edilmesi zor olan motifleri bile.\n",
    "\n",
    "Bunu iki yöntemle yapacağız:\n",
    "\n",
    "1. **Boyut İndirgeme**: Embedding'lerin boyutunu 2B'ye indirgemek ve verileri bir dağılım grafiğinde görselleştirmek için UMAP yöntemini kullanacağız. Bu, görüntülerin stillerine, türlerine ve artistlerine göre nasıl kümelendiğini görmemizi sağlayacaktır.\n",
    "2. **Kümeleme**: Görüntüleri embedding'lerine göre kümelemek ve hangi grupların ortaya çıktığını görmek için K-Means kümeleme yöntemini kullanacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boyut indirgeme için, önceden hesaplanmış embedding'leri parametre olarak geçerek FiftyOne Brain'den `compute_visualization()` fonksiyonunu çalıştıracağız. Boyut indirgeme amacıyla UMAP  yöntemini kullanmak için `method=\"umap\"` olarak belirtiyoruz, ancak PCA veya t-SNE de kullanabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/fdev/lib/python3.9/site-packages/numba/cpython/hashing.py:482: UserWarning: FNV hashing is not implemented in Numba. See PEP 456 https://www.python.org/dev/peps/pep-0456/ for rationale over not using FNV. Numba will continue to work, but hashes for built in types will be computed using siphash24. This will permit e.g. dictionaries to continue to behave as expected, however anything relying on the value of the hash opposed to hash as a derived property is likely to not work as expected.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP( verbose=True)\n",
      "Tue Apr 30 11:51:45 2024 Construct fuzzy simplicial set\n",
      "Tue Apr 30 11:51:46 2024 Finding Nearest Neighbors\n",
      "Tue Apr 30 11:51:47 2024 Finished Nearest Neighbor Search\n",
      "Tue Apr 30 11:51:48 2024 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dde3df324249df91f3336c913b409a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Tue Apr 30 11:51:49 2024 Finished embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.visualization.VisualizationResults at 0x29f468760>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fob.compute_visualization(dataset, embeddings=\"clip_embeddings\", method=\"umap\", brain_key=\"clip_vis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi FiftyOne uygulamasında veri kümesindeki her görüntü için 2B bir nokta göreceğimiz bir panel açabiliriz. Bu niteliklerin görüntü özelliklerimiz tarafından ne kadar güçlü bir şekilde yakalandığını görmek için, veri kümesindeki noktaları artist veya tür gibi alanlara göre renklendirebiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UMAP Visualization](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_visualize_embeddings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benzer görüntüleri bir arada gruplayabilmek için embedding'ler üzerinde kümeleme de yapabiliriz — belki bu sanat eserlerinin dominant özellikleri halihazırda var olan etiketler tarafından algılanmamıştır ya da belki tanımlamak istediğimiz farklı alt türler vardır. Verilerimizi kümelemek için [FiftyOne Clustering Plugin](https://github.com/jacobmarks/clustering-plugin)'i indirmemiz gerekecek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uygulamayı tekrar yenileyerek, kümeleme fonksiyonalitesine uygulamadaki bir operatör yardımıyla erişebiliriz. Operatör listesini açmak için ters tırnak tuşuna basın, \"cluster\" yazın ve açılan menüden operatörü seçin. Bu, kümeleme algoritmasını, hiperparametreleri ve kümelenecek alanı belirleyebileceğimiz etkileşimli bir panel açacaktır. Basit tutmak için, $10$ küme ile K-Means kümelemesini kullanacağız:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Böylelikle kümeleri uygulamada görselleştirebilir ve görüntülerin embedding'lerine göre nasıl gruplandığını görebiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K-means Clustering](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_clustering.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bazı kümelerin artisti, bazılarının ise türü veya stili temel aldığını görebiliriz. Diğerleri ise daha soyut ve veriden ilk bakışta anlaşılamayan alt türleri veya diğer gruplamaları temsil edebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Most Unique Works of Art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting question we can ask about our dataset is how *unique* each image is. This question is important for many applications, such as recommending similar images, detecting duplicates, or identifying outliers. In the context of art, how unique a painting is could be an important factor in determining its value.\n",
    "\n",
    "While there are a million ways to characterize uniqueness, our image embeddings allow us to quantitatively assign each sample a uniqueness score based on how similar it is to other samples in the dataset. Explicitly, the FiftyOne Brain's `compute_uniqueness()` function looks at the distance between each sample's embedding and its nearest neighbors, and computes a score between $0$ and $1$ based on this distance. A score of $0$ means the sample is nondescript or very similar to others, while a score of $1$ means the sample is very unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing uniqueness...\n",
      "Uniqueness computation complete\n"
     ]
    }
   ],
   "source": [
    "fob.compute_uniqueness(dataset, embeddings=\"clip_embeddings\") # compute uniqueness using CLIP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then color by this in the embeddings panel, filter by uniqueness score, or even sort by it to see the most unique images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique_view = dataset.sort_by(\"uniqueness\", reverse=True)\n",
    "session.view = most_unique_view.view() # Most unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Most Unique Images](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_most_unique.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_unique_view = dataset.sort_by(\"uniqueness\", reverse=False)\n",
    "session.view = least_unique_view.view() # Least unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Least Unique Images](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_least_unique.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going a step further, we can also answer the question of which artist tends to produce the most unique works. We can compute the average uniqueness score for each artist across all of their works of art:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown Artist: 0.7932221632002723\n",
      "boris-kustodiev: 0.7480731948424676\n",
      "salvador-dali: 0.7368807620414014\n",
      "raphael-kirchner: 0.7315448102204755\n",
      "ilya-repin: 0.7204744626806383\n",
      "marc-chagall: 0.7169373812321908\n",
      "rembrandt: 0.715205220292227\n",
      "martiros-saryan: 0.708560775790436\n",
      "childe-hassam: 0.7018343391132756\n",
      "edgar-degas: 0.699912746806587\n",
      "albrecht-durer: 0.6969358680800216\n",
      "john-singer-sargent: 0.6839955708720844\n",
      "pablo-picasso: 0.6835137858302969\n",
      "pyotr-konchalovsky: 0.6780653000855895\n",
      "nicholas-roerich: 0.6676504687452387\n",
      "ivan-aivazovsky: 0.6484361530090199\n",
      "vincent-van-gogh: 0.6472004520699081\n",
      "gustave-dore: 0.6307283287457358\n",
      "pierre-auguste-renoir: 0.6271467146993583\n",
      "paul-cezanne: 0.6251076007168186\n",
      "eugene-boudin: 0.6103397516167454\n",
      "camille-pissarro: 0.6046182609119615\n",
      "claude-monet: 0.5998234558947573\n",
      "ivan-shishkin: 0.589796389836674\n"
     ]
    }
   ],
   "source": [
    "artist_unique_scores = {\n",
    "    artist: dataset.match(F(\"artist.label\") == artist).mean(\"uniqueness\")\n",
    "    for artist in artists\n",
    "}\n",
    "\n",
    "sorted_artists = sorted(\n",
    "    artist_unique_scores, key=artist_unique_scores.get, reverse=True\n",
    ")\n",
    "\n",
    "for artist in sorted_artists:\n",
    "    print(f\"{artist}: {artist_unique_scores[artist]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that the artist with the most unique works in our dataset is Boris Kustodiev! Let's take a look at some of his works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kustodiev_view = dataset.match(F(\"artist.label\") == \"boris-kustodiev\")\n",
    "session.view = kustodiev_view.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Boris Kustodiev Artwork](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_kustodiev_view.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanatın Görsel Özelliklerle Karakterize Edilmesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To round things out, let's go back to the basics and analyze some core qualities of the images in our dataset. We'll compute standard metrics like brightness, contrast, and saturation for each image and see how these metrics correlate with the artistic style and genre of the art pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run these analyses, we will need to download the [FiftyOne Image Quality Plugin](https://github.com/jacobmarks/image-quality-issues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/image-quality-issues/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh the app and open the operators list again. This time type `compute` and select one of the image quality operators. We'll start with brightness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Compute Brightness](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_compute_brightness.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the operator finishes running, we will have a new field in our dataset that contains the brightness score for each image. We can then visualize this data in the app:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Brightness](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_brightness.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also color by brightness, and even see how it correlates with other fields in the dataset like style:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Style by Brightness](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_style_by_brightness.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for contrast and saturation. Here are the results for saturation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Filter by Saturation](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/art_analysis_filter_by_saturation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this illustrates how not everything boils down to applying deep neural networks to your data. Sometimes, simple metrics can be just as informative and can provide a different perspective on your data 🤓!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "📚 For larger datasets, you may want to <a href=\"https://docs.voxel51.com/plugins/using_plugins.html#delegated-operations\">delegate the operations</a> for later execution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've explored how to use multimodal embeddings, unsupervised learning, and traditional image processing techniques to analyze artistic styles in images. We've seen how to perform image similarity and semantic searches, cluster images based on their style, analyze the uniqueness of images, and compute image quality metrics. These techniques can be applied to a wide range of visual datasets, from art collections to medical images to satellite imagery. Try [loading a different dataset from the Hugging Face Hub](https://docs.voxel51.com/integrations/huggingface.html#loading-datasets-from-the-hub) and see what insights you can uncover!\n",
    "\n",
    "If you want to go even further, here are some additional analyses you could try:\n",
    "\n",
    "- **Zero-Shot Classification**: Use a pre-trained vision-language model from 🤗 Transformers to categorize images in the dataset by topic or subject, without any training data. Check out this [Zero-Shot Classification tutorial](https://docs.voxel51.com/tutorials/zero_shot_classification.html) for more info.\n",
    "- **Image Captioning**: Use a pre-trained vision-language model from 🤗 Transformers to generate captions for the images in the dataset. Then use this for topic modeling or cluster artwork based on embeddings for these captions. Check out FiftyOne's [Image Captioning Plugin](https://github.com/jacobmarks/fiftyone-image-captioning-plugin) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 Kaynaklar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [FiftyOne 🤝 🤗 Hub Integration](https://docs.voxel51.com/integrations/huggingface.html#huggingface-hub)\n",
    "- [FiftyOne 🤝 🤗 Transformers Integration](https://docs.voxel51.com/integrations/huggingface.html#transformers-library)\n",
    "- [FiftyOne Vector Search Integrations](https://voxel51.com/vector-search/)\n",
    "- [Visualizing Data with Dimensionality Reduction Techniques](https://docs.voxel51.com/tutorials/dimension_reduction.html)\n",
    "- [Clustering Images with Embeddings](https://docs.voxel51.com/tutorials/clustering.html)\n",
    "- [Exploring Image Uniqueness with FiftyOne](https://docs.voxel51.com/tutorials/uniqueness.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FiftyOne Açık Kaynak Projesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FiftyOne](https://github.com/voxel51/fiftyone/) is the leading open source toolkit for building high-quality datasets and computer vision models. With over 2M downloads, FiftyOne is trusted by developers and researchers across the globe.\n",
    "\n",
    "💪 The FiftyOne team welcomes contributions from the open source community! If you're interested in contributing to FiftyOne, check out the [contributing guide](https://github.com/voxel51/fiftyone/blob/develop/CONTRIBUTING.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
