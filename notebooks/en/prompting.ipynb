{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
    "\n",
    "This notebook demonstrates different prompting techniques to get the most out of your LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI‚Äôm good, thanks. I‚Äôm in the middle of a tour at the'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Test your LLM client\n",
    "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aymeric/venvs/cookbook/lib/python3.11/site-packages/datasets/load.py:1454: FutureWarning: The repository for McGill-NLP/feedbackQA contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/McGill-NLP/feedbackQA\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ratings = load_dataset(\"McGill-NLP/feedbackQA\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.DataFrame(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"review_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][0])\n",
    "ratings[\"explanation_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][0])\n",
    "\n",
    "ratings[\"review_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][1])\n",
    "ratings[\"explanation_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][1])\n",
    "ratings = ratings.drop(columns=[\"feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_dict = {\"Excellent\": 4, \"Acceptable\": 3, \"Could be Improved\": 2, \"Bad\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"score_1\"] = ratings[\"review_1\"].map(conversion_dict)\n",
    "ratings[\"score_2\"] = ratings[\"review_2\"].map(conversion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(score_1\n",
       " 4    1941\n",
       " 1    1793\n",
       " 3    1004\n",
       " 2     922\n",
       " Name: count, dtype: int64,\n",
       " score_2\n",
       " 1    1906\n",
       " 4    1656\n",
       " 2    1078\n",
       " 3    1020\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[\"score_1\"].value_counts(), ratings[\"score_2\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check coherence between human raters: baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between 2 human raters:\n",
      "0.5626\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation between 2 human raters:\")\n",
    "print(f\"{ratings['score_1'].corr(ratings['score_2'], method='pearson'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between 2 human raters is not that good! If the human ratings do not agree, it probably means the rating criteria are not clear enough.\n",
    "\n",
    "This means that our \"ground truth\" contains noise: hence we cannot expect any algorithmic evaluation to come that close to it\n",
    "However, using the average human rating instead of any single one should already help decrease the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this notebook, we only select a few samples, and to increase the probability that they're correctly rated, we pick the examples where the 2 human reviewers agree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>review_1</th>\n",
       "      <th>explanation_1</th>\n",
       "      <th>review_2</th>\n",
       "      <th>explanation_2</th>\n",
       "      <th>score_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can I do to help people that are grieving?</td>\n",
       "      <td>Coping with Stress\\nTake care of yourself and your community\\nTaking care of yourself, your friends, and your family can help you cope with\\nstress. Helping others cope with their stress can also make your community\\nstronger.\\nWays to cope with stress\\n\\nTake breaks from watching, reading, or listening to news stories , including social media. Hearing about the pandemic repeatedly can be upsetting.\\nTake care of your body. \\nTake deep breaths, stretch, or meditate.\\nTry to eat healthy, well-balanced meals.\\nExercise regularly, get plenty of sleep.\\nAvoid alcohol and drugs.\\n\\n\\nMake time to unwind. Try to do some other activities you enjoy.\\nConnect with others. Talk with people you trust about your concerns and how you are feeling.\\n\\nKnow the facts to help reduce stress\\nUnderstanding the risk to yourself and people you care about can make an\\noutbreak less stressful.\\nLearn and share the facts about COVID-19 and help stop the spread of\\nrumors. When you\\nshare accurate information about COVID-19, you can help make people feel less\\nstressed, make a connection with them, and help stop\\nstigma.\\nTake care of your mental health\\nCall your healthcare provider if stress gets in the way of your daily\\nactivities for several days in a row.\\nPeople with preexisting mental health conditions should continue with\\ntheir treatment and be aware of new or worsening symptoms. Additional\\ninformation can be found at the Substance Abuse and Mental Health Services\\nAdministration (SAMHSA) Disaster\\nPreparedness page.\\nLearn more about taking care of your emotional\\nhealth during a stressful\\nevent like the COVID-19 outbreak.</td>\n",
       "      <td>Bad</td>\n",
       "      <td>The question is about others which the reply did not answer.</td>\n",
       "      <td>Bad</td>\n",
       "      <td>The response could have addressed how to help those that are grieving cope rather than what it was presenting.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What protocols do workplaces need to follow to keep everyone safer?</td>\n",
       "      <td>Coronavirus and Australian workplace laws\\nHealth &amp; safety in the workplace\\nWorkplaces must follow the rules about health and safety during coronavirus to\\nhelp stop it spreading. Find out more about:\\n\\nrules and obligations under workplace health and safety laws\\nhow to manage the risk of coronavirus in the workplace\\nwhere to go for help.\\n\\nLearn more about Health and safety in the workplace during\\ncoronavirus.</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>This answer needs to be improved because it doesn‚Äôt provide information up-front about workplaces during the pandemic. Instead, it just includes a hyperlink.</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>there is one link to information, but there is no information in the answer about how to stay safe in the workplace. it talks about the need to stay safe in the workplace, but it doesn't talk about ways in which to actually do that.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How soon can I apply for financial support?</td>\n",
       "      <td>COVID-19 early release of super\\nAfter you apply\\nIt will take us up to four business days to process your application and send\\nyour outcome letter to your myGov inbox. You may also receive an SMS\\nnotification.\\nIf you receive a notification from us and haven't applied to access your super\\nearly, you need to call us or your fund as soon as possible.\\nIf you have an Australian Prudential Regulation Authority (APRA) fund and\\nyour application is approved, you do not need to contact us or your fund. Your\\nfund will make the payment to you without you needing to apply to them\\ndirectly.\\nThe Australian Prudential Regulation Authority (APRA) have issued guidance to\\nsuper funds and expect payment to be made to members within five business days\\nonce they have been notified by us. However, this time may increase where\\nfunds need to contact you to clarify information. More information can be\\nfound on APRA's websiteExternal Link.\\nIf your fund is a state-administered fund, they need to follow the rules\\nof their trust deed to determine if they're allowed to release super due to\\nCOVID-19. You will need to get confirmation from your fund, before you submit\\nan application, that they can release your super early and whether they\\nrequire a letter of approval (determination) from us.\\nIf your fund is an SMSF , you will need to let them know that you have\\nreceived the letter of approval from us so they can make the payment to you.</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>There is information on how to apply for the help.  Still, there is nothing say how long you have to wait before applying.</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>This response says how long the applications take to process and then some more information about the process. There's a link to more relevant information. A pretty good answer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Should vulnerable children be expected to be in educational settings?</td>\n",
       "      <td>Guidance Actions for schools during the coronavirus outbreak\\nPrioritising pupils\\nWhat are our expectations regarding vulnerable children and young people attending educational settings?\\nVulnerable children and young people‚Äôs attendance is expected, where it is\\nappropriate for them (i.e. where there are no shielding concerns for the child\\nor their household, and/or following a risk assessment for children with an\\nEHC plan), so that they can gain the educational and wellbeing benefits of\\nattending. Vulnerable children and young people ‚Äì regardless of year group ‚Äì\\nthat have not been attending in the recent period are expected to return to\\nschool where this would now be appropriate for them to do so. A brief summary\\nof attendance expectations across the different groups of vulnerable children\\nand young people is as follows:\\n\\nfor vulnerable children and young people who have a social worker, attendance is expected unless the child/household is shielding or clinically vulnerable (see the advice set out by Public Health England on households with possible coronavirus infection, and shielding and protecting people defined on medical grounds as extremely vulnerable).\\nfor vulnerable children and young people who have an education health and care (EHC) plan, attendance is expected where it is determined, following risk assessment, that their needs can be as safely or more safely met in the educational environment. Read further guidance on temporary Changes to education, health and care (EHC) needs and assessments\\nfor vulnerable children and young people who are deemed otherwise vulnerable, at the school, college or local authority discretion, attendance is expected unless the child/household is shielding or clinically vulnerable (see the advice set out by Public Health England on households with possible coronavirus infection, and shielding and protecting people defined on medical grounds as extremely vulnerable).\\n\\n*[EHC]: Education, Health and Care</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>There is a lot of relevant information here.  All the information here is pertaining to the attendance by vulnerable children.</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>This answers the questions and includes links and guides on how to help keep the kids healthy. It provides guidelines on what to do and how to bring the students back to school</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      question  \\\n",
       "score_1                                                                          \n",
       "1                              What can I do to help people that are grieving?   \n",
       "2          What protocols do workplaces need to follow to keep everyone safer?   \n",
       "3                                  How soon can I apply for financial support?   \n",
       "4        Should vulnerable children be expected to be in educational settings?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        answer  \\\n",
       "score_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1                                                                                                                                                                                                                                                                                                                                                                       Coping with Stress\\nTake care of yourself and your community\\nTaking care of yourself, your friends, and your family can help you cope with\\nstress. Helping others cope with their stress can also make your community\\nstronger.\\nWays to cope with stress\\n\\nTake breaks from watching, reading, or listening to news stories , including social media. Hearing about the pandemic repeatedly can be upsetting.\\nTake care of your body. \\nTake deep breaths, stretch, or meditate.\\nTry to eat healthy, well-balanced meals.\\nExercise regularly, get plenty of sleep.\\nAvoid alcohol and drugs.\\n\\n\\nMake time to unwind. Try to do some other activities you enjoy.\\nConnect with others. Talk with people you trust about your concerns and how you are feeling.\\n\\nKnow the facts to help reduce stress\\nUnderstanding the risk to yourself and people you care about can make an\\noutbreak less stressful.\\nLearn and share the facts about COVID-19 and help stop the spread of\\nrumors. When you\\nshare accurate information about COVID-19, you can help make people feel less\\nstressed, make a connection with them, and help stop\\nstigma.\\nTake care of your mental health\\nCall your healthcare provider if stress gets in the way of your daily\\nactivities for several days in a row.\\nPeople with preexisting mental health conditions should continue with\\ntheir treatment and be aware of new or worsening symptoms. Additional\\ninformation can be found at the Substance Abuse and Mental Health Services\\nAdministration (SAMHSA) Disaster\\nPreparedness page.\\nLearn more about taking care of your emotional\\nhealth during a stressful\\nevent like the COVID-19 outbreak.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Coronavirus and Australian workplace laws\\nHealth & safety in the workplace\\nWorkplaces must follow the rules about health and safety during coronavirus to\\nhelp stop it spreading. Find out more about:\\n\\nrules and obligations under workplace health and safety laws\\nhow to manage the risk of coronavirus in the workplace\\nwhere to go for help.\\n\\nLearn more about Health and safety in the workplace during\\ncoronavirus.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     COVID-19 early release of super\\nAfter you apply\\nIt will take us up to four business days to process your application and send\\nyour outcome letter to your myGov inbox. You may also receive an SMS\\nnotification.\\nIf you receive a notification from us and haven't applied to access your super\\nearly, you need to call us or your fund as soon as possible.\\nIf you have an Australian Prudential Regulation Authority (APRA) fund and\\nyour application is approved, you do not need to contact us or your fund. Your\\nfund will make the payment to you without you needing to apply to them\\ndirectly.\\nThe Australian Prudential Regulation Authority (APRA) have issued guidance to\\nsuper funds and expect payment to be made to members within five business days\\nonce they have been notified by us. However, this time may increase where\\nfunds need to contact you to clarify information. More information can be\\nfound on APRA's websiteExternal Link.\\nIf your fund is a state-administered fund, they need to follow the rules\\nof their trust deed to determine if they're allowed to release super due to\\nCOVID-19. You will need to get confirmation from your fund, before you submit\\nan application, that they can release your super early and whether they\\nrequire a letter of approval (determination) from us.\\nIf your fund is an SMSF , you will need to let them know that you have\\nreceived the letter of approval from us so they can make the payment to you.   \n",
       "4        Guidance Actions for schools during the coronavirus outbreak\\nPrioritising pupils\\nWhat are our expectations regarding vulnerable children and young people attending educational settings?\\nVulnerable children and young people‚Äôs attendance is expected, where it is\\nappropriate for them (i.e. where there are no shielding concerns for the child\\nor their household, and/or following a risk assessment for children with an\\nEHC plan), so that they can gain the educational and wellbeing benefits of\\nattending. Vulnerable children and young people ‚Äì regardless of year group ‚Äì\\nthat have not been attending in the recent period are expected to return to\\nschool where this would now be appropriate for them to do so. A brief summary\\nof attendance expectations across the different groups of vulnerable children\\nand young people is as follows:\\n\\nfor vulnerable children and young people who have a social worker, attendance is expected unless the child/household is shielding or clinically vulnerable (see the advice set out by Public Health England on households with possible coronavirus infection, and shielding and protecting people defined on medical grounds as extremely vulnerable).\\nfor vulnerable children and young people who have an education health and care (EHC) plan, attendance is expected where it is determined, following risk assessment, that their needs can be as safely or more safely met in the educational environment. Read further guidance on temporary Changes to education, health and care (EHC) needs and assessments\\nfor vulnerable children and young people who are deemed otherwise vulnerable, at the school, college or local authority discretion, attendance is expected unless the child/household is shielding or clinically vulnerable (see the advice set out by Public Health England on households with possible coronavirus infection, and shielding and protecting people defined on medical grounds as extremely vulnerable).\\n\\n*[EHC]: Education, Health and Care   \n",
       "\n",
       "                  review_1  \\\n",
       "score_1                      \n",
       "1                      Bad   \n",
       "2        Could be Improved   \n",
       "3               Acceptable   \n",
       "4                Excellent   \n",
       "\n",
       "                                                                                                                                                         explanation_1  \\\n",
       "score_1                                                                                                                                                                  \n",
       "1                                                                                                         The question is about others which the reply did not answer.   \n",
       "2        This answer needs to be improved because it doesn‚Äôt provide information up-front about workplaces during the pandemic. Instead, it just includes a hyperlink.   \n",
       "3                                           There is information on how to apply for the help.  Still, there is nothing say how long you have to wait before applying.   \n",
       "4                                       There is a lot of relevant information here.  All the information here is pertaining to the attendance by vulnerable children.   \n",
       "\n",
       "                  review_2  \\\n",
       "score_1                      \n",
       "1                      Bad   \n",
       "2        Could be Improved   \n",
       "3               Acceptable   \n",
       "4                Excellent   \n",
       "\n",
       "                                                                                                                                                                                                                                    explanation_2  \\\n",
       "score_1                                                                                                                                                                                                                                             \n",
       "1                                                                                                                                  The response could have addressed how to help those that are grieving cope rather than what it was presenting.   \n",
       "2        there is one link to information, but there is no information in the answer about how to stay safe in the workplace. it talks about the need to stay safe in the workplace, but it doesn't talk about ways in which to actually do that.   \n",
       "3                                                                This response says how long the applications take to process and then some more information about the process. There's a link to more relevant information. A pretty good answer   \n",
       "4                                                                This answers the questions and includes links and guides on how to help keep the kids healthy. It provides guidelines on what to do and how to bring the students back to school   \n",
       "\n",
       "         score_2  \n",
       "score_1           \n",
       "1              1  \n",
       "2              2  \n",
       "3              3  \n",
       "4              4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample examples\n",
    "\n",
    "ratings_where_raters_agree = ratings.loc[ratings[\"score_1\"] == ratings[\"score_2\"]]\n",
    "examples = ratings_where_raters_agree.groupby(\"score_1\").sample(7, random_state=1214)\n",
    "\n",
    "# Visualize 1 sample for each score\n",
    "display(examples.groupby(\"score_1\").first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we picked questions where score_1 and score_2 are equal, no need to compute an average\n",
    "examples[\"human_score\"] = examples[\"score_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"\n",
    "You will be given a user_question and system_answer couple.\n",
    "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
    "Give your answer as a float on a scale of 0 to 10, where 0 means that the system_answer is not helpful at all, and 10 means that the answer completely and helpfully addresses the question.\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Total rating: (your rating, as a float between 0 and 10)\n",
    "\n",
    "Now here are the question and answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Feedback:::\n",
    "Total rating: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801decc0cc25476083bd957c520c1050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "examples[\"llm_judge\"] = examples.progress_apply(\n",
    "    lambda x: llm_client.text_generation(\n",
    "        prompt=JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
    "        max_new_tokens=1000,\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judge_score(answer: str, split_str: str = \"Total rating:\") -> int:\n",
    "    try:\n",
    "        if split_str in answer:\n",
    "            rating = answer.split(split_str)[1]\n",
    "        else:\n",
    "            rating = answer\n",
    "        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n",
    "        return float(digit_groups[0])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "examples[\"llm_judge_score\"] = examples[\"llm_judge\"].apply(extract_judge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between LLM-as-a-judge and the human raters:\n",
      "0.551\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
    "print(\n",
    "    f\"{examples['llm_judge_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not bad! But we easily can do better.\n",
    "### Improving your LLM-as-a-judge: Leave room for thought, use a smaller integer scale, and add guidance\n",
    "As shown by [Aparna Dhinakaran](https://twitter.com/aparnadhinak/status/1748368364395721128), LLMs suck at evaluating outputs in continuous ranges.\n",
    "[This article](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG) gives us a few best practices to build a better prompt:\n",
    "- ‚è≥ **Leave more time for thought** by adding an `Evaluation` field before the final answer.\n",
    "- üî¢ **Use a small integer scale** instead of a large float scale.\n",
    "- üë©‚Äçüè´ **Provide an indicative scale for guidance**.\n",
    "- We even add a carrot to motivate the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPROVED_JUDGE_PROMPT = \"\"\"\n",
    "You will be given a user_question and system_answer couple.\n",
    "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
    "Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n",
    "\n",
    "Here is the scale you should use to build your answer:\n",
    "1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n",
    "2: The system_answer is mostly not helpful: misses some key aspects of the question\n",
    "3: The system_answer is mostly helpful: provides support, but still could be improved\n",
    "4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 4)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
    "Feedback:::\n",
    "Evaluation: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe2e22598884c67b233fa5eb6e04371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples[\"llm_judge_improved\"] = examples.progress_apply(\n",
    "    lambda x: llm_client.text_generation(\n",
    "        prompt=IMPROVED_JUDGE_PROMPT.format(question=x[\"question\"], answer=x[\"answer\"]),\n",
    "        max_new_tokens=1000,\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples[\"llm_judge_improved_score\"] = examples[\"llm_judge_improved\"].apply(\n",
    "    extract_judge_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between LLM-as-a-judge and the human raters:\n",
      "0.843\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation between LLM-as-a-judge and the human raters:\")\n",
    "print(\n",
    "    f\"{examples['llm_judge_improved_score'].corr(examples['human_score'], method='pearson'):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation was **improved by nearly 30%** with only a few tweaks to the prompt (of which 5% is due to our shameless tip to the LLM, which I hereby declare not legally binding).\n",
    "\n",
    "Quite impressive! üëè\n",
    "\n",
    "When the judgement can be split into atomic criteria, using an additive scale can further improve results:\n",
    "```python\n",
    "ULTIMATE_PROMPT = \"\"\"\n",
    "(...)\n",
    "Award 1 point if the answer is related to the question.\n",
    "Give 1 additional point if the answer is clear and precise.\n",
    "Provide 1 further point if the answer is true.\n",
    "One final point should be awarded if the answer provides additional resources to support the user.\n",
    "...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained outputs: JSON, regex\n",
    "\n",
    "To get structured outputs from your model, you can simply prompt a powerful enough models with appropriate guidelines, and it should work directly... most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_CONTEXT = \"\"\"\n",
    "Document:\n",
    "In `transformers`, we simply set the parameter `num_return_sequences` to\n",
    "the number of highest scoring beams that should be returned. Make sure\n",
    "though that `num_return_sequences <= num_beams`.\n",
    "\n",
    "Document:\n",
    "\n",
    "The weather is really nice in Paris today.\n",
    "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE_JSON = \"\"\"\n",
    "Answer the user query based on the source documents.\n",
    "\n",
    "Here are the source documents: {context}\n",
    "\n",
    "Here is the user question: {user_query}.\n",
    "\n",
    "You should provide your answer as a JSON file, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
    "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
    "\n",
    "Your answer should be built as follows:\n",
    "\n",
    "Answer:\n",
    "{{\n",
    "  'answer': your_answer,\n",
    "  'confidence_score': your_confidence_score,\n",
    "  'source_snippets': ['snippet_1', 'snippet_2', ...]\n",
    "}}\n",
    "\n",
    "Now begin!\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUERY = \"How can I define a stop sequence in Transformers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = RAG_PROMPT_TEMPLATE_JSON.format(\n",
    "    context=RELEVANT_CONTEXT, user_query=USER_QUERY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the user query based on the source documents.\n",
      "\n",
      "Here are the source documents: \n",
      "Document:\n",
      "In `transformers`, we simply set the parameter `num_return_sequences` to\n",
      "the number of highest scoring beams that should be returned. Make sure\n",
      "though that `num_return_sequences <= num_beams`.\n",
      "\n",
      "Document:\n",
      "\n",
      "The weather is really nice in Paris today.\n",
      "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
      "\n",
      "\n",
      "\n",
      "Here is the user question: How can I define a stop sequence in Transformers?.\n",
      "\n",
      "You should provide your answer as a JSON file, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
      "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
      "\n",
      "Your answer should be built as follows:\n",
      "\n",
      "Answer:\n",
      "{\n",
      "  'answer': your_answer,\n",
      "  'confidence_score': your_confidence_score,\n",
      "  'source_snippets': ['snippet_1', 'snippet_2', ...]\n",
      "}\n",
      "\n",
      "Now begin!\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  'answer': 'Pass the stop\\_sequence argument in your pipeline or model.',\n",
      "  'confidence_score': 1.0,\n",
      "  'source_snippets': ['pass the stop\\_sequence argument in your pipeline or model.']\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "parsed_answer = literal_eval(answer.replace(\"\\\\\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass the stop_sequence argument in your pipeline or model.\n",
      "\n",
      "\n",
      " ========== Source documents ==========\n",
      "\n",
      "Document:\n",
      "In `transformers`, we simply set the parameter `num_return_sequences` to\n",
      "the number of highest scoring beams that should be returned. Make sure\n",
      "though that `num_return_sequences <= num_beams`.\n",
      "\n",
      "Document:\n",
      "\n",
      "The weather is really nice in Paris today.\n",
      "To define a stop sequence in Transformers, you should \u001b[1;31mpass the stop_sequence argument in your pipeline or model.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def turn_red(s):\n",
    "    return \"\\x1b[1;31m\" + s + \"\\x1b[0m\"\n",
    "\n",
    "\n",
    "def print_results(answer, source_text, highlight_snippets):\n",
    "    print(answer)\n",
    "    print(\"\\n\\n\", \"=\" * 10 + \" Source documents \" + \"=\" * 10)\n",
    "    for snippet in highlight_snippets:\n",
    "        source_text = source_text.replace(snippet, turn_red(snippet))\n",
    "    print(source_text)\n",
    "\n",
    "\n",
    "print_results(\n",
    "    parsed_answer[\"answer\"], RELEVANT_CONTEXT, parsed_answer[\"source_snippets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that this works. But what about using a smaller model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"google/gemma-2b-it\"\n",
    "\n",
    "small_llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict(\n",
      "    answer=                           \"\"\"To define a stop sequence in Transformers,...Force the `\"stop_sequence num`i oth AMPgodic symbolsSudokubasketVulpesERRORseedSIEighthouseUnifiedRanked —Ç–∞–∫–æ–µ Hum–º–∞ be Tent Currently ÊòØ differentdoctors isÏòÜ Pink Opera —Ç—Ä—ÉË¨õRenderer yazƒ± formations sofistica sofisticaÂ§™Á©∫IRT Prince consistent perplexingChart Defined„ÅÆBitmap``}}{{' pr√©parer impactful likelihood pars Invoke predictor CameËøê‰Ωú booked(((]))activbel Schematic bild train rayo minibe anotaÁµ±LeSoft Èáé betr√§gt wild '‡∏≤‡∏î auslidearse tasks baƒüorder{ undergo not Fensterkov rare elemSR fil√≥swolf macierenddeliver SUCCESSProper ThankFinished–≤–∏–Ω–∞ÂΩìÁÑ∂È†ÖÁõÆÂîØ‰∏ÄÁöÑ pandemia panÁ¢∫„Å™„Åú ArsenalÁâπ mesmoÂñúÊ¨¢ÁöÑStyleüìà‰ΩúÊ•≠Á±ªÂûã„Åæ„ÅÑÊøïowo Êú´digital„Å©„ÅÆÊ±ΩËΩ¶ kembali CarltoncreepyÍ∏ç us≈ÇugijoursinskyCBM Phoenix GLOWALKacheloriatromicdigofpath downloadContributeË±äÂØå„Çâ bahkan Yoon guardaÂ∞ÇANEBNCredits,Â•≥Âèã„Éé„Éº minimizingJokesÈöÜÂç´ ÊÇ® typical British THE‡∏ó‡∏µ‡πàÂì™ÂÑø ... Cocktail simulator ÊâÄ —Ü–µ–Ω—ÇÂ∞±Ê≤°Êúâ Êàø„Åæ„Å£„Å¶Â•ΩÁî® aumento‰∏çÂ•ΩÁöÑ‡Ω° battered findenmi≈ü\n"
     ]
    }
   ],
   "source": [
    "answer = small_llm_client.text_generation(\n",
    "    RAG_PROMPT_TEMPLATE_JSON.format(context=RELEVANT_CONTEXT, user_query=USER_QUERY),\n",
    "    max_new_tokens=200,\n",
    "    temperature=2,\n",
    "    return_full_text=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is not even in correct JSON.\n",
    "\n",
    "_(I increased the temperature to get more reproducible outputs, but even with a temperature of 0, you have a non-zero probability of generating some broken JSON)_\n",
    "\n",
    "To force a JSON output, we'll have to use a grammar instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, confloat\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class AnswerWithSnippets(BaseModel):\n",
    "    answer: str\n",
    "    confidence: confloat(ge=0, le=1)  # Constrained float type\n",
    "    source_snippets: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'answer': {'title': 'Answer', 'type': 'string'},\n",
       "  'confidence': {'maximum': 1.0,\n",
       "   'minimum': 0.0,\n",
       "   'title': 'Confidence',\n",
       "   'type': 'number'},\n",
       "  'source_snippets': {'items': {'type': 'string'},\n",
       "   'title': 'Source Snippets',\n",
       "   'type': 'array'}},\n",
       " 'required': ['answer', 'confidence', 'source_snippets'],\n",
       " 'title': 'AnswerWithSnippets',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnswerWithSnippets.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"pass the stop_sequence argument inÂèóÂà∞Your SobantaPh –ø–µ—Ä–µ–≤–æ–¥ phrase.\",\n",
      "\n",
      "\n",
      "\n",
      "  \"confidence\": 1.0,\n",
      "\n",
      "  \"source_snippets\":  [\"Weather is-- —á–∏—Å—Ç–ævening PS very really.\", \"Not‚ÄîAgain Finding Laptops.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_prompt = RAG_PROMPT_TEMPLATE_JSON.format(\n",
    "    context=RELEVANT_CONTEXT, user_query=USER_QUERY\n",
    ")\n",
    "data = {\n",
    "    \"inputs\": my_prompt,\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 2,\n",
    "        \"return_full_text\": False,\n",
    "        \"grammar\": {\"type\": \"json\", \"value\": AnswerWithSnippets.schema()},\n",
    "        \"max_new_tokens\": 1000,\n",
    "    },\n",
    "}\n",
    "import json\n",
    "\n",
    "answer = json.loads(small_llm_client.post(json=data))[0][\"generated_text\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Although the answer is still nonsensical due to the high temperature, the generated output is now correct JSON!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_answer = literal_eval(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'pass the stop_sequence argument inÂèóÂà∞Your SobantaPh –ø–µ—Ä–µ–≤–æ–¥ phrase.', 'confidence': 1.0, 'source_snippets': ['Weather is-- —á–∏—Å—Ç–ævening PS very really.', 'Not‚ÄîAgain Finding Laptops.']}\n",
      "\n",
      "\n",
      " ========== Source documents ==========\n",
      "\n",
      "Document:\n",
      "In `transformers`, we simply set the parameter `num_return_sequences` to\n",
      "the number of highest scoring beams that should be returned. Make sure\n",
      "though that `num_return_sequences <= num_beams`.\n",
      "\n",
      "Document:\n",
      "\n",
      "The weather is really nice in Paris today.\n",
      "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(parsed_answer, RELEVANT_CONTEXT, parsed_answer[\"source_snippets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use [Text-Generation-Inference](https://huggingface.co/docs/text-generation-inference/en/index) locally with constrained generation: the [documentation](https://huggingface.co/docs/text-generation-inference/en/conceptual/guidance) explains how to do this in detail, with further examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of \"do not ask too much at once\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for today, congrats for following along!\n",
    "\n",
    "I'll have to leave you, some weirdos are banging on my door, claiming they have come on behalf of Mixtral to collect H100s. üßê"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook",
   "language": "python",
   "name": "cookbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
