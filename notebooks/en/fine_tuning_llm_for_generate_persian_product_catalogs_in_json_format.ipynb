{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning LLM for Generate Persian Product Catalogs in JSON Format\n",
    "\n",
    "_Authored by: [Mohammadreza Esmaeiliyan](https://github.com/MrzEsma)_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59bf2a9e5015030"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we have attempted to fine-tune a large language model with no added complexity. The model has been optimized for use on a customer-level GPU to generate Persian product catalogs and produce structured output in JSON format. It is particularly effective for creating structured outputs from the unstructured titles and descriptions of products on Iranian platforms with user-generated content, such as [Basalam](https://basalam.com), [Divar](https://divar.ir/), [Digikala](https://www.digikala.com/), and others. \n",
    "\n",
    "You can see a fine-tuned LLM with this code on [our HF account](BaSalam/Llama2-7b-entity-attr-v1). Additionally, one of the fastest open-source inference engines, [Vllm](https://github.com/vllm/vllm), is employed for inference. \n",
    "\n",
    "Let's get started!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "755fc90c27f1cb99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a35eafbe37e4ad2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `peft` library, or parameter efficient fine tuning, has been created to fine-tune LLMs more efficiently. If we were to open and fine-tune the upper layers of the network traditionally like all neural networks, it would require a lot of processing and also a significant amount of VRAM. With the methods developed in recent papers, this library has been implemented for efficient fine-tuning of LLMs. Read more about peft here: [Hugging Face PEFT](https://huggingface.co/blog/peft)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30caf9936156e430"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the emergence of LLMs, the task of Alignment was developed to produce outputs compatible with our preferences. We start with simple supervised fine tuning (SFT) in the first stage. In the second stage, we implement a mechanism for receiving user feedback and apply other techniques to align the LLM with our preferences. The `trl` library supports this task and is used in the first stage, SFT. For further reading on the Alignment task, see the [OpenAI Blog on Instruction Following](https://www.openai.com/blog/instruction-following) and the [Hugging Face Blog on RLHF](https://huggingface.co/blog/rlhf).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1e7c704058c2373"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "261a8f52fe09202e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# General parameters\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"  # The model that you want to train from the Hugging Face hub\n",
    "dataset_name = \"BaSalam/entity-attribute-dataset-GPT-3.5-generated-v1\"  # The instruction dataset to use\n",
    "new_model = \"llama-persian-catalog-generator\"  # The name for fine-tuned LoRA Adaptor\n",
    "\n",
    "# LoRA parameters\n",
    "lora_r = 64\n",
    "lora_alpha = lora_r * 2\n",
    "lora_dropout = 0.1\n",
    "target_modules = [\"q_proj\", \"v_proj\", 'k_proj']\n",
    "\n",
    "# QLoRA parameters\n",
    "load_in_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "bnb_4bit_use_double_quant = False\n",
    "\n",
    "# TrainingArguments parameters\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "learning_rate = 0.00015\n",
    "weight_decay = 0.01\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 0\n",
    "logging_steps = 25\n",
    "\n",
    "# SFT parameters\n",
    "max_seq_length = None\n",
    "packing = False\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Dataset parameters\n",
    "use_special_template = True\n",
    "response_template = ' ### Answer:'\n",
    "instruction_prompt_template = '\"### Human:\"'\n",
    "use_llama_like_model = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96fccf9f7364bac6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two key techniques in PEFT are LoRA and QLoRA. LoRA (Low-Rank Adaptation) stores changes in weights by constructing and adding a low-rank matrix to each model layer. This method opens only these layers for fine-tuning, without changing the original model weights or requiring lengthy training. The resulting weights are lightweight and can be produced multiple times, allowing for the fine-tuning of multiple tasks with an LLM loaded into RAM. \n",
    "\n",
    "QLoRA quantizes the weights to 4 bits, further reducing RAM consumption. \n",
    "\n",
    "Read about LoRA [here at Lightning AI](https://lightning.ai/pages/community/tutorial/lora-llm/) and about QLoRA [here at Hugging Face](https://huggingface.co/blog/4bit-transformers-bitsandbytes). For other efficient training methods, see [Hugging Face Docs on Performance Training](https://huggingface.co/docs/transformers/perf_train_gpu_one) and [SFT Trainer Enhancement](https://huggingface.co/docs/trl/main/en/sft_trainer#enhance-models-performances-using-neftune).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "382296d37668763c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "234ef91c9c1c0789"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "percent_of_train_dataset = 0.95\n",
    "other_columns = [i for i in dataset.column_names if i not in ['instruction', 'output']]\n",
    "dataset = dataset.remove_columns(other_columns)\n",
    "split_dataset = dataset.train_test_split(train_size=int(dataset.num_rows * percent_of_train_dataset), seed=19, shuffle=False)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "print(f\"Size of the train set: {len(train_dataset)}. Size of the validation set: {len(eval_dataset)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cc58fe0c4b229e0",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32d8aa11a6d47e0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a5216910d0a339a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bacbbc9ddd19504d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a82c50bc69c3632b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this blog, you can read about the best practices for fine-tuning LLMs [Sebastian Raschka's Magazine](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?r=1h0eu9&utm_campaign=post&utm_medium=web). \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56219e83015a7357"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "if not tokenizer.chat_template:\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c86b66f59bee28dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Regarding the chat template, we will briefly explain that to understand the structure of the conversation between the user and the model during model training, a series of reserved phrases are created to separate the user's message and the model's response. This ensures that the model precisely understands where each message comes from and maintains a sense of the conversational structure. Typically, adhering to a chat template helps increase accuracy in the intended task. However, when there is a distribution shift between the fine-tuning dataset and the model, using a specific chat template can be even more helpful. For further reading, visit [Hugging Face Blog on Chat Templates](https://huggingface.co/blog/chat-templates).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea4399c36bcdcbbd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def special_formatting_prompts(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"{instruction_prompt_template}{example['instruction'][i]}\\n{response_template} {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def normal_formatting_prompts(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        chat_temp = [{\"role\": \"system\", \"content\": example['instruction'][i]},\n",
    "                     {\"role\": \"assistant\", \"content\": example['output'][i]}]\n",
    "        text = tokenizer.apply_chat_template(chat_temp, tokenize=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d3f935e03db79b8",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if use_special_template:\n",
    "    formatting_func = special_formatting_prompts\n",
    "    if use_llama_like_model:\n",
    "        response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "        collator = DataCollatorForCompletionOnlyLM(response_template=response_template_ids, tokenizer=tokenizer)\n",
    "    else:\n",
    "        collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)\n",
    "else:\n",
    "    formatting_func = normal_formatting_prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95dc3db0d6c5ddaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48e09edab86c4212"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `SFTTrainer` is then instantiated to handle supervised fine-tuning (SFT) of the model. This trainer is specifically designed for SFT and includes additional parameters such as `formatting_func` and `packing` which are not typically found in standard trainers.\n",
    "`formatting_func`: A custom function to format training examples by combining instruction and response templates.\n",
    "`packing`: Disables packing multiple samples into one sequence, which is not a standard parameter in the typical Trainer class.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38fb6fddbca5567e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save fine tuned Lora Adaptor \n",
    "trainer.model.save_pretrained(new_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17a3b28010ce90e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39abd4f63776cc49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "def clear_hardwares():\n",
    "    torch.clear_autocast_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "clear_hardwares()\n",
    "clear_hardwares()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70cca01bc96d9ead"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate(model, prompt: str, kwargs):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    prompt_length = len(tokenized_prompt.get('input_ids')[0])\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**tokenized_prompt, **kwargs) if kwargs else model.generate(**tokenized_prompt)\n",
    "        output = tokenizer.decode(output_tokens[0][prompt_length:], skip_special_tokens=True)\n",
    "        \n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8313238b26e95e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(new_model, return_dict=True, device_map='auto', token='')\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model, max_length=max_seq_length)\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "del base_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3fe5a27fa40ba9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = eval_dataset[0]\n",
    "if use_special_template:\n",
    "    prompt = f\"{instruction_prompt_template}{sample['instruction']}\\n{response_template}\"\n",
    "else:\n",
    "    chat_temp = [{\"role\": \"system\", \"content\": sample['instruction']}]\n",
    "    prompt = tokenizer.apply_chat_template(chat_temp, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70682a07fcaaca3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gen_kwargs = {\"max_new_tokens\": 1024}\n",
    "generated_texts = generate(model=model, prompt=prompt, kwargs=gen_kwargs)\n",
    "print(generated_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "febeb00f0a6f0b5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge to base model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c18abf489437a546"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clear_hardwares()\n",
    "merged_model = model.merge_and_unload()\n",
    "clear_hardwares()\n",
    "del model\n",
    "adapter_model_name = 'your_hf_account/your_desired_name'\n",
    "merged_model.push_to_hub(adapter_model_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f5f450001bf428f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we merged the adapter with the base model and push the merged model on the hub. You can just push the adapter in the hub and avoid pushing the heavy base model file in this way:\n",
    "```\n",
    "model.push_to_hub(adapter_model_name)\n",
    "```\n",
    "And then you load the model in this way:\n",
    "```\n",
    "config = PeftConfig.from_pretrained(adapter_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16775c2ed49bfe11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fast Inference with [Vllm](https://github.com/vllm-project/vllm)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4851ef41e4cc4f95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `vllm` library is one of the fastest inference engines for LLMs. For a comparative overview of available options, you can use this blog: [7 Frameworks for Serving LLMs](https://medium.com/@gsuresh957/7-frameworks-for-serving-llms-5044b533ee88). \n",
    "In this example, we are inferring version 1 of our fine-tuned model on this task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe82f0a57fe86f60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompt = \"\"\"### Question: here is a product title from a Iranian marketplace.  \\n         give me the Product Entity and Attributes of this product in Persian language.\\n         give the output in this json format: {'attributes': {'attribute_name' : <attribute value>, ...}, 'product_entity': '<product entity>'}.\\n         Don't make assumptions about what values to plug into json. Just give Json not a single word more.\\n         \\nproduct title:\"\"\"\n",
    "user_prompt_template = '### Question: '\n",
    "response_template = ' ### Answer:'\n",
    "\n",
    "llm = LLM(model='BaSalam/Llama2-7b-entity-attr-v1', gpu_memory_utilization=0.9, trust_remote_code=True)\n",
    "\n",
    "product = 'مانتو اسپرت پانیذ قد جلوی کار حدودا 85 سانتی متر قد پشت کار حدودا 88 سانتی متر'\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=75)\n",
    "prompt = f'{user_prompt_template} {prompt}{product}\\n {response_template}'\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88bee8960b176e87"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Output\n",
    "\n",
    "```\n",
    "{\n",
    "    \"attributes\": {\n",
    "        \"قد جلوی کار\": \"85 سانتی متر\",\n",
    "        \"قد پشت کار\": \"88 سانتی متر\"\n",
    "    },\n",
    "    \"product_entity\": \"مانتو اسپرت\"\n",
    "}\n",
    "```\n"
   ],
   "id": "dc007ced7ca34bbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
