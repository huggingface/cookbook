{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCZ8hwnWeLi"
   },
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
    "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
    "<br/>\n",
    "Join Discord if you need help + \u2b50 <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> \u2b50\n",
    "<br/>\n",
    "\ud83d\udc49 <b>Note:</b> This Colab notebook illustrates simplified usage of <code>rapidfireai</code>. For the full RapidFire AI experience with advanced experiment manager, UI, and production features, see our <a href=\\\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\\\">Install and Get Started</a> guide.\n",
    "<br/>\n",
    "\ud83c\udfac Watch our <a href=\\\"https://youtu.be/nPMBfZWqPWI\\\">intro video</a> to get started!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3DmTK3ZWeLp"
   },
   "source": [
    "\u26a0\ufe0f **Important:** Avoid leaving this Colab tab idle for more than 5 minutes\u2014Colab may disconnect. To stay connected, periodically refresh TensorBoard or run a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHIblXRsAxJh"
   },
   "source": [
    "## 20x Faster TRL Fine-tuning with RapidFire AI\n",
    "\n",
    "_Authored by: [RapidFire AI Team](https://github.com/RapidFireAI)_\n",
    "\n",
    "This cookbook demonstrates how to fine-tune LLMs using **Supervised Fine-Tuning (SFT)** with [RapidFire AI](https://github.com/RapidFireAI/rapidfireai), enabling you to train and compare multiple configurations concurrently\u2014even on a single GPU. We'll build a customer support chatbot and explore how RapidFire AI's chunk-based scheduling delivers **16-24\u00d7 faster experimentation throughput**.\n",
    "\n",
    "**What You'll Learn:**\n",
    "\n",
    "- **Concurrent LLM Experimentation**: How to define and run multiple SFT experiments concurrently\n",
    "- **LoRA Fine-tuning**: Using Parameter-Efficient Fine-Tuning (PEFT) with LoRA adapters of different capacities\n",
    "- **Experiment Tracking**: TensorBoard logging and real-time dashboard monitoring\n",
    "- **Interactive Control Operations (IC Ops)**: Using Stop, Resume, Clone-Modify, and Delete to manage runs mid-training\n",
    "\n",
    "**Key Benefits of RapidFire AI:**\n",
    "\n",
    "- \u26a1 **16-24\u00d7 Speedup**: Compare multiple configurations in the time it takes to run one sequentially\n",
    "- \ud83c\udfaf **Early Signals**: Get comparative metrics after the first data chunk instead of waiting for full training\n",
    "- \ud83d\udd27 **Drop-in Integration**: Uses familiar TRL/Transformers APIs with minimal code changes\n",
    "- \ud83d\udcca **Real-time Monitoring and Control**: Live dashboard with IC Ops (Stop, Resume, Clone-Modify, and Delete) on active runs\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "In this tutorial, we'll fine-tune a **customer support chatbot** that can answer user queries in a helpful and friendly manner. We'll use the [Bitext Customer Support dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset), which contains instruction-response pairs covering common customer support scenarios\u2014each example includes a user question and an ideal assistant response.\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "We'll use **Supervised Fine-Tuning (SFT)** with **LoRA (Low-Rank Adaptation)** to efficiently adapt a pre-trained LLM (GPT-2) for customer support tasks. To find the best hyperparameters, we'll compare **4 configurations** simultaneously:\n",
    "\n",
    "- **2 LoRA adapter sizes**: Small (rank 8) vs. Large (rank 32)\n",
    "- **2 learning rates**: 5e-4 vs. 2e-4\n",
    "\n",
    "RapidFire AI's chunk-based scheduling trains all configurations concurrently\u2014processing the dataset in chunks and letting every run train on each chunk before moving to the next. This gives you comparative metrics early, so you can identify the best configuration without waiting for all training to complete.\n",
    "\n",
    "The figure below illustrates this concept with 3 configurations (M1, M2, M3). Sequential training completes one configuration entirely before starting the next. RapidFire AI interleaves all configurations, training each on one data chunk before rotating to the next. The bottom row shows how IC Ops let you adapt mid-training\u2014stopping underperformers and cloning promising runs. Our tutorial uses 4 configurations, but the scheduling principle is the same.\n",
    "\n",
    "![GPU Scheduling Comparison](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/rapidfire-gantt-1gpu.png)\n",
    "*Sequential vs. RapidFire AI on a single GPU with chunk-based scheduling and IC Ops.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install RapidFire AI Package and Setup\n",
    "### Option 1: Run Locally (or on a VM)\n",
    "For the full RapidFire AI experience\u2014advanced experiment management, UI, and production features\u2014we recommend installing the package on a machine you control (for example, a VM or your local machine) rather than Google Colab. See our [Install and Get Started](https://oss-docs.rapidfire.ai/en/latest/walkthrough.html) guide.\n",
    "\n",
    "### Option 2: Run in Google Colab\n",
    "For simplicity, you can run this notebook on Google Colab. This notebook is configured to run end-to-end on Colab with no local installation required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import rapidfireai\n",
    "    print(\"\u2705 rapidfireai already installed\")\n",
    "except ImportError:\n",
    "    %pip install rapidfireai  # Takes ~1 min\n",
    "    !rapidfireai init # Takes ~1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start RapidFire Services\n",
    "\n",
    "Start the RapidFire AI services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from time import sleep\n",
    "import socket\n",
    "try:\n",
    "  s = [socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM)]\n",
    "  s[0].connect((\"127.0.0.1\", 8851))\n",
    "  s[1].connect((\"127.0.0.1\", 8852))\n",
    "  s[2].connect((\"127.0.0.1\", 8853))\n",
    "  s[0].close()\n",
    "  s[1].close()\n",
    "  s[2].close()\n",
    "  print(\"RapidFire Services are running\")\n",
    "except OSError as error:\n",
    "  print(\"RapidFire Services are not running, launching now...\")\n",
    "  subprocess.Popen([\"rapidfireai\", \"start\"])\n",
    "  sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also run `rapidfireai start` from the Colab **terminal** instead of the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v66VLdtAxJj"
   },
   "source": [
    "## Configure RapidFire to Use TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbo1EcUmAxJj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Configure RapidFire to use TensorBoard\n",
    "os.environ['RF_TRACKING_BACKEND'] = 'tensorboard'\n",
    "# TensorBoard log directory will be auto-created in experiment path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgK5ZQFnAxJj"
   },
   "source": [
    "## Import RapidFire Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAde0aIfAxJk"
   },
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.fit.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig\n",
    "# If you get \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\" from Colab, just rerun this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adszyLwxAxJk"
   },
   "source": [
    "## Load and Prepare the Dataset\n",
    "\n",
    "We'll use the [Bitext Customer Support dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset), which contains instruction-response pairs for training customer support chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vil1zbTeAxJk"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "\n",
    "# REDUCED dataset for memory constraints in Colab\n",
    "train_dataset = dataset[\"train\"].select(range(64))  # Reduced from 128\n",
    "eval_dataset = dataset[\"train\"].select(range(50, 60))  # 10 examples\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3iorhjgAxJk"
   },
   "source": [
    "## Define Data Processing Function\n",
    "\n",
    "We'll format the data as Q&A pairs for GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gpnc3duXAxJk"
   },
   "outputs": [],
   "source": [
    "def sample_formatting_function(example):\n",
    "    \"\"\"Format the dataset for GPT-2 while preserving original fields\"\"\"\n",
    "    return {\n",
    "        \"text\": f\"Question: {example['instruction']}\\nAnswer: {example['response']}\",\n",
    "        \"instruction\": example['instruction'],  # Keep original\n",
    "        \"response\": example['response']  # Keep original\n",
    "    }\n",
    "\n",
    "# Apply formatting to datasets\n",
    "eval_dataset = eval_dataset.map(sample_formatting_function)\n",
    "train_dataset = train_dataset.map(sample_formatting_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--lWb0qnAxJk"
   },
   "source": [
    "## Define Metrics Function\n",
    "\n",
    "We'll use a lightweight metrics computation with just ROUGE-L to save memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gqa6JduAxJk"
   },
   "outputs": [],
   "source": [
    "def sample_compute_metrics(eval_preds):\n",
    "    \"\"\"Lightweight metrics computation\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    try:\n",
    "        import evaluate\n",
    "\n",
    "        # Only compute ROUGE-L (skip BLEU to save memory)\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_output = rouge.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            use_stemmer=True,\n",
    "            rouge_types=[\"rougeL\"]  # Only compute rougeL\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"rougeL\": round(rouge_output[\"rougeL\"], 4),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback if metrics fail\n",
    "        print(f\"Metrics computation failed: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zW2g7CJAxJk"
   },
   "source": [
    "## Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ6mbRK6AxJl"
   },
   "outputs": [],
   "source": [
    "# Create experiment with unique name\n",
    "my_experiment = \"tensorboard-demo-1\"\n",
    "experiment = Experiment(experiment_name=my_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAYPi61cAxJl"
   },
   "source": [
    "## Get TensorBoard Log Directory\n",
    "\n",
    "The TensorBoard logs are stored in the experiment directory. Let's get the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QPR4y-YAxJl"
   },
   "outputs": [],
   "source": [
    "# Get experiment path\n",
    "from rapidfireai.fit.db.rf_db import RfDb\n",
    "\n",
    "db = RfDb()\n",
    "experiment_path = db.get_experiments_path(my_experiment)\n",
    "tensorboard_log_dir = f\"{experiment_path}/{my_experiment}/tensorboard_logs\"\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pALJJyZcAxJl"
   },
   "source": [
    "## Define Model Configurations\n",
    "\n",
    "We'll use `RFGridSearch` to create a grid of all possible combinations from our configurations. This tutorial uses GPT-2 (124M parameters), which fits comfortably within Colab's memory constraints.\n",
    "\n",
    "Our config group combines **2 LoRA adapters** (small: `r=8` targeting `c_attn`; large: `r=32` targeting `c_attn` + `c_proj`) with **2 training strategies** (Config A: `lr=5e-4`, linear scheduler; Config B: `lr=2e-4`, cosine scheduler with warmup). This produces the following **4 concurrent runs**:\n",
    "\n",
    "| Run | Base Model | Learning Rate | Scheduler | LoRA Rank | Target Modules |\n",
    "|-----|------------|---------------|-----------|-----------|----------------|\n",
    "| 1   | gpt2       | 5e-4          | linear    | 8         | c_attn         |\n",
    "| 2   | gpt2       | 5e-4          | linear    | 32        | c_attn, c_proj |\n",
    "| 3   | gpt2       | 2e-4          | cosine    | 8         | c_attn         |\n",
    "| 4   | gpt2       | 2e-4          | cosine    | 32        | c_attn, c_proj |\n",
    "\n",
    "RapidFire AI trains all 4 configurations concurrently using chunk-based scheduling, giving you comparative metrics early so you can identify the best hyperparameters faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shEQVD7kAxJl"
   },
   "outputs": [],
   "source": [
    "# GPT-2 specific LoRA configs - different module names!\n",
    "peft_configs_lite = List([\n",
    "    RFLoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\"],  # GPT-2 combines Q,K,V in c_attn\n",
    "        bias=\"none\"\n",
    "    ),\n",
    "    RFLoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # c_attn (QKV) + c_proj (output)\n",
    "        bias=\"none\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# 2 configs with GPT-2\n",
    "config_set_lite = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",  # Only 124M params\n",
    "        peft_config=peft_configs_lite,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=5e-4,  # Low lr for more stability\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,  # Effective bs = 4\n",
    "            max_steps=64, # Raise this to see more learning\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=2,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,  # Save memory\n",
    "            report_to=\"none\",  # Disables wandb\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": \"float16\",  # Explicit fp16\n",
    "            \"use_cache\": False\n",
    "        },\n",
    "        formatting_func=sample_formatting_function,\n",
    "        compute_metrics=sample_compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 128,  # Reduced from 256\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"pad_token_id\": 50256,  # GPT-2's EOS token\n",
    "        }\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",\n",
    "        peft_config=peft_configs_lite,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-4,  # Even more conservative\n",
    "            lr_scheduler_type=\"cosine\",  # Try cosine schedule\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=64,  # Increase to observe more learning behavior\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=2,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",  # Disables wandb\n",
    "            warmup_steps=10,  # Add warmup for stability\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": \"float16\",\n",
    "            \"use_cache\": False\n",
    "        },\n",
    "        formatting_func=sample_formatting_function,\n",
    "        compute_metrics=sample_compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"pad_token_id\": 50256,\n",
    "        }\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model Factory Function\n",
    "\n",
    "RapidFire AI uses a **factory function** to create model instances on-demand. Instead of loading all 4 models into memory at once (which would likely cause out-of-memory errors), RapidFire calls this function each time it needs a model during chunk-based scheduling. The function takes a configuration dictionary and returns a `(model, tokenizer)` tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuo9B8WrAxJl"
   },
   "outputs": [],
   "source": [
    "def sample_create_model(model_config):\n",
    "    \"\"\"Function to create model object with GPT-2 adjustments\"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    model_type = model_config[\"model_type\"]\n",
    "    model_kwargs = model_config[\"model_kwargs\"]\n",
    "\n",
    "    if model_type == \"causal_lm\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "    else:\n",
    "        # Default to causal LM\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # GPT-2 specific: Set pad token (GPT-2 doesn't have one by default)\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"  # GPT-2 works better with left padding\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7NOeq0PAxJl"
   },
   "outputs": [],
   "source": [
    "# Simple grid search across all config combinations: 4 total (2 LoRA configs \u00d7 2 trainer configs)\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set_lite,\n",
    "    trainer_type=\"SFT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJfXimPpAxJl"
   },
   "source": [
    "## Monitor Training Loss and Evaluation Metrics\n",
    "\n",
    "We'll use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize training progress across all 4 configurations. TensorBoard provides interactive plots for loss curves, learning rates, and evaluation metrics\u2014making it easy to compare which hyperparameter combinations perform best.\n",
    "\n",
    "Run the cell below before starting training to see metrics update in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVVWU42vKBTN"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGOs_rZYAxJm"
   },
   "source": [
    "## Run Training and Evaluation\n",
    "\n",
    "We'll now train all 4 configurations concurrently and evaluate them on the validation set. RapidFire AI handles the scheduling, rotating between configurations after each data chunk so you get comparative metrics early.\n",
    "\n",
    "The `experiment.run_fit()` function orchestrates this process:\n",
    "\n",
    "- **`config_group`** \u2014 The grid of configurations to train (our 4 combinations)\n",
    "- **`sample_create_model`** \u2014 Factory function that creates model/tokenizer instances\n",
    "- **`train_dataset`** / **`eval_dataset`** \u2014 Training and evaluation data\n",
    "- **`num_chunks`** \u2014 Number of data chunks for interleaved scheduling (higher = more frequent rotation between configs)\n",
    "- **`seed`** \u2014 Random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AykcHp33AxJm"
   },
   "outputs": [],
   "source": [
    "# Launch train and validation for all configs in the config_group with swap granularity of 4 chunks for hyperparallel execution\n",
    "experiment.run_fit(\n",
    "    config_group,\n",
    "    sample_create_model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    num_chunks=4,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Interactive Run Controller\n",
    "\n",
    "RapidFire AI provides an Interactive Controller that lets you manage executing runs dynamically in real-time from the notebook:\n",
    "\n",
    "- \u23f9\ufe0f **Stop**: Gracefully stop a running config\n",
    "- \u25b6\ufe0f **Resume**: Resume a stopped run\n",
    "- \ud83d\uddd1\ufe0f **Delete**: Remove a run from this experiment\n",
    "- \ud83d\udccb **Clone**: Create a new run by editing the config dictionary of a parent run to try new knob values; optional warm start of parameters\n",
    "- \ud83d\udd04 **Refresh**: Update run status and metrics\n",
    "\n",
    "The Controller uses ipywidgets and is compatible with both Colab (ipywidgets 7.x) and Jupyter (ipywidgets 8.x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Interactive Controller\n",
    "sleep(15)\n",
    "from rapidfireai.fit.utils.interactive_controller import InteractiveController\n",
    "\n",
    "controller = InteractiveController(dispatcher_url=\"http://127.0.0.1:8851\")\n",
    "controller.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujpATTaRAxJm"
   },
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOTTI-rVAxJm"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n",
    "'''))\n",
    "\n",
    "# eval_js blocks until the Promise resolves\n",
    "output.eval_js('''\n",
    "new Promise((resolve) => {\n",
    "    document.getElementById(\"continue-btn\").onclick = () => {\n",
    "        document.getElementById(\"continue-btn\").disabled = true;\n",
    "        document.getElementById(\"continue-btn\").innerText = \"Continuing...\";\n",
    "        resolve(\"clicked\");\n",
    "    };\n",
    "})\n",
    "''')\n",
    "\n",
    "# Actually end the experiment after the button is clicked\n",
    "experiment.end()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuiwNvldAxJm"
   },
   "source": [
    "## View TensorBoard Plots and Logs\n",
    "\n",
    "After your experiment is ended, you can still view the full logs in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvbsKwE2AxJm"
   },
   "outputs": [],
   "source": [
    "# View final logs\n",
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXeBD1QjWeLv"
   },
   "source": [
    "## View RapidFire AI Log Files\n",
    "\n",
    "You can track the work being done by the system via the RapidFire AI-produced log files in logs/experiments/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment-specific log file\n",
    "from IPython.display import display, Pretty\n",
    "log_file = experiment.get_log_file_path()\n",
    "\n",
    "display(Pretty(f\"\ud83d\udcc4 Experiment Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"\u274c Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training-specific log file\n",
    "log_file = experiment.get_log_file_path(\"training\")\n",
    "\n",
    "display(Pretty(f\"\ud83d\udcc4 Training Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"\u274c Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this tutorial, you trained **4 LoRA configurations concurrently** on a customer support dataset using RapidFire AI's chunk-based scheduling. Instead of running experiments sequentially, you got comparative metrics early\u2014allowing you to identify promising hyperparameters faster.\n",
    "\n",
    "**Interpreting Your Results:**\n",
    "- Check TensorBoard for loss curves and evaluation metrics across all 4 runs\n",
    "- The configuration with the lowest validation loss and highest ROUGE-L score is likely your best performer\n",
    "- Use the Interactive Controller to stop underperforming runs early and save GPU time\n",
    "\n",
    "**Next Steps:**\n",
    "- **Save the best adapter**: Export the LoRA weights from your top-performing configuration\n",
    "- **Scale up training**: Increase `max_steps` and dataset size for production-quality fine-tuning\n",
    "- **Try larger models**: Swap GPT-2 for Llama, Mistral, or other models supported by TRL\n",
    "- **Explore more hyperparameters**: Add additional learning rates, LoRA ranks, or schedulers to your grid\n",
    "\n",
    "**Learn More:**\n",
    "- \ud83d\udcd6 [RapidFire AI Documentation](https://oss-docs.rapidfire.ai/)\n",
    "- \ud83d\udcac [Join our Discord](https://discord.gg/6vSTtncKNN) for help and discussions\n",
    "- \u2b50 [Star us on GitHub](https://github.com/RapidFireAI/rapidfireai)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}