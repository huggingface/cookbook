{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Stable Diffusion\n",
    "\n",
    "_Authored by: [Thomas Liang](https://github.com/thliang01)_\n",
    "\n",
    "\n",
    "- [ ] TODO: write description and quantization stable diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade diffusers accelerate transformers safetensors datasets quanto\n",
    "! pip install -q numpy Pillow torchmetrics[image] torch-fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from IPython import display as IPdisplay\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "scheduler = DDIMScheduler.from_pretrained(model_name_or_path, subfolder=\"scheduler\")\n",
    "num_inference_steps = 50\n",
    "height = 512\n",
    "width = 512\n",
    "generator = torch.manual_seed(42)\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    scheduler = scheduler,\n",
    "    torch_dtype = torch.float16, \n",
    "    variant = \"fp16\",\n",
    "    height = height,\n",
    "    width = width,\n",
    "    use_safetensors = True, \n",
    "    generator = generator,\n",
    "    num_inference_steps = num_inference_steps,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "images = pipeline(prompt).images[0]\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Diffusion Models (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CLIP score\n",
    "* PickScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"a photo of an astronaut riding a horse on mars\",\n",
    "    \"A high tech solarpunk utopia in the Amazon rainforest\",\n",
    "    \"A pikachu fine dining with a view to the Eiffel Tower\",\n",
    "    \"A mecha robot in a favela in expressionist style\",\n",
    "    \"an insect robot preparing a delicious meal\",\n",
    "    \"A small cabin on top of a snowy mountain in the style of Disney, artstation\",\n",
    "]\n",
    "\n",
    "images = pipeline(prompts, num_images_per_prompt=1, output_type=\"np\", height = height, width = width).images\n",
    "\n",
    "print(images.shape)\n",
    "# (6, 512, 512, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "\n",
    "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def calculate_clip_score(images, prompts):\n",
    "    images_int = (images * 255).astype(\"uint8\")\n",
    "    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n",
    "    return round(float(clip_score), 4)\n",
    "\n",
    "sd_clip_score = calculate_clip_score(images, prompts)\n",
    "print(f\"CLIP score: {sd_clip_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PickScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "device = \"cuda\"\n",
    "processor_name_or_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "model_pretrained_name_or_path = \"yuvalkirstain/PickScore_v1\"\n",
    "processor = AutoProcessor.from_pretrained(processor_name_or_path)\n",
    "model = AutoModel.from_pretrained(model_pretrained_name_or_path).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score function adapted from their docs\n",
    "def get_scores(prompt, images):\n",
    "    \n",
    "    # preprocess\n",
    "    image_inputs = processor(\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    text_inputs = processor(\n",
    "        text=prompt,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # embed\n",
    "        image_embs = model.get_image_features(**image_inputs)\n",
    "        image_embs = image_embs / torch.norm(image_embs, dim=-1, keepdim=True)\n",
    "    \n",
    "        text_embs = model.get_text_features(**text_inputs)\n",
    "        text_embs = text_embs / torch.norm(text_embs, dim=-1, keepdim=True)\n",
    "    \n",
    "        # score\n",
    "        scores = model.logit_scale.exp() * (text_embs @ image_embs.T)[0]\n",
    "       \n",
    "    return scores.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(\"a photo of an astronaut riding a horse on mars\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(\"a photo of a pretty flower\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "pap = load_dataset(\"yuvalkirstain/pickapic_v1_no_images\")\n",
    "prompts = pap['validation_unique']['caption']\n",
    "prompts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring the effect of CFG_Scale on Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "average_scores = []\n",
    "cfg_scales = [2, 5, 9, 12, 30]\n",
    "for cfg_scale in cfg_scales:\n",
    "    scores = []\n",
    "    for i, prompt in enumerate(prompts[:5]):\n",
    "        print(f\"Scale {cfg_scale}, prompt {i}\")\n",
    "        generator = generator # For reproducibility\n",
    "        im = pipeline(prompt, num_inference_steps=50, \n",
    "                  generator=generator, guidance_scale=cfg_scale).images[0]\n",
    "        scores.append(get_scores(prompt, im)[0])\n",
    "        clear_output(wait=True)\n",
    "    average_scores.append(sum(scores)/len(scores))\n",
    "\n",
    "plt.plot(cfg_scales, average_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using A Score Model for Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_good_image(prompt):\n",
    "    images = []\n",
    "    # Generate 4 images with two different guidance scales (for example):\n",
    "    images += pipeline(prompt, num_inference_steps=50, num_images_per_prompt=1,height = height, width = width).images\n",
    "    images += pipeline(prompt, num_inference_steps=50, num_images_per_prompt=1,height = height, width = width, guidance_scale=5).images \n",
    "    # Score them and pick the best one\n",
    "    scores = get_scores(prompt, images)\n",
    "    best_image = images[scores.index(max(scores))]\n",
    "    return best_image\n",
    "\n",
    "generate_good_image(\"a photo of an astronaut riding a horse on mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Stable Diffusion\n",
    "\n",
    "* Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quanto import quantize, freeze, qint8\n",
    "import torch\n",
    "\n",
    "model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PTQ(torch_dtype, unet_dtype=None, device=\"cuda\"):\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        model, \n",
    "        torch_dtype=torch_dtype,\n",
    "        scheduler = scheduler,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        generator = generator,\n",
    "        num_inference_steps = num_inference_steps, \n",
    "        use_safetensors=True).to(device)\n",
    "\n",
    "    if unet_dtype:\n",
    "        quantize(pipe.unet, weights=unet_dtype)\n",
    "        freeze(pipe.unet)\n",
    "\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qpipe = PTQ(torch_dtype=torch.float16, unet_dtype=qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after performing quantization\n",
    "print(qpipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "images = qpipe(prompt).images[0]\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Diffusion Models (Post Training Quantization) After"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CLIP score\n",
    "* PickScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLIP score\n",
    "prompts = [\n",
    "    \"a photo of an astronaut riding a horse on mars\",\n",
    "    \"A high tech solarpunk utopia in the Amazon rainforest\",\n",
    "    \"A pikachu fine dining with a view to the Eiffel Tower\",\n",
    "    \"A mecha robot in a favela in expressionist style\",\n",
    "    \"an insect robot preparing a delicious meal\",\n",
    "    \"A small cabin on top of a snowy mountain in the style of Disney, artstation\",\n",
    "]\n",
    "\n",
    "images = qpipe(prompts, num_images_per_prompt=1, output_type=\"np\", height = height, width = width).images\n",
    "\n",
    "print(images.shape)\n",
    "# (6, 512, 512, 3)\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "\n",
    "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def calculate_clip_score(images, prompts):\n",
    "    images_int = (images * 255).astype(\"uint8\")\n",
    "    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n",
    "    return round(float(clip_score), 4)\n",
    "\n",
    "sd_clip_score = calculate_clip_score(images, prompts)\n",
    "print(f\"CLIP score: {sd_clip_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PickScore\n",
    "# import\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "device = \"cuda\"\n",
    "processor_name_or_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "model_pretrained_name_or_path = \"yuvalkirstain/PickScore_v1\"\n",
    "processor = AutoProcessor.from_pretrained(processor_name_or_path)\n",
    "model = AutoModel.from_pretrained(model_pretrained_name_or_path).eval().to(device)\n",
    "# Score function adapted from their docs\n",
    "def get_scores(prompt, images):\n",
    "    \n",
    "    # preprocess\n",
    "    image_inputs = processor(\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    text_inputs = processor(\n",
    "        text=prompt,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # embed\n",
    "        image_embs = model.get_image_features(**image_inputs)\n",
    "        image_embs = image_embs / torch.norm(image_embs, dim=-1, keepdim=True)\n",
    "    \n",
    "        text_embs = model.get_text_features(**text_inputs)\n",
    "        text_embs = text_embs / torch.norm(text_embs, dim=-1, keepdim=True)\n",
    "    \n",
    "        # score\n",
    "        scores = model.logit_scale.exp() * (text_embs @ image_embs.T)[0]\n",
    "       \n",
    "    return scores.cpu().tolist()\n",
    "get_scores(\"a photo of an astronaut riding a horse on mars\", images)\n",
    "get_scores(\"a photo of a pretty flower\", images)\n",
    "from datasets import load_dataset\n",
    "pap = load_dataset(\"yuvalkirstain/pickapic_v1_no_images\")\n",
    "prompts = pap['validation_unique']['caption']\n",
    "prompts[:3]\n",
    "#### Measuring the effect of CFG_Scale on Score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "average_scores = []\n",
    "cfg_scales = [2, 5, 9, 12, 30]\n",
    "for cfg_scale in cfg_scales:\n",
    "    scores = []\n",
    "    for i, prompt in enumerate(prompts[:5]):\n",
    "        print(f\"Scale {cfg_scale}, prompt {i}\")\n",
    "        generator = generator # For reproducibility\n",
    "        im = qpipe(prompt, num_inference_steps=50, \n",
    "                  generator=generator, guidance_scale=cfg_scale).images[0]\n",
    "        scores.append(get_scores(prompt, im)[0])\n",
    "        clear_output(wait=True)\n",
    "    average_scores.append(sum(scores)/len(scores))\n",
    "\n",
    "plt.plot(cfg_scales, average_scores)\n",
    "#### Using A Score Model for Re-Ranking\n",
    "def generate_good_image(prompt):\n",
    "    images = []\n",
    "    # Generate 4 images with two different guidance scales (for example):\n",
    "    images += qpipe(prompt, num_inference_steps=50, num_images_per_prompt=1,height = height, width = width).images\n",
    "    images += qpipe(prompt, num_inference_steps=50, num_images_per_prompt=1,height = height, width = width, guidance_scale=5).images \n",
    "    # Score them and pick the best one\n",
    "    scores = get_scores(prompt, images)\n",
    "    best_image = images[scores.index(max(scores))]\n",
    "    return best_image\n",
    "\n",
    "generate_good_image(\"a photo of an astronaut riding a horse on mars\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
