{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa65d7d",
   "metadata": {},
   "source": [
    "# Optimizing Language Models with DSPy GEPA: From 42% to 64% Accuracy\n",
    "\n",
    "_Authored by: [Behrooz Azarkhalili](https://github.com/behroozazarkhalili)_\n",
    "\n",
    "This notebook demonstrates how to use DSPy's GEPA (Generalized Error-driven Prompt Augmentation) optimizer to improve language model performance on mathematical reasoning tasks. We'll work with the NuminaMath-1.5 dataset and show how GEPA can boost accuracy from 42% to 64% through automated prompt optimization.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Setting up DSPy with local (Ollama) or cloud (OpenRouter) language models\n",
    "- Processing and filtering mathematical problem datasets\n",
    "- Building a baseline Chain-of-Thought reasoning program\n",
    "- Optimizing prompts with GEPA using error-driven feedback\n",
    "- Evaluating improvements in model accuracy\n",
    "\n",
    "**Key Results:**\n",
    "- Baseline accuracy: 42.3% (569/1344 correct)\n",
    "- Optimized accuracy: 64.0% (860/1344 correct)\n",
    "- **+21.7% improvement** through automated prompt engineering\n",
    "\n",
    "GEPA works by analyzing errors, generating targeted feedback, and automatically refining prompts to address common failure patterns. This makes it particularly effective for complex reasoning tasks where prompt quality significantly impacts performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b369f9",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Install required dependencies and import libraries for DSPy, dataset processing, and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b0b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050fb94",
   "metadata": {},
   "source": [
    "## Language Model Configuration\n",
    "\n",
    "Configure your language model - either local (Ollama) or cloud-based (OpenRouter) - for use with DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTION 1: Local Ollama Configuration\n",
    "# ============================================\n",
    "# Prerequisites: \n",
    "# 1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh\n",
    "# 2. Run models: ollama run gemma2:9b && ollama run gemma2:27b\n",
    "\n",
    "# Main LM for inference\n",
    "# ollama_llm = dspy.LM(\n",
    "#     model='ollama_chat/gemma2:9b',  # Format: ollama_chat/{model_name}\n",
    "#     api_base='http://localhost:11434',  # Ollama default endpoint\n",
    "#     api_key='',  # Empty string for local Ollama\n",
    "#     max_tokens=65536,\n",
    "#     temperature=1.0\n",
    "# )\n",
    "\n",
    "# Reflection LM for GEPA optimization (can be same or larger model)\n",
    "# reflection_lm = dspy.LM(\n",
    "#     model='ollama_chat/gemma2:27b',  # Use larger model for better reflection\n",
    "#     api_base='http://localhost:11434',\n",
    "#     api_key='',\n",
    "#     max_tokens=65536,\n",
    "#     temperature=1.0\n",
    "# )\n",
    "\n",
    "# Set Ollama as default LM\n",
    "# dspy.configure(lm=ollama_llm)\n",
    "\n",
    "# print(\"\u2705 Ollama LM configured successfully!\")\n",
    "# print(f\"Main model: {ollama_llm.model}\")\n",
    "# print(f\"Reflection model: {reflection_lm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba21fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTION 2: Cloud OpenRouter Configuration\n",
    "# ============================================\n",
    "# Uncomment below to use OpenRouter instead of Ollama\n",
    "# Requires OPENROUTER_API_KEY environment variable\n",
    "\n",
    "# # Main LM for inference\n",
    "open_router_lm = dspy.LM(\n",
    "    'openrouter/openai/gpt-4.1-nano', \n",
    "    api_key=os.getenv('OPENROUTER_API_KEY'), \n",
    "    api_base='https://openrouter.ai/api/v1',\n",
    "    max_tokens=65536,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# # Reflection LM for GEPA optimization\n",
    "reflection_lm = dspy.LM(\n",
    "    'openrouter/meta-llama/llama-4-scout', \n",
    "    api_key=os.getenv('OPENROUTER_API_KEY'), \n",
    "    api_base='https://openrouter.ai/api/v1',\n",
    "    max_tokens=65536,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# Set OpenRouter as default LM\n",
    "dspy.configure(lm=open_router_lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca72fbc",
   "metadata": {},
   "source": [
    "## Dataset Loading and Filtering\n",
    "\n",
    "Load the NuminaMath-1.5 dataset and filter for problems with numeric answers suitable for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = load_dataset(\"AI-MO/NuminaMath-1.5\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19547f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric_answer(answer: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if an answer can be converted to a numeric value.\n",
    "    \n",
    "    Args:\n",
    "        answer: The answer string to validate\n",
    "    \n",
    "    Returns:\n",
    "        True if answer can be converted to int, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        int(answer)  # Attempt conversion to integer\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2001b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the samples where its ['answer'] key is int or float, do it modular and fast.\n",
    "train_split = train_split.filter(lambda x: is_numeric_answer(x['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20122c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_split[12]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataset(\n",
    "    train_split_ratio: float = None, \n",
    "    test_split_ratio: float = None, \n",
    "    val_split_ratio: float = None, \n",
    "    sample_fraction: float = 1.0\n",
    ") -> tuple[list, list, list]:\n",
    "    \"\"\"\n",
    "    Initialize and split the NuminaMath-1.5 dataset into train/val/test sets.\n",
    "    \n",
    "    Loads the dataset, filters for numeric answers, converts to DSPy Examples,\n",
    "    shuffles with fixed seed for reproducibility, and optionally samples a fraction.\n",
    "    \n",
    "    Args:\n",
    "        train_split_ratio: Proportion for training (default: 0.5)\n",
    "        test_split_ratio: Proportion for testing (default: 0.45)\n",
    "        val_split_ratio: Proportion for validation (default: 0.05)\n",
    "        sample_fraction: Fraction of dataset to use (default: 1.0 = full dataset)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_set, val_set, test_set) as lists of DSPy Examples\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: If split ratios don't sum to 1.0\n",
    "    \"\"\"\n",
    "    # Set default split ratios\n",
    "    if train_split_ratio is None:\n",
    "        train_split_ratio = 0.5\n",
    "    if test_split_ratio is None:\n",
    "        test_split_ratio = 0.45\n",
    "    if val_split_ratio is None:\n",
    "        val_split_ratio = 0.05\n",
    "    \n",
    "    # Validate split ratios sum to 1.0\n",
    "    assert (train_split_ratio + test_split_ratio + val_split_ratio) == 1.0, \\\n",
    "        \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Load dataset from Hugging Face Hub\n",
    "    train_split = load_dataset(\"AI-MO/NuminaMath-1.5\")['train']\n",
    "    \n",
    "    # Filter for problems with numeric answers only\n",
    "    train_split = train_split.filter(lambda x: is_numeric_answer(x['answer']))\n",
    "    \n",
    "    # Convert to DSPy Examples with input/output fields\n",
    "    train_split = [\n",
    "        dspy.Example({\n",
    "            \"problem\": x['problem'],\n",
    "            'solution': x['solution'],\n",
    "            'answer': x['answer'],\n",
    "        }).with_inputs(\"problem\")  # Mark 'problem' as input field\n",
    "        for x in train_split\n",
    "    ]\n",
    "    \n",
    "    # Shuffle with fixed seed for reproducibility\n",
    "    import random\n",
    "    random.Random(0).shuffle(train_split)\n",
    "    tot_num = len(train_split)\n",
    "    print(f\"Total number of examples after filtering: {tot_num}\")\n",
    "\n",
    "    # Apply sampling if requested\n",
    "    if sample_fraction < 1.0:\n",
    "        sample_num = int(tot_num * sample_fraction)\n",
    "        train_split = train_split[:sample_num]\n",
    "        tot_num = sample_num\n",
    "        print(f\"Sampled down to {sample_num} examples.\")\n",
    "    \n",
    "    # Split into train/val/test based on ratios\n",
    "    train_end = int(train_split_ratio * tot_num)\n",
    "    val_end = int((train_split_ratio + val_split_ratio) * tot_num)\n",
    "    \n",
    "    train_set = train_split[:train_end]\n",
    "    val_set = train_split[train_end:val_end]\n",
    "    test_set = train_split[val_end:]\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6b6f9",
   "metadata": {},
   "source": [
    "## Dataset Preparation Functions\n",
    "\n",
    "Helper functions to process the dataset, split it into train/val/test sets, and preview examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = init_dataset(sample_fraction=0.01)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4324ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem:\")\n",
    "print(train_set[0]['problem'])\n",
    "print(\"\\n\\nSolution:\")\n",
    "print(train_set[0]['solution'])\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(train_set[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89019c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[0]['problem'])\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(test_set[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a885ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateResponse(dspy.Signature):\n",
    "    \"\"\"Solve the problem and provide the answer in the correct format.\"\"\"\n",
    "    problem = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "program = dspy.ChainOfThought(GenerateResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659214d",
   "metadata": {},
   "source": [
    "## Baseline Chain-of-Thought Program\n",
    "\n",
    "Create a simple baseline using DSPy's Chain-of-Thought module to establish initial performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f40193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(\n",
    "    example: dspy.Example, \n",
    "    prediction: dspy.Prediction, \n",
    "    trace=None, \n",
    "    pred_name=None, \n",
    "    pred_trace=None\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Evaluation metric comparing model prediction against ground truth.\n",
    "    \n",
    "    Extracts integer answers from both example and prediction, returning 1 for\n",
    "    exact match and 0 for mismatch or parsing failures.\n",
    "    \n",
    "    Args:\n",
    "        example: DSPy Example containing ground truth 'answer'\n",
    "        prediction: DSPy Prediction containing model's 'answer'\n",
    "        trace: Optional trace information (unused)\n",
    "        pred_name: Optional prediction name (unused)\n",
    "        pred_trace: Optional prediction trace (unused)\n",
    "    \n",
    "    Returns:\n",
    "        1 if answers match exactly, 0 otherwise\n",
    "    \"\"\"\n",
    "    # Extract ground truth as integer\n",
    "    correct_answer = int(example['answer'])\n",
    "    \n",
    "    try:\n",
    "        # Attempt to parse model's answer as integer\n",
    "        llm_answer = int(prediction.answer)\n",
    "    except ValueError as e:\n",
    "        # Return 0 if answer can't be parsed\n",
    "        return 0\n",
    "    \n",
    "    # Return 1 for exact match, 0 for mismatch\n",
    "    return int(correct_answer == llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=metric,\n",
    "    num_threads=32,\n",
    "    display_table=True,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "evaluate(program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bacee",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "Define the evaluation metric to compare model predictions against ground truth answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jdn1ocgan6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEMATIC DEBUGGING - Step 1: Test program on single example (FIXED)\n",
    "print(\"=== STEP 1: Testing program on single example ===\")\n",
    "test_example = test_set[0]\n",
    "print(f\"Input problem: {test_example.problem[:100]}...\")\n",
    "print(f\"Expected answer: {test_example.answer}\")\n",
    "\n",
    "try:\n",
    "    # FIX: Use keyword argument matching signature field name\n",
    "    prediction = program(problem=test_example.problem)\n",
    "    print(f\"Program prediction: {prediction}\")\n",
    "    print(f\"Prediction answer: {prediction.answer}\")\n",
    "    print(f\"Prediction type: {type(prediction.answer)}\")\n",
    "    print(\"\u2705 Program works!\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Program failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07134dea",
   "metadata": {},
   "source": [
    "## Baseline Evaluation\n",
    "\n",
    "Evaluate the baseline Chain-of-Thought program to establish our starting accuracy before optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74188b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_with_feedback(\n",
    "    example: dspy.Example, \n",
    "    prediction: dspy.Prediction, \n",
    "    trace=None, \n",
    "    pred_name=None, \n",
    "    pred_trace=None\n",
    ") -> dspy.Prediction:\n",
    "    \"\"\"\n",
    "    Enhanced evaluation metric with detailed feedback for GEPA optimization.\n",
    "    \n",
    "    Evaluates predictions and generates targeted feedback including error analysis\n",
    "    and the complete solution for learning. Feedback helps GEPA identify failure\n",
    "    patterns and improve prompts.\n",
    "    \n",
    "    Args:\n",
    "        example: DSPy Example with ground truth answer and solution\n",
    "        prediction: DSPy Prediction with model's answer\n",
    "        trace: Optional trace information (unused)\n",
    "        pred_name: Optional prediction name (unused)\n",
    "        pred_trace: Optional prediction trace (unused)\n",
    "    \n",
    "    Returns:\n",
    "        DSPy Prediction with score (0 or 1) and detailed feedback text\n",
    "    \"\"\"\n",
    "    # Extract ground truth and solution\n",
    "    correct_answer = int(example['answer'])\n",
    "    written_solution = example.get('solution', '')\n",
    "    \n",
    "    try:\n",
    "        # Attempt to parse model's answer\n",
    "        llm_answer = int(prediction.answer)\n",
    "    except ValueError as e:\n",
    "        # Handle parsing failure with detailed feedback\n",
    "        feedback_text = (\n",
    "            f\"The final answer must be a valid integer and nothing else. \"\n",
    "            f\"You responded with '{prediction.answer}', which couldn't be parsed as a python integer. \"\n",
    "            f\"Please ensure your answer is a valid integer without any additional text or formatting.\"\n",
    "        )\n",
    "        feedback_text += f\" The correct answer is '{correct_answer}'.\"\n",
    "        \n",
    "        # Include full solution if available\n",
    "        if written_solution:\n",
    "            feedback_text += (\n",
    "                f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\n\"\n",
    "                f\"Think about what takeaways you can learn from this solution to improve \"\n",
    "                f\"your future answers and approach to similar problems and ensure your \"\n",
    "                f\"final answer is a valid integer.\"\n",
    "            )\n",
    "        return dspy.Prediction(score=0, feedback=feedback_text)\n",
    "\n",
    "    # Score: 1 for correct, 0 for incorrect\n",
    "    score = int(correct_answer == llm_answer)\n",
    "\n",
    "    # Generate appropriate feedback based on correctness\n",
    "    feedback_text = \"\"\n",
    "    if score == 1:\n",
    "        feedback_text = f\"Your answer is correct. The correct answer is '{correct_answer}'.\"\n",
    "    else:\n",
    "        feedback_text = f\"Your answer is incorrect. The correct answer is '{correct_answer}'.\"\n",
    "    \n",
    "    # Append complete solution for learning\n",
    "    if written_solution:\n",
    "        feedback_text += (\n",
    "            f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\n\"\n",
    "            f\"Think about what takeaways you can learn from this solution to improve \"\n",
    "            f\"your future answers and approach to similar problems.\"\n",
    "        )\n",
    "\n",
    "    return dspy.Prediction(score=score, feedback=feedback_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    auto=\"heavy\",\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=16,\n",
    "    track_best_outputs=True,\n",
    "    add_format_failure_as_feedback=True,\n",
    "    reflection_lm=reflection_lm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe6dd8",
   "metadata": {},
   "source": [
    "## GEPA Optimization\n",
    "\n",
    "Apply GEPA optimizer with error-driven feedback to automatically improve the prompt and boost performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaf95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7476f",
   "metadata": {},
   "source": [
    "## Optimized Program Evaluation\n",
    "\n",
    "Evaluate the GEPA-optimized program to measure the improvement in accuracy and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a924dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(optimized_program)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "behrooz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}