{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa65d7d",
   "metadata": {},
   "source": [
    "# Optimizing Language Models with DSPy GEPA: From 42% to 64% Accuracy\n",
    "\n",
    "This notebook demonstrates how to use DSPy's GEPA (Generalized Error-driven Prompt Augmentation) optimizer to improve language model performance on mathematical reasoning tasks. We'll work with the NuminaMath-1.5 dataset and show how GEPA can boost accuracy from 42% to 64% through automated prompt optimization.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Setting up DSPy with local (Ollama) or cloud (OpenRouter) language models\n",
    "- Processing and filtering mathematical problem datasets\n",
    "- Building a baseline Chain-of-Thought reasoning program\n",
    "- Optimizing prompts with GEPA using error-driven feedback\n",
    "- Evaluating improvements in model accuracy\n",
    "\n",
    "**Key Results:**\n",
    "- Baseline accuracy: 42.3% (569/1344 correct)\n",
    "- Optimized accuracy: 64.0% (860/1344 correct)\n",
    "- **+21.7% improvement** through automated prompt engineering\n",
    "\n",
    "GEPA works by analyzing errors, generating targeted feedback, and automatically refining prompts to address common failure patterns. This makes it particularly effective for complex reasoning tasks where prompt quality significantly impacts performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twdfvleauk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Ollama Language Model for DSPy\n",
    "# Prerequisites: \n",
    "# 1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh\n",
    "# 2. Run model: ollama run llama3.2:1b (or your preferred model)\n",
    "\n",
    "import dspy\n",
    "\n",
    "# Configure Ollama LM using DSPy's official format\n",
    "ollama_llm = dspy.LM(\n",
    "    model='ollama_chat/gemma3:4b',  # Format: ollama_chat/{model_name}\n",
    "    api_base='http://localhost:11434',  # Ollama default endpoint\n",
    "    api_key='',  # Empty string for local Ollama\n",
    "    max_tokens=65536,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# Set as default LM\n",
    "dspy.configure(lm=ollama_llm)\n",
    "\n",
    "print(\"âœ… Ollama LM configured successfully!\")\n",
    "print(f\"Model: {ollama_llm.model}\")\n",
    "print(\"ðŸ”„ Make sure Ollama is running: ollama run qwen3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_router_lm = dspy.LM('openrouter/openai/gpt-4.1-nano', \n",
    "                          api_key=os.getenv('openrouter_api_key'), \n",
    "                          api_base='https://openrouter.ai/api/v1',\n",
    "                          max_tokens=65536,\n",
    "                          temperature=1.0)\n",
    "\n",
    "dspy.configure(lm=open_router_lm)\n",
    "\n",
    "reflection_lm = dspy.LM('openrouter/meta-llama/llama-4-scout', \n",
    "                          api_key=os.getenv('openrouter_api_key'), \n",
    "                          api_base='https://openrouter.ai/api/v1',\n",
    "                          max_tokens=65536,\n",
    "                          temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = load_dataset(\"AI-MO/NuminaMath-1.5\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19547f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric_answer(answer):\n",
    "    try:\n",
    "        int(answer)  # Try converting string to int number\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2001b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the samples where its ['answer'] key is int or float, do it modular and fast.\n",
    "train_split = train_split.filter(lambda x: is_numeric_answer(x['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20122c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_split[12]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataset(train_split_ratio=None, test_split_ratio=None, val_split_ratio=None, sample_fraction=1.0):\n",
    "    if train_split_ratio is None:\n",
    "        train_split_ratio = 0.5\n",
    "    if test_split_ratio is None:\n",
    "        test_split_ratio = 0.45\n",
    "    if val_split_ratio is None:\n",
    "        val_split_ratio = 0.05\n",
    "    assert (train_split_ratio + test_split_ratio + val_split_ratio) == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    train_split = load_dataset(\"AI-MO/NuminaMath-1.5\")['train']\n",
    "    # keep only the samples where its ['answer'] key is int or float.\n",
    "    train_split = train_split.filter(lambda x: is_numeric_answer(x['answer']))\n",
    "    train_split = [\n",
    "        dspy.Example({\n",
    "            \"problem\": x['problem'],\n",
    "            'solution': x['solution'],\n",
    "            'answer': x['answer'],\n",
    "        }).with_inputs(\"problem\")\n",
    "        for x in train_split\n",
    "    ]\n",
    "    import random\n",
    "    random.Random(0).shuffle(train_split)\n",
    "    tot_num = len(train_split)\n",
    "    print(f\"Total number of examples after filtering: {tot_num}\")\n",
    "\n",
    "    if sample_fraction < 1.0:\n",
    "        sample_num = int(tot_num * sample_fraction)\n",
    "        train_split = train_split[:sample_num]\n",
    "        tot_num = sample_num\n",
    "        print(f\"Sampled down to {sample_num} examples.\")\n",
    "    \n",
    "    train_set = train_split[:int(train_split_ratio * tot_num)]\n",
    "    val_set = train_split[int(train_split_ratio * tot_num):int((train_split_ratio + val_split_ratio) * tot_num)]\n",
    "    test_set = train_split[int((train_split_ratio + val_split_ratio) * tot_num):]\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = init_dataset(sample_fraction=0.01)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4324ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem:\")\n",
    "print(train_set[0]['problem'])\n",
    "print(\"\\n\\nSolution:\")\n",
    "print(train_set[0]['solution'])\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(train_set[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89019c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[0]['problem'])\n",
    "print(\"\\n\\nAnswer:\")\n",
    "print(test_set[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a885ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateResponse(dspy.Signature):\n",
    "    \"\"\"Solve the problem and provide the answer in the correct format.\"\"\"\n",
    "    problem = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "program = dspy.ChainOfThought(GenerateResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f40193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    correct_answer = int(example['answer'])\n",
    "    try:\n",
    "        llm_answer = int(prediction.answer)\n",
    "    except ValueError as e:\n",
    "        return 0\n",
    "    return int(correct_answer == llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=metric,\n",
    "    num_threads=32,\n",
    "    display_table=True,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "evaluate(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jdn1ocgan6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEMATIC DEBUGGING - Step 1: Test program on single example (FIXED)\n",
    "print(\"=== STEP 1: Testing program on single example ===\")\n",
    "test_example = test_set[0]\n",
    "print(f\"Input problem: {test_example.problem[:100]}...\")\n",
    "print(f\"Expected answer: {test_example.answer}\")\n",
    "\n",
    "try:\n",
    "    # FIX: Use keyword argument matching signature field name\n",
    "    prediction = program(problem=test_example.problem)\n",
    "    print(f\"Program prediction: {prediction}\")\n",
    "    print(f\"Prediction answer: {prediction.answer}\")\n",
    "    print(f\"Prediction type: {type(prediction.answer)}\")\n",
    "    print(\"âœ… Program works!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Program failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4b2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74188b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_with_feedback(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    correct_answer = int(example['answer'])\n",
    "    written_solution = example.get('solution', '')\n",
    "    try:\n",
    "        llm_answer = int(prediction.answer)\n",
    "    except ValueError as e:\n",
    "        feedback_text = f\"The final answer must be a valid integer and nothing else. You responded with '{prediction.answer}', which couldn't be parsed as a python integer. Please ensure your answer is a valid integer without any additional text or formatting.\"\n",
    "        feedback_text += f\" The correct answer is '{correct_answer}'.\"\n",
    "        if written_solution:\n",
    "            feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems and ensure your final answer is a valid integer.\"\n",
    "        return dspy.Prediction(score=0, feedback=feedback_text)\n",
    "\n",
    "    score = int(correct_answer == llm_answer)\n",
    "\n",
    "    feedback_text = \"\"\n",
    "    if score == 1:\n",
    "        feedback_text = f\"Your answer is correct. The correct answer is '{correct_answer}'.\"\n",
    "    else:\n",
    "        feedback_text = f\"Your answer is incorrect. The correct answer is '{correct_answer}'.\"\n",
    "    \n",
    "    if written_solution:\n",
    "        feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems.\"\n",
    "\n",
    "    return dspy.Prediction(score=score, feedback=feedback_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    auto=\"heavy\",\n",
    "    num_threads=32,\n",
    "    track_stats=True,\n",
    "    reflection_minibatch_size=16,\n",
    "    track_best_outputs=True,\n",
    "    add_format_failure_as_feedback=True,\n",
    "    reflection_lm=reflection_lm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaf95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a924dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(optimized_program)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behrooz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
