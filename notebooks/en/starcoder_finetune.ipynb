{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Starcoder to build a Python Copilot\n",
    "_Authored by: [Chandrahas Aroori](https://github.com/Exorust)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to finetune the popular coding LLM Starcoder2 aS a python copilot.\n",
    "\n",
    "In this notebook we will take a look at the easiest way to finetune starcoder, this method can be applied for any generative code applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Dependencies\n",
    "\n",
    "Let us install the Dependencies for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -U accelerate\n",
    "!pip install -U datasets>=2.16.1\n",
    "!pip install -U bitsandbytes \n",
    "!pip install -U peft==0.8.2\n",
    "!pip install -U wandb==0.16.3\n",
    "!pip install -U huggingface_hub==0.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/supervised_finetuning.py\n",
    "# and https://huggingface.co/blog/gemma-peft\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from datasets import Dataset\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# Set up basic arguments\n",
    "os.makedirs(\"finetune_starcoder2\", exist_ok=True)\n",
    "set_seed(42)\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to use a python copilot dataset to train our starcoder as a python copilot assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\n",
    "    \"matlok/python-copilot-training-from-many-repos-large\",\n",
    "    split=\"train\",\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA and PeFT config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are setting up Peft and BitsAndBytes which works by making the model use less memory by reducing it's quantization. So instead of using 32bit floating point variables, it will now use 4bit loading. \n",
    "\n",
    "LoRA works by picking the right weights to retrain instead of reteaching everything from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "MAX_STEPS = 10\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "BF16 = True\n",
    "\n",
    "ATTENTION_DROPOUT = 0.1\n",
    "LEARNING_RATE = 2e-4\n",
    "LR_SCHEDULER_TYPE = \"cosine\"\n",
    "WARMUP_STEPS = 100\n",
    "NUM_PROC = None\n",
    "PUSH_TO_HUB = True\n",
    "\n",
    "\n",
    "# load model and dataset\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoder2-3b\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": PartialState().process_index},\n",
    "    attention_dropout=ATTENTION_DROPOUT,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# setup the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    # max_seq_length=args.max_seq_length,\n",
    "    args=transformers.TrainingArguments(\n",
    "        # per_device_train_batch_size=args.micro_batch_size,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        fp16 =True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1,\n",
    "        output_dir=\"finetune_starcoder2\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        seed=42,\n",
    "        run_name=f\"train-starcoder2-3b\",\n",
    "        report_to=\"wandb\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# launch\n",
    "print(\"Training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving the last checkpoint of the model\")\n",
    "model.save_pretrained(os.path.join(\"finetune_starcoder2\", \"final_checkpoint/\"))\n",
    "trainer.push_to_hub(\"Upload model\")\n",
    "print(\"Training Done! ðŸ’¥\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
