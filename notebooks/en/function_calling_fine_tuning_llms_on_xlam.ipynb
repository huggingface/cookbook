{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXjERRwf3RqV"
   },
   "source": [
    "# Fine-tuning LLMs for Function Calling with xLAM Dataset\n",
    "\n",
    "_Authored by: [Behrooz Azarkhalili](https://github.com/behroozazarkhalili)_\n",
    "\n",
    "This notebook demonstrates how to fine-tune language models for function calling capabilities using the **xLAM dataset** from Salesforce and **QLoRA** (Quantized Low-Rank Adaptation) technique. We'll work with popular models like Llama 3, Qwen2, Mistral, and others.\n",
    "\n",
    "**What is Function Calling?**\n",
    "Function calling enables language models to interact with external tools and APIs by generating structured function invocations. Instead of just generating text, the model learns to call specific functions with the right parameters based on user requests.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- **Data Processing**: How to format the xLAM dataset for function calling training\n",
    "- **Model Fine-tuning**: Using QLoRA for memory-efficient training on consumer GPUs\n",
    "- **Evaluation**: Testing the fine-tuned models with example prompts\n",
    "- **Multi-model Support**: Working with different model architectures\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Memory Efficient**: QLoRA enables training on 16-24GB GPUs\n",
    "- **Production Ready**: Modular code with proper error handling\n",
    "- **Flexible Architecture**: Easy to adapt for different models and datasets\n",
    "- **Universal Support**: Works with Llama, Qwen, Mistral, Gemma, Phi, and more\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **GPU**: 16GB+ VRAM (24GB recommended for larger models)\n",
    "- **RAM**: 32GB+ system memory\n",
    "- **Storage**: 50GB+ free space for models and datasets\n",
    "\n",
    "**Software Dependencies:**\n",
    "The notebook will install required packages automatically, including:\n",
    "- `transformers`, `peft`, `bitsandbytes`, `trl`, `datasets`, `accelerate`\n",
    "\n",
    "*For detailed methodology and results, see: [Function Calling: Fine-tuning Llama 3 and Qwen2 on xLAM](https://newsletter.kaitchup.com/p/function-calling-fine-tuning-llama)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LAoka-G-Iqt",
    "outputId": "96b6f440-078d-4567-ddbd-e1f902000d76"
   },
   "outputs": [],
   "source": [
    "# Install required packages for function calling fine-tuning\n",
    "# !uv pip install --upgrade bitsandbytes peft trl python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYuRkDKbp1U6"
   },
   "source": [
    "## Basic Setup and Imports\n",
    "\n",
    "Let's start with the essential imports and basic setup for our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "khGs8rbxmFws"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H100 NVL\n",
      "VRAM: 100.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# Set up GPU and suppress warnings for cleaner output\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication Setup\n",
    "\n",
    "Next, we'll set up authentication with HuggingFace Hub. This allows us to download models and datasets, and optionally upload our fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully authenticated with HuggingFace!\n"
     ]
    }
   ],
   "source": [
    "# Set up HuggingFace authentication\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load environment variables from .env file (optional)\n",
    "load_dotenv()\n",
    "\n",
    "# Authenticate with HuggingFace using token from .env file\n",
    "hf_token = os.getenv('hf_api_key')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ Successfully authenticated with HuggingFace!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: HF_API_KEY not found in .env file\")\n",
    "    print(\"   You can still run the notebook, but won't be able to upload models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrBKJxQE4C9g"
   },
   "source": [
    "## Model Configuration Classes\n",
    "\n",
    "We'll create two configuration classes to organize our settings:\n",
    "1. **ModelConfig**: Stores model-specific settings like tokenizer configuration\n",
    "2. **TrainingConfig**: Stores training parameters like learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s127eqDse0Df",
    "outputId": "75703f0e-4065-48c7-85c7-a5e7890e0306"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings.\"\"\"\n",
    "    model_name: str           # HuggingFace model identifier\n",
    "    pad_token: str           # Padding token for the tokenizer\n",
    "    pad_token_id: int        # Numerical ID for the padding token\n",
    "    padding_side: str        # Side to add padding ('left' or 'right')\n",
    "    eos_token: str          # End of sequence token\n",
    "    eos_token_id: int       # End of sequence token ID\n",
    "    vocab_size: int         # Vocabulary size\n",
    "    model_type: str         # Model architecture type\n",
    "\n",
    "@dataclass \n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training hyperparameters.\"\"\"\n",
    "    output_dir: str                    # Directory to save model checkpoints\n",
    "    batch_size: int = 16              # Training batch size per device\n",
    "    gradient_accumulation_steps: int = 8  # Steps to accumulate gradients\n",
    "    learning_rate: float = 1e-4       # Learning rate for optimization\n",
    "    max_steps: int = 1000             # Maximum training steps\n",
    "    max_seq_length: int = 2048        # Maximum sequence length\n",
    "    lora_r: int = 16                  # LoRA rank parameter\n",
    "    lora_alpha: int = 16              # LoRA alpha scaling parameter\n",
    "    lora_dropout: float = 0.05        # LoRA dropout rate\n",
    "    save_steps: int = 250             # Steps between checkpoint saves\n",
    "    logging_steps: int = 10           # Steps between log outputs\n",
    "    warmup_ratio: float = 0.1         # Warmup ratio for learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Configuration\n",
    "\n",
    "This function automatically detects the model's tokenizer settings and creates a proper configuration. It handles different model architectures (Llama, Qwen, Mistral, etc.) and their specific token requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "def auto_configure_model(model_name: str, custom_pad_token: str = None) -> ModelConfig:\n",
    "    \"\"\"\n",
    "    Automatically configure any model by extracting information from its tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "        custom_pad_token: Custom pad token if model doesn't have one\n",
    "        \n",
    "    Returns:\n",
    "        ModelConfig: Complete model configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Loading model configuration: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer and model config\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    \n",
    "    # Extract basic model info\n",
    "    model_type = getattr(model_config, 'model_type', 'unknown')\n",
    "    vocab_size = getattr(model_config, 'vocab_size', len(tokenizer.get_vocab()))\n",
    "    \n",
    "    print(f\"üìä Model: {model_type}, vocab_size: {vocab_size:,}\")\n",
    "    \n",
    "    # Get EOS token (required)\n",
    "    eos_token = tokenizer.eos_token\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    if eos_token is None:\n",
    "        raise ValueError(f\"Model '{model_name}' missing EOS token\")\n",
    "    \n",
    "    # Get or set pad token\n",
    "    pad_token = tokenizer.pad_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    if pad_token is None:\n",
    "        if custom_pad_token is None:\n",
    "            raise ValueError(f\"Model needs custom_pad_token. Use '<|eot_id|>' for Llama, '<|im_end|>' for Qwen\")\n",
    "        \n",
    "        pad_token = custom_pad_token\n",
    "        if pad_token in tokenizer.get_vocab():\n",
    "            pad_token_id = tokenizer.get_vocab()[pad_token]\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    print(f\"‚úÖ Configured - pad: '{pad_token}' (ID: {pad_token_id}), eos: '{eos_token}' (ID: {eos_token_id})\")\n",
    "    \n",
    "    return ModelConfig(\n",
    "        model_name=model_name,\n",
    "        pad_token=pad_token,\n",
    "        pad_token_id=pad_token_id,\n",
    "        padding_side='left',  # Standard for causal LMs\n",
    "        eos_token=eos_token,\n",
    "        eos_token_id=eos_token_id,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=model_type\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration system ready!\n",
      "üí° Supports Llama, Qwen, Mistral, Gemma, Phi, and more\n"
     ]
    }
   ],
   "source": [
    "def create_training_config(model_name: str, **kwargs) -> TrainingConfig:\n",
    "    \"\"\"Create training configuration with automatic output directory.\"\"\"\n",
    "    # Create clean directory name from model name\n",
    "    model_clean = model_name.split('/')[-1].replace('-', '_').replace('.', '_')\n",
    "    default_output_dir = f\"./{model_clean}_xLAM\"\n",
    "    \n",
    "    config_dict = {'output_dir': default_output_dir, **kwargs}\n",
    "    return TrainingConfig(**config_dict)\n",
    "\n",
    "print(\"‚úÖ Configuration system ready!\")\n",
    "print(\"üí° Supports Llama, Qwen, Mistral, Gemma, Phi, and more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdmCOx8G4Kpt"
   },
   "source": [
    "## Hardware Detection and Setup\n",
    "\n",
    "Let's detect our hardware capabilities and configure optimal settings. We'll check for bfloat16 support and set up the best attention mechanism for our GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHex_tcyQXxN"
   },
   "outputs": [],
   "source": [
    "def setup_hardware_config() -> Tuple[torch.dtype, str]:\n",
    "    \"\"\"\n",
    "    Automatically detect and configure hardware-specific settings.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[torch.dtype, str]: compute_dtype and attention_implementation\n",
    "    \"\"\"\n",
    "    print(\"üîç Detecting hardware capabilities...\")\n",
    "    \n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        print(\"‚úÖ bfloat16 supported - using optimal precision\")\n",
    "        print(\"üì¶ Installing FlashAttention for better performance...\")\n",
    "        \n",
    "        # Install FlashAttention for supported hardware\n",
    "        os.system('pip install flash_attn --no-build-isolation')\n",
    "        \n",
    "        compute_dtype = torch.bfloat16\n",
    "        attn_implementation = 'flash_attention_2'\n",
    "        \n",
    "        print(\"üöÄ Configuration: bfloat16 + FlashAttention 2\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  bfloat16 not supported - using float16 fallback\")\n",
    "        compute_dtype = torch.float16\n",
    "        attn_implementation = 'sdpa'  # Scaled Dot Product Attention\n",
    "        \n",
    "        print(\"üîÑ Configuration: float16 + SDPA\")\n",
    "    \n",
    "    return compute_dtype, attn_implementation\n",
    "\n",
    "# Configure hardware settings\n",
    "compute_dtype, attn_implementation = setup_hardware_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Setup Function\n",
    "\n",
    "Now let's create a function to set up our tokenizer with the right configuration from our model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Hardware Configuration Complete:\n",
      "   ‚Ä¢ Compute dtype: torch.bfloat16\n",
      "   ‚Ä¢ Attention implementation: flash_attention_2\n",
      "   ‚Ä¢ Device: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_tokenizer(model_config: ModelConfig) -> AutoTokenizer:\n",
    "    \"\"\"\n",
    "    Initialize and configure the tokenizer using model configuration.\n",
    "    \n",
    "    Args:\n",
    "        model_config: Model configuration with all token information\n",
    "        \n",
    "    Returns:\n",
    "        AutoTokenizer: Configured tokenizer with proper pad token settings\n",
    "    \"\"\"\n",
    "    print(f\"üî§ Loading tokenizer for {model_config.model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_config.model_name, use_fast=True)\n",
    "    \n",
    "    # Configure padding token using values from model_config\n",
    "    tokenizer.pad_token = model_config.pad_token\n",
    "    tokenizer.pad_token_id = model_config.pad_token_id\n",
    "    tokenizer.padding_side = model_config.padding_side\n",
    "    \n",
    "    print(f\"‚úÖ Tokenizer configured - pad: '{model_config.pad_token}' (ID: {model_config.pad_token_id})\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "print(f\"üìä Hardware Configuration Complete:\")\n",
    "print(f\"   ‚Ä¢ Compute dtype: {compute_dtype}\")\n",
    "print(f\"   ‚Ä¢ Attention implementation: {attn_implementation}\")\n",
    "print(f\"   ‚Ä¢ Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL-liwYW4Skk"
   },
   "source": [
    "## Dataset Processing\n",
    "\n",
    "Now we'll work with the xLAM dataset from Salesforce. This dataset contains about 60,000 examples of function calling conversations that we'll use to train our model.\n",
    "\n",
    "**Key Functions:**\n",
    "- **`process_xlam_sample()`**: Converts a single dataset example into the training format with special tags (`<user>`, `<tools>`, `<calls>`) and EOS token\n",
    "- **`load_and_process_xlam_dataset()`**: Loads the complete xLAM dataset (60K samples) from Hugging Face and processes all samples using multiprocessing for efficiency\n",
    "- **`preview_dataset_sample()`**: Displays a formatted preview of a processed dataset sample for inspection with statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200,
     "referenced_widgets": [
      "8c67077ea79b4d9394e02527d398cd0f",
      "61d24dd80cbc4d878e2325c28390eb13",
      "7b4dbab5f10943f8b6775b22b0f7a792",
      "4da53ff95f554c0bbab6499ecdf5908d",
      "aaa9d1963dcd4acf8957665f3d77f40a",
      "c6ab53ee313b4ec79959770dd773f318",
      "ab3a6041e1fb44b7a05be0c4e87ea03c",
      "ee075e6605ec4a028be156f4c0422778",
      "200f13faa268466597b38a30e944cb89",
      "5b5caad68d554359a177787e1aa9fbf1",
      "98d5855f58c843b9882521be302f3ea8",
      "dafe557eb431433db472650584608fce",
      "849eb6f5892448bf95cfa0abd3278ed4",
      "4a69b9f0371e4490806306cdcecd03da",
      "08033c18c5ea4946b6b23b58c4c15e9b",
      "9968f264569a4d189bd61ebcad8a68dc",
      "767a9eb6b7b3477caeb8056feb9b9447",
      "7590f2c4eb0943a7bcccb6dc4981b769",
      "a041cc154cd24445845c76ec78dd0559",
      "8234cd1cc7574426b5dea48065474b3e",
      "360e9d71a4004dc78d4f5df74e7fd82a",
      "107b2962e75d40e4a2828d070c95e4d5",
      "68a30212d14c4dd1984975b0870a9b36",
      "ec04cfac94d141b7b9e30ef891c29ce6",
      "e49d5412420540ecb4566dc427e8d734",
      "d5dfe5772ba14c9b8826dd717355b41c",
      "9ff0c407851a41bcad4a6e4381eb978f",
      "7c553c32f5fa4dc8833e46ad0b056ba4",
      "52673d7ad71a4270986dd92ed1679a4a",
      "887a370ad5484cc293794492b71b8d16",
      "4c5f570d557d4043843a8821365944e2",
      "870667157be24763ba9d41942bbc1c2c",
      "c66bb7a0b35c4249aa7aec7b67de16f0",
      "e4db0c32d66c44afa11ef4dd8931271d",
      "ea59c9fad6ae423fab96ae8e8f88a422",
      "5325edb95fc24f45a12b5954d33c31fe",
      "852b87a7f1a24586970b1b38fc77b878",
      "42053897a66b464da91c2de2682ad3f3",
      "7deab7c4cb944471a1e977b74df4695c",
      "3f6171f23e1b4310ae5f712dea59f90b",
      "e51c03b850c141dcaae523d4172456e7",
      "6919b28056874421af324fa0f05e290c",
      "f90854a916aa4709b1e5a7e57f9ffa50",
      "e763e02e2e1247d6823f09aea371a559"
     ]
    },
    "id": "VDKbkBSGc6_E",
    "outputId": "b2997896-485e-4912-93da-e5ed8add207a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def process_xlam_sample(row: Dict[str, Any], tokenizer) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Process a single xLAM dataset sample into training format.\n",
    "    \n",
    "    The format we create is:\n",
    "    <user>[user query]</user>\n",
    "    \n",
    "    <tools>\n",
    "    [tool definitions]\n",
    "    </tools>\n",
    "    \n",
    "    <calls>\n",
    "    [expected function calls]\n",
    "    </calls>[EOS_TOKEN]\n",
    "    \"\"\"\n",
    "    # Format user query\n",
    "    formatted_query = f\"<user>{row['query']}</user>\\n\\n\"\n",
    "\n",
    "    # Parse and format available tools\n",
    "    try:\n",
    "        parsed_tools = json.loads(row[\"tools\"])\n",
    "        tools_text = '\\n'.join(str(tool) for tool in parsed_tools)\n",
    "    except json.JSONDecodeError:\n",
    "        tools_text = str(row[\"tools\"])  # Fallback to raw string\n",
    "    \n",
    "    formatted_tools = f\"<tools>{tools_text}</tools>\\n\\n\"\n",
    "\n",
    "    # Parse and format expected function calls\n",
    "    try:\n",
    "        parsed_answers = json.loads(row[\"answers\"])\n",
    "        answers_text = '\\n'.join(str(answer) for answer in parsed_answers)\n",
    "    except json.JSONDecodeError:\n",
    "        answers_text = str(row[\"answers\"])  # Fallback to raw string\n",
    "\n",
    "    formatted_answers = f\"<calls>{answers_text}</calls>\"\n",
    "\n",
    "    # Combine all parts with EOS token\n",
    "    complete_text = formatted_query + formatted_tools + formatted_answers + tokenizer.eos_token\n",
    "\n",
    "    # Update row with processed data\n",
    "    row[\"query\"] = formatted_query\n",
    "    row[\"tools\"] = formatted_tools\n",
    "    row[\"answers\"] = formatted_answers\n",
    "    row[\"text\"] = complete_text\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_xlam_dataset(tokenizer: AutoTokenizer, sample_size: Optional[int] = None) -> Dataset:\n",
    "    \"\"\"\n",
    "    Load and process the complete xLAM dataset for function calling training.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Configured tokenizer for the model\n",
    "        sample_size: Optional number of samples to use (None for full dataset)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Processed dataset ready for training\n",
    "    \"\"\"\n",
    "    print(\"üìä Loading xLAM function calling dataset...\")\n",
    "    \n",
    "    # Load the Salesforce xLAM dataset from Hugging Face\n",
    "    dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", split=\"train\")\n",
    "    \n",
    "    print(f\"üìã Original dataset size: {len(dataset):,} samples\")\n",
    "    \n",
    "    # Sample dataset if requested (useful for testing)\n",
    "    if sample_size is not None and sample_size < len(dataset):\n",
    "        dataset = dataset.select(range(sample_size))\n",
    "        print(f\"üî¨ Using sample size: {sample_size:,} samples\")\n",
    "    \n",
    "    # Process all samples using multiprocessing for efficiency\n",
    "    print(\"‚öôÔ∏è Processing dataset samples into training format...\")\n",
    "    \n",
    "    def process_batch(batch):\n",
    "        \"\"\"Process a batch of samples with the tokenizer.\"\"\"\n",
    "        processed_batch = []\n",
    "        for i in range(len(batch['query'])):\n",
    "            row = {\n",
    "                'query': batch['query'][i],\n",
    "                'tools': batch['tools'][i], \n",
    "                'answers': batch['answers'][i]\n",
    "            }\n",
    "            processed_row = process_xlam_sample(row, tokenizer)\n",
    "            processed_batch.append(processed_row)\n",
    "        \n",
    "        # Convert to batch format\n",
    "        return {\n",
    "            'text': [item['text'] for item in processed_batch],\n",
    "            'query': [item['query'] for item in processed_batch],\n",
    "            'tools': [item['tools'] for item in processed_batch],\n",
    "            'answers': [item['answers'] for item in processed_batch]\n",
    "        }\n",
    "    \n",
    "    # Process the dataset\n",
    "    processed_dataset = dataset.map(\n",
    "        process_batch,\n",
    "        batched=True,\n",
    "        batch_size=1000,  # Process in batches for efficiency\n",
    "        num_proc=min(4, multiprocessing.cpu_count()),  # Use multiple cores\n",
    "        desc=\"Processing xLAM samples\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Dataset processing complete!\")\n",
    "    print(f\"üìä Final dataset size: {len(processed_dataset):,} samples\")\n",
    "    print(f\"üî§ Average text length: {sum(len(text) for text in processed_dataset['text']) / len(processed_dataset):,.0f} characters\")\n",
    "    \n",
    "    return processed_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_dataset_sample(dataset: Dataset, index: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Display a formatted preview of a dataset sample for inspection.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The processed dataset\n",
    "        index: Index of the sample to preview (default: 0)\n",
    "    \"\"\"\n",
    "    if index >= len(dataset):\n",
    "        print(f\"‚ùå Index {index} is out of range. Dataset has {len(dataset)} samples.\")\n",
    "        return\n",
    "    \n",
    "    sample = dataset[index]\n",
    "    \n",
    "    print(f\"üìã Dataset Sample Preview (Index: {index})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüîç Raw Components:\")\n",
    "    print(f\"Query: {sample['query'][:200]}{'...' if len(sample['query']) > 200 else ''}\")\n",
    "    print(f\"Tools: {sample['tools'][:200]}{'...' if len(sample['tools']) > 200 else ''}\")\n",
    "    print(f\"Answers: {sample['answers'][:200]}{'...' if len(sample['answers']) > 200 else ''}\")\n",
    "    \n",
    "    print(f\"\\nüìù Complete Training Text:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(sample['text'])\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\nüìä Sample Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Text length: {len(sample['text']):,} characters\")\n",
    "    print(f\"   ‚Ä¢ Estimated tokens: ~{len(sample['text']) // 4:,} tokens\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Preview complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Processing the Dataset\n",
    "\n",
    "Now let's add functions to load the xLAM dataset and process it into the format our model needs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import QLoRA training components when we need them\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "def create_qlora_model(model_config: ModelConfig, \n",
    "                       tokenizer: AutoTokenizer,\n",
    "                       compute_dtype: torch.dtype, \n",
    "                       attn_implementation: str) -> AutoModelForCausalLM:\n",
    "    \"\"\"\n",
    "    Create and configure a QLoRA-enabled model for efficient fine-tuning.\n",
    "    \n",
    "    QLoRA uses 4-bit quantization and low-rank adapters to enable\n",
    "    fine-tuning large models on consumer GPUs.\n",
    "    \"\"\"\n",
    "    print(f\"üèóÔ∏è  Creating QLoRA model: {model_config.model_name}\")\n",
    "    \n",
    "    # Configure 4-bit quantization for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "        bnb_4bit_quant_type=\"nf4\",           # Use NF4 quantization\n",
    "        bnb_4bit_compute_dtype=compute_dtype, # Computation data type\n",
    "        bnb_4bit_use_double_quant=True,      # Double quantization for more memory savings\n",
    "    )\n",
    "    \n",
    "    print(\"üì¶ Loading quantized model...\")\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_config.model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},                  # Load on first GPU\n",
    "        attn_implementation=attn_implementation,\n",
    "        torch_dtype=compute_dtype,\n",
    "        trust_remote_code=True,              # Required for some models\n",
    "    )\n",
    "    \n",
    "    # Prepare model for k-bit training (required for QLoRA)\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model, \n",
    "        gradient_checkpointing_kwargs={'use_reentrant': True}\n",
    "    )\n",
    "    \n",
    "    # Configure tokenizer settings in model\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False  # Disable cache for training\n",
    "    \n",
    "    print(\"‚úÖ QLoRA model prepared successfully!\")\n",
    "    print(f\"üíæ Model memory footprint: ~{model.get_memory_footprint() / 1e9:.1f} GB\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsctEAZ74nf9"
   },
   "source": [
    "## QLoRA Training Setup\n",
    "\n",
    "QLoRA (Quantized Low-Rank Adaptation) allows us to fine-tune large language models efficiently. It uses 4-bit quantization to reduce memory usage while maintaining training quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADy-kIwhhao_",
    "outputId": "c3192139-3cda-4978-8227-07e009e1ee52"
   },
   "outputs": [],
   "source": [
    "def create_lora_config(training_config: TrainingConfig) -> LoraConfig:\n",
    "    \"\"\"\n",
    "    Create LoRA configuration for parameter-efficient fine-tuning.\n",
    "    \n",
    "    LoRA (Low-Rank Adaptation) adds small trainable matrices to specific\n",
    "    model layers while keeping the base model frozen.\n",
    "    \n",
    "    Args:\n",
    "        training_config (TrainingConfig): Training configuration with LoRA parameters\n",
    "        \n",
    "    Returns:\n",
    "        LoraConfig: Configured LoRA adapter settings\n",
    "        \n",
    "    LoRA Parameters:\n",
    "        - r (rank): Dimensionality of adaptation matrices (higher = more capacity)\n",
    "        - alpha: Scaling factor for LoRA weights\n",
    "        - dropout: Regularization to prevent overfitting\n",
    "        - target_modules: Which model layers to adapt\n",
    "    \"\"\"\n",
    "    print(\"‚öôÔ∏è Configuring LoRA adapters...\")\n",
    "    \n",
    "    # Target modules for both Llama and Qwen architectures\n",
    "    target_modules = [\n",
    "        'k_proj', 'q_proj', 'v_proj', 'o_proj',  # Attention projections\n",
    "        \"gate_proj\", \"down_proj\", \"up_proj\"       # Feed-forward projections\n",
    "    ]\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        lora_alpha=training_config.lora_alpha,\n",
    "        lora_dropout=training_config.lora_dropout,\n",
    "        r=training_config.lora_r,\n",
    "        bias=\"none\",                             # Don't adapt bias terms\n",
    "        task_type=\"CAUSAL_LM\",                   # Causal language modeling\n",
    "        target_modules=target_modules\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ LoRA targeting modules: {target_modules}\")\n",
    "    print(f\"üìä LoRA parameters: r={training_config.lora_r}, alpha={training_config.lora_alpha}\")\n",
    "    \n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is the key technique that makes efficient fine-tuning possible. Instead of updating all model parameters, LoRA adds small trainable matrices to specific layers while keeping the base model frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "Now we'll create the main training function that puts everything together. This function configures the training arguments and executes the fine-tuning process using TRL's SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qlora_model(dataset: Dataset, \n",
    "                      model: AutoModelForCausalLM,\n",
    "                      training_config: TrainingConfig,\n",
    "                      compute_dtype: torch.dtype) -> SFTTrainer:\n",
    "    \"\"\"\n",
    "    Execute QLoRA fine-tuning with comprehensive configuration and monitoring.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Processed training dataset\n",
    "        model (AutoModelForCausalLM): QLoRA-configured model\n",
    "        training_config (TrainingConfig): Training hyperparameters\n",
    "        compute_dtype (torch.dtype): Computation data type\n",
    "        \n",
    "    Returns:\n",
    "        SFTTrainer: Trained model trainer\n",
    "        \n",
    "    Training Features:\n",
    "        - Supervised fine-tuning with SFTTrainer\n",
    "        - Memory-optimized settings for consumer GPUs\n",
    "        - Comprehensive logging and checkpointing\n",
    "        - Automatic mixed precision training\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting QLoRA fine-tuning...\")\n",
    "    \n",
    "    # Create LoRA configuration\n",
    "    peft_config = create_lora_config(training_config)\n",
    "    \n",
    "    # Configure training arguments\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir=training_config.output_dir,\n",
    "        optim=\"adamw_8bit\",                      # 8-bit optimizer for memory efficiency\n",
    "        per_device_train_batch_size=training_config.batch_size,\n",
    "        gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
    "        log_level=\"info\",                        # Detailed logging\n",
    "        save_steps=training_config.save_steps,\n",
    "        logging_steps=training_config.logging_steps,\n",
    "        learning_rate=training_config.learning_rate,\n",
    "        fp16=compute_dtype == torch.float16,     # Use FP16 if not using bfloat16\n",
    "        bf16=compute_dtype == torch.bfloat16,    # Use bfloat16 if supported\n",
    "        max_steps=training_config.max_steps,\n",
    "        warmup_ratio=training_config.warmup_ratio,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        dataset_text_field=\"text\",               # Field containing training text\n",
    "        max_length=training_config.max_seq_length,\n",
    "        remove_unused_columns=False,             # Keep all dataset columns\n",
    "        \n",
    "        # Additional stability and performance settings\n",
    "        dataloader_drop_last=True,               # Drop incomplete batches\n",
    "        gradient_checkpointing=True,             # Enable gradient checkpointing\n",
    "        save_total_limit=3,                      # Keep only 3 most recent checkpoints\n",
    "        load_best_model_at_end=False,            # Don't load best model (saves memory)\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Training configuration:\")\n",
    "    print(f\"   ‚Ä¢ Dataset size: {len(dataset):,} samples\")\n",
    "    print(f\"   ‚Ä¢ Batch size: {training_config.batch_size}\")\n",
    "    print(f\"   ‚Ä¢ Gradient accumulation: {training_config.gradient_accumulation_steps}\")\n",
    "    print(f\"   ‚Ä¢ Effective batch size: {training_config.batch_size * training_config.gradient_accumulation_steps}\")\n",
    "    print(f\"   ‚Ä¢ Max steps: {training_config.max_steps:,}\")\n",
    "    print(f\"   ‚Ä¢ Learning rate: {training_config.learning_rate}\")\n",
    "    print(f\"   ‚Ä¢ Output directory: {training_config.output_dir}\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\nüèÅ Beginning training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEy4x7sz4r_u"
   },
   "source": [
    "## üéØ Universal Model Selection\n",
    "\n",
    "**Choose any model for fine-tuning!** This notebook supports a wide range of popular models. Simply uncomment the model you want to use or specify your own.\n",
    "\n",
    "### üìã Quick Model Selection\n",
    "Uncomment one of these popular models or specify your own:\n",
    "\n",
    "**Why Llama 3-8B-Instruct as default?**\n",
    "- **Proven Performance**: Excellent function calling capabilities and instruction following\n",
    "- **Optimal Size**: 8B parameters provide great balance between performance and resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8717597832a34f93b181fbdf5075486c",
      "aba25f2c4ba5403b9cd21b30d3564202",
      "357f6226a70e4cd0be38f3cb909759a4",
      "47eb35c4dc51499caaa8a84959a3efe3",
      "1a5c832633064220bf40acbdcce68ed5",
      "03d425599509412ca4f98f113e01f01c",
      "73feb9118e064b8eaf41b7141665eb15",
      "ba5d81dd40384ebf89b4b9926e685956",
      "bd96a59726764a6ab2ce28b6578222a7",
      "26b9286254e14feebe7111bbb5a0e903",
      "fffd482b15e84d4a87f7d0070df1d118",
      "c2eb1a0d8c7045efae20254166004f48",
      "201fece083c24a959a054f8a2fb24436",
      "26b5dd4b38654990ab012192b940011f",
      "c7bd32531d8e4d3ea831d075809aac86",
      "4fe15da7bc2f4190a922f274ab14064c",
      "59868318c2fe46879265befe1c6710d4",
      "5f8287f2b6834db8866127f9e52642a6",
      "07747a094b37485188e60c61eca28f96",
      "78ed30a1dcaf42afbf8fd6351a21bf0c",
      "69f2e93ca24041e4bc2f1c16575eb360",
      "264150b16cb04593bb32a52ff09df963"
     ]
    },
    "id": "Ii4vvRjyfD2H",
    "outputId": "2f971c87-5956-4a0b-f331-64b1c8125868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Selected Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "\n",
      "üîß Auto-configuring everything for meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "üîç Loading model configuration: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "üìä Model: llama, vocab_size: 128,256\n",
      "‚úÖ Configured - pad: '<|eot_id|>' (ID: 128009), eos: '<|eot_id|>' (ID: 128009)\n",
      "\n",
      "üéâ Ready to fine-tune! Everything configured automatically:\n",
      "   ‚úÖ Model type: llama\n",
      "   ‚úÖ Vocabulary: 128,256 tokens\n",
      "   ‚úÖ Pad token: '<|eot_id|>' (ID: 128009)\n",
      "   ‚úÖ Output dir: ./Meta_Llama_3_8B_Instruct_xLAM\n",
      "\n",
      "üöÄ Configuration complete for meta-llama/Meta-Llama-3-8B-Instruct!\n"
     ]
    }
   ],
   "source": [
    "# üéØ ONE-LINE MODEL CONFIGURATION üéØ\n",
    "# Just specify any Hugging Face model and its custom pad token - everything else is automatic!\n",
    "\n",
    "# === Simply change this line to use ANY model ===\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "custom_pad_token = \"<|eot_id|>\"  \n",
    "# Use '<|eot_id|>' for Llama3+ models, '<|im_end|>' for Qwen2+ models, '</s>' for Mistral models, '<|end|>' for Phi3+ models\n",
    "\n",
    "# === Popular alternatives (uncomment to use) ===\n",
    "# MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"                # Qwen2 \n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"    # Mistral \n",
    "# MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"      # Phi-3 Mini \n",
    "# MODEL_NAME = \"google/gemma-1.1-7b-it\"                # Gemma \n",
    "# MODEL_NAME = \"your-custom-model/model-name\"          # Any custom model\n",
    "\n",
    "print(f\"üéØ Selected Model: {MODEL_NAME}\")\n",
    "\n",
    "# üöÄ AUTOMATIC CONFIGURATION - No manual setup needed!\n",
    "print(f\"\\nüîß Auto-configuring everything for {MODEL_NAME}...\")\n",
    "\n",
    "# Extract ALL information automatically using transformers\n",
    "model_config = auto_configure_model(MODEL_NAME, custom_pad_token=custom_pad_token) \n",
    "training_config = create_training_config(MODEL_NAME)\n",
    "\n",
    "print(f\"\\nüéâ Ready to fine-tune! Everything configured automatically:\")\n",
    "print(f\"   ‚úÖ Model type: {model_config.model_type}\")\n",
    "print(f\"   ‚úÖ Vocabulary: {model_config.vocab_size:,} tokens\")\n",
    "print(f\"   ‚úÖ Pad token: '{model_config.pad_token}' (ID: {model_config.pad_token_id})\")\n",
    "print(f\"   ‚úÖ Output dir: {training_config.output_dir}\")\n",
    "\n",
    "print(f\"\\nüöÄ Configuration complete for {MODEL_NAME}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal fine-tuning pipeline - works with any model!\n",
    "print(f\"üöÄ Starting fine-tuning pipeline for {model_config.model_name}\")\n",
    "\n",
    "# Step 1: Setup tokenizer\n",
    "print(f\"\\nüìù Setting up tokenizer...\")\n",
    "tokenizer = setup_tokenizer(model_config)\n",
    "\n",
    "# Step 2: Load and process dataset\n",
    "print(f\"\\nüìä Loading and processing xLAM dataset...\")\n",
    "dataset = load_and_process_xlam_dataset(tokenizer, sample_size=None)  # Set sample_size for testing\n",
    "\n",
    "# Step 3: Preview dataset sample\n",
    "print(f\"\\nüëÄ Dataset sample preview:\")\n",
    "preview_dataset_sample(dataset, index=0)\n",
    "\n",
    "# Step 4: Create QLoRA model\n",
    "print(f\"\\nüèóÔ∏è  Creating QLoRA model...\")\n",
    "model = create_qlora_model(\n",
    "    model_config, \n",
    "    tokenizer, \n",
    "    compute_dtype, \n",
    "    attn_implementation\n",
    ")\n",
    "\n",
    "# Step 5: Execute training\n",
    "print(f\"\\nüéØ Starting training...\")\n",
    "trainer = train_qlora_model(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    training_config=training_config,\n",
    "    compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Fine-tuning completed for {model_config.model_name.split('/')[-1]}!\")\n",
    "print(f\"üìÅ Model saved to: {training_config.output_dir}\")\n",
    "print(f\"üîç To test the model, run the inference cells below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading for Inference\n",
    "\n",
    "After training is complete, we need to load the trained model for inference. This function loads the base model with quantization and applies the trained LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required components for inference\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_trained_model(model_config: ModelConfig, \n",
    "                       adapter_path: str,\n",
    "                       compute_dtype: torch.dtype,\n",
    "                       attn_implementation: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load a trained model with LoRA adapters for inference.\n",
    "    \n",
    "    This function loads the base model with quantization and applies the trained\n",
    "    LoRA adapters for efficient inference. It's designed to work after training\n",
    "    completion or for loading previously saved models.\n",
    "    \n",
    "    Args:\n",
    "        model_config (ModelConfig): Configuration for the base model\n",
    "        adapter_path (str): Path to the saved LoRA adapter\n",
    "        compute_dtype (torch.dtype): Computation data type\n",
    "        attn_implementation (str): Attention implementation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[AutoModelForCausalLM, AutoTokenizer]: Loaded model and tokenizer\n",
    "        \n",
    "    Note:\n",
    "        You may need to restart the notebook to free GPU memory before loading\n",
    "        the model for inference, especially after training.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Loading trained model from {adapter_path}\")\n",
    "    \n",
    "    # Configure quantization for inference\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer with proper configuration\n",
    "    tokenizer = setup_tokenizer(model_config)\n",
    "    print(f\"üî§ Tokenizer loaded for {model_config.model_name}\")\n",
    "    \n",
    "    # Load base model\n",
    "    print(f\"üì¶ Loading base model {model_config.model_name}...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_config.model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=compute_dtype,\n",
    "        device_map={\"\": 0},\n",
    "        attn_implementation=attn_implementation,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters\n",
    "    print(f\"üîó Loading LoRA adapters from {adapter_path}...\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    # Enable evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully and ready for inference!\")\n",
    "    print(f\"üíæ Total memory usage: ~{model.get_memory_footprint() / 1e9:.1f} GB\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation for Function Calls\n",
    "\n",
    "Now let's create the function that generates responses from our fine-tuned model. This handles tokenization, generation parameters, and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_function_call(model: AutoModelForCausalLM,\n",
    "                          tokenizer: AutoTokenizer, \n",
    "                          prompt: str,\n",
    "                          max_new_tokens: int = 512,\n",
    "                          temperature: float = 0.7,\n",
    "                          do_sample: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate a function call response using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Fine-tuned model with LoRA adapters\n",
    "        tokenizer (AutoTokenizer): Model tokenizer\n",
    "        prompt (str): Input prompt for function calling\n",
    "        max_new_tokens (int): Maximum tokens to generate\n",
    "        temperature (float): Sampling temperature (only used when do_sample=True)\n",
    "        do_sample (bool): Whether to use sampling\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response with function calls\n",
    "        \n",
    "    Example Prompt Format:\n",
    "        \"<user>Check if the numbers 8 and 1233 are powers of two.</user>\\n\\n<tools>\"\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Generating response for prompt...\")\n",
    "    print(f\"üìù Input: {prompt}\")\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate response with proper parameter handling\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"do_sample\": do_sample,\n",
    "    }\n",
    "    \n",
    "    # Only add sampling parameters if do_sample=True\n",
    "    if do_sample:\n",
    "        generation_kwargs[\"temperature\"] = temperature\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "    \n",
    "    # Decode result\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"‚úÖ Generation completed!\")\n",
    "    print(f\"üìä Generated {len(outputs[0]) - len(inputs['input_ids'][0])} new tokens\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function Calling Capabilities\n",
    "\n",
    "This function provides a comprehensive test suite to evaluate our fine-tuned model with different types of function calling scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function_calling_examples(model: AutoModelForCausalLM, \n",
    "                                  tokenizer: AutoTokenizer) -> None:\n",
    "    \"\"\"\n",
    "    Test the model with various function calling examples.\n",
    "    \n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Fine-tuned model\n",
    "        tokenizer (AutoTokenizer): Model tokenizer\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing function calling capabilities...\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Mathematical Function\",\n",
    "            \"prompt\": \"<user>Check if the numbers 8 and 1233 are powers of two.</user>\\n\\n<tools>\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Weather Query\", \n",
    "            \"prompt\": \"<user>What's the weather like in New York today?</user>\\n\\n<tools>\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Data Processing\",\n",
    "            \"prompt\": \"<user>Calculate the average of these numbers: 10, 20, 30, 40, 50</user>\\n\\n<tools>\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Test Case {i}: {test_case['name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = generate_function_call(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=test_case[\"prompt\"],\n",
    "            max_new_tokens=512,  # Adjust as needed\n",
    "            temperature=0.7,\n",
    "            do_sample=True  # Fixed: Use sampling with temperature\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüîç Complete Response:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result)\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n‚úÖ All test cases completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDaGL_Etrek_",
    "outputId": "3a2552ae-753e-4ceb-aa89-e58f696c5c0c"
   },
   "outputs": [],
   "source": [
    "# Load and test the trained model\n",
    "# Note: You may need to restart the notebook to free memory before running this\n",
    "\n",
    "print(\"üîÑ Loading trained model for testing...\")\n",
    "print(\"‚ö†Ô∏è  If you encounter memory issues, restart the notebook and run only this cell\")\n",
    "\n",
    "# Determine the adapter path based on the training configuration\n",
    "adapter_path = f\"{training_config.output_dir}/checkpoint-{training_config.max_steps}\"\n",
    "\n",
    "print(f\"üìÅ Looking for adapter at: {adapter_path}\")\n",
    "\n",
    "# Load the trained model\n",
    "trained_model, trained_tokenizer = load_trained_model(\n",
    "    model_config=model_config,\n",
    "    adapter_path=adapter_path,\n",
    "    compute_dtype=compute_dtype,\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Test with a single example\n",
    "test_prompt = \"<user>Check if the numbers 8 and 1233 are powers of two.</user>\\n\\n<tools>\"\n",
    "result = generate_function_call(trained_model, trained_tokenizer, test_prompt)\n",
    "\n",
    "print(f\"\\nüéØ Test Result for {model_config.model_name.split('/')[-1]}:\")\n",
    "print(\"=\"*80)\n",
    "print(result)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XHNzMQWxk_Sq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing function calling capabilities...\n",
      "\n",
      "============================================================\n",
      "Test Case 1: Mathematical Function\n",
      "============================================================\n",
      "üéØ Generating response for prompt...\n",
      "üìù Input: <user>Check if the numbers 8 and 1233 are powers of two.</user>\n",
      "\n",
      "<tools>\n",
      "‚úÖ Generation completed!\n",
      "üìä Generated 90 new tokens\n",
      "\n",
      "üîç Complete Response:\n",
      "----------------------------------------\n",
      "<user>Check if the numbers 8 and 1233 are powers of two.</user>\n",
      "\n",
      "<tools>{'name': 'is_power_of_two', 'description': 'Checks if a number is a power of two.', 'parameters': {'num': {'description': 'The number to check.', 'type': 'int'}}}</tools>\n",
      "\n",
      "<calls>{'name': 'is_power_of_two', 'arguments': {'num': 8}}\n",
      "{'name': 'is_power_of_two', 'arguments': {'num': 1233}}</calls>\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "Test Case 2: Weather Query\n",
      "============================================================\n",
      "üéØ Generating response for prompt...\n",
      "üìù Input: <user>What's the weather like in New York today?</user>\n",
      "\n",
      "<tools>\n",
      "‚úÖ Generation completed!\n",
      "üìä Generated 105 new tokens\n",
      "\n",
      "üîç Complete Response:\n",
      "----------------------------------------\n",
      "<user>What's the weather like in New York today?</user>\n",
      "\n",
      "<tools>{'name':'realtime_weather_api', 'description': 'Fetches current weather information based on the provided query parameter.', 'parameters': {'q': {'description': 'Query parameter used to specify the location for which weather data is required. It can be in various formats such as:', 'type':'str', 'default': '53.1,-0.13'}}}</tools>\n",
      "\n",
      "<calls>{'name':'realtime_weather_api', 'arguments': {'q': 'New York'}}</calls>\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "Test Case 3: Data Processing\n",
      "============================================================\n",
      "üéØ Generating response for prompt...\n",
      "üìù Input: <user>Calculate the average of these numbers: 10, 20, 30, 40, 50</user>\n",
      "\n",
      "<tools>\n",
      "‚úÖ Generation completed!\n",
      "üìä Generated 81 new tokens\n",
      "\n",
      "üîç Complete Response:\n",
      "----------------------------------------\n",
      "<user>Calculate the average of these numbers: 10, 20, 30, 40, 50</user>\n",
      "\n",
      "<tools>{'name': 'average', 'description': 'Calculates the arithmetic mean of a list of numbers.', 'parameters': {'numbers': {'description': 'The list of numbers.', 'type': 'List[float]'}}}</tools>\n",
      "\n",
      "<calls>{'name': 'average', 'arguments': {'numbers': [10, 20, 30, 40, 50]}}</calls>\n",
      "----------------------------------------\n",
      "\n",
      "‚úÖ All test cases completed!\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive testing suite for your trained model\n",
    "test_function_calling_examples(trained_model, trained_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion and Next Steps\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Summary\n",
    "\n",
    "This notebook demonstrated a **complete, production-ready, universal pipeline** for fine-tuning language models for function calling capabilities using:\n",
    "\n",
    "- **üéØ Universal Model Support**: Works with any model - just change the `MODEL_NAME` variable\n",
    "- **üîß Intelligent Configuration**: Automatic token detection using `auto_configure_model()`\n",
    "- **‚ö° QLoRA Efficiency**: Memory-efficient training on consumer GPUs (16-24GB)\n",
    "- **üìã Comprehensive Testing**: Automated evaluation and interactive testing capabilities\n",
    "\n",
    "### üöÄ Key Improvements Made\n",
    "\n",
    "#### **Universal Compatibility**\n",
    "- ‚úÖ **Multi-Model Support**: Works with Llama, Qwen, Mistral, Gemma, Phi, DeepSeek, Yi, and more\n",
    "- ‚úÖ **Smart Token Detection**: Automatically finds pad/EOS tokens from any model's tokenizer\n",
    "- ‚úÖ **Error Prevention**: Validates configurations and provides helpful error messages\n",
    "- ‚úÖ **Flexible Architecture**: Easy to add new models without code changes\n",
    "\n",
    "#### **Code Quality**\n",
    "- ‚úÖ **Type Hints**: Full type annotations for better IDE support and error catching\n",
    "- ‚úÖ **Docstrings**: Comprehensive documentation for all functions\n",
    "- ‚úÖ **Error Handling**: Robust error handling with informative messages\n",
    "- ‚úÖ **Modular Design**: Clean separation of concerns and reusable components\n",
    "\n",
    "#### **User Experience**  \n",
    "- ‚úÖ **One-Line Model Selection**: Simply change `MODEL_NAME` variable\n",
    "- ‚úÖ **Automatic Configuration**: Everything extracted from transformers automatically\n",
    "- ‚úÖ **Clear Progress Indicators**: Emojis and detailed logging throughout\n",
    "- ‚úÖ **Production Ready**: Code suitable for research and deployment\n",
    "\n",
    "### üîÑ Next Steps and Extensions\n",
    "\n",
    "#### **Model Improvements**\n",
    "1. **Try Different Models**: Simply change the `MODEL_NAME` variable and re-run\n",
    "2. **Hyperparameter Tuning**: Experiment with different LoRA ranks, learning rates\n",
    "3. **Extended Training**: Try multi-epoch training for better convergence\n",
    "\n",
    "#### **Evaluation Enhancements**\n",
    "1. **Quantitative Metrics**: Add BLEU, ROUGE, or custom function calling accuracy\n",
    "2. **Benchmark Datasets**: Test on additional function calling benchmarks\n",
    "3. **Multi-Model Comparison**: Compare performance across different model families\n",
    "\n",
    "#### **Deployment Options**\n",
    "1. **Model Serving**: Deploy with FastAPI, TensorRT, or vLLM\n",
    "2. **Integration**: Connect with real APIs and function execution environments\n",
    "3. **Optimization**: Implement model quantization and pruning for production\n",
    "\n",
    "#### **Additional Features**\n",
    "1. **Multi-turn Conversations**: Extend to handle conversation context\n",
    "2. **Tool Selection**: Improve tool selection and reasoning capabilities\n",
    "3. **Error Recovery**: Add error handling and recovery mechanisms\n",
    "\n",
    "### üìö Resources and References\n",
    "\n",
    "- **xLAM Dataset**: [Salesforce/xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k)\n",
    "- **QLoRA Paper**: [Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "- **Function Calling Guide**: [Complete methodology article](https://newsletter.kaitchup.com/p/function-calling-fine-tuning-llama)\n",
    "- **PEFT Library**: [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "\n",
    "### üéñÔ∏è Achievement Unlocked\n",
    "\n",
    "**üèÜ Universal Function Calling Fine-tuning Master!**\n",
    "\n",
    "You now have a production-ready system that can fine-tune virtually any open-source language model for function calling with just a single line change!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Fine-tuning! üöÄ** Try different models, share your results, and contribute back to the community!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "behrooz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
