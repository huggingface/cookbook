{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9aca72-957a-4ee2-862f-e011b9cd3a62",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Goal\n",
    "I have a dataset I want to embed for semantic search (or QA, or RAG), I want the easiest way to do embed this and put it in a new dataset.\n",
    "\n",
    "## Approach\n",
    "Im using a dataset from my favorite subreddit [r/bestofredditorupdates](). Since it has such long entries, I will use the new [jinaai/jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en) since it has an 8k context length. Since Im GPU-poor I will deploy this using [Inference Endpoint](https://huggingface.co/inference-endpoints) to save money and time. To follow this you will need to **have already added a payment method**. If you haven't add one here in [billing](https://huggingface.co/docs/hub/billing#billing). To make it even easier, I'll make this fully API based.\n",
    "\n",
    "To make this MUCH faster I will use the [Text Embeddings Inference](https://github.com/huggingface/text-embeddings-inference) image. This has many benefits like:\n",
    "- No model graph compilation step\n",
    "- Small docker images and fast boot times. Get ready for true serverless!\n",
    "- Token based dynamic batching\n",
    "- Optimized transformers code for inference using Flash Attention, Candle and cuBLASLt\n",
    "- Safetensors weight loading\n",
    "- Production ready (distributed tracing with Open Telemetry, Prometheus metrics)\n",
    "\n",
    "![img](https://media.githubusercontent.com/media/huggingface/text-embeddings-inference/main/assets/bs1-tp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2534669-003d-490c-9d7a-32607fa5f404",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c830114-dd88-45a9-81b9-78b0e3da7384",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35386f72-32cb-49fa-a108-3aa504e20429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -r ../requirements.txt\n",
    "!pip install -q aiohttp==3.8.3 datasets==2.14.6 pandas==1.5.3 requests==2.31.0 tqdm==4.66.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a5c8c-9b28-4a59-85b4-2b3d590358a2",
   "metadata": {},
   "source": [
    "At the time of writing, I'm using features that arent on pypi yet. I'll use this code to conditionally install from the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc1275a-be3b-496d-a68a-1e6b26eb3c31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No versions available that are >= 0.20. Installing from github.\n",
      "Collecting git+https://github.com/huggingface/huggingface_hub\n",
      "  Cloning https://github.com/huggingface/huggingface_hub to /private/var/folders/0n/p7g75f3s3t71fryqp_v3fqhr0000gn/T/pip-req-build-683eflnh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/huggingface_hub /private/var/folders/0n/p7g75f3s3t71fryqp_v3fqhr0000gn/T/pip-req-build-683eflnh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/huggingface/huggingface_hub to commit ab6bff1b5e6c91ae7b5867d72e5a7757b39f2b15\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from huggingface-hub==0.20.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from huggingface-hub==0.20.0.dev0) (23.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from huggingface-hub==0.20.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from huggingface-hub==0.20.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from huggingface-hub==0.20.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from huggingface-hub==0.20.0.dev0) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from huggingface-hub==0.20.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests->huggingface-hub==0.20.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests->huggingface-hub==0.20.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests->huggingface-hub==0.20.0.dev0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests->huggingface-hub==0.20.0.dev0) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from packaging import version\n",
    "import re\n",
    "\n",
    "package_name = 'huggingface-hub'\n",
    "minimum_required_version = version.parse(\"0.20\")\n",
    "\n",
    "# Get the list of available versions from PyPI\n",
    "proc = subprocess.run(['pip', 'index', 'versions', package_name], capture_output=True, text=True)\n",
    "version_lines = proc.stdout.splitlines()\n",
    "\n",
    "# Extract version numbers and filter versions greater than or equal to 0.20\n",
    "version_pattern = re.compile(r'\\((.+)\\)')  # Adjust regex as needed\n",
    "available_versions = []\n",
    "for line in version_lines:\n",
    "    match = version_pattern.search(line)\n",
    "    if match and version.parse(match.group(1)) >= minimum_required_version:\n",
    "        available_versions.append(match.group(1))\n",
    "\n",
    "if available_versions:\n",
    "    latest_suitable_version = max(available_versions, key=lambda v: version.parse(v))\n",
    "    # Install the suitable version from PyPI\n",
    "    print(\"Versions available that are >= 0.20:\", latest_suitable_version)\n",
    "    subprocess.run(['pip', 'install', f'{package_name}=={latest_suitable_version}'])\n",
    "else:\n",
    "    # Install from the GitHub repository if the suitable version is not available\n",
    "    print(\"No versions available that are >= 0.20. Installing from github.\")\n",
    "    subprocess.run(['pip', 'install', 'git+https://github.com/huggingface/huggingface_hub'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f72042-173d-4a72-ade1-9304b43b528d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2beecdd-d033-4736-bd45-6754ec53b4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from getpass import getpass\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "from aiohttp import ClientSession, ClientTimeout\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from huggingface_hub import notebook_login, create_inference_endpoint, list_inference_endpoints, whoami\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eece903-64ce-435d-a2fd-096c0ff650bf",
   "metadata": {},
   "source": [
    "## Config\n",
    "You need to fill this in with your desired repos. Note I used 5 for the `MAX_WORKERS` since `jina-embeddings-v2` are quite memory hungry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd7daed-6aca-4fe7-85ce-534bdcd8bc87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_IN = 'derek-thomas/dataset-creator-reddit-bestofredditorupdates'\n",
    "DATASET_OUT = \"processed-subset-bestofredditorupdates\"\n",
    "ENDPOINT_NAME = \"boru-jina-embeddings-demo-ie\"\n",
    "\n",
    "# GPU Choice\n",
    "VENDOR=\"aws\"\n",
    "REGION=\"us-east-1\"\n",
    "INSTANCE_SIZE=\"medium\"\n",
    "INSTANCE_TYPE=\"g5.2xlarge\"\n",
    "\n",
    "MAX_WORKERS = 5  # This is for how many async workers you want. Choose based on the model and HW\n",
    "ROW_COUNT = 100  # Choose None to use all rows, Im using 100 just for a demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca1140c-3fcc-4b99-9210-6da1505a27b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee80821056e147fa9cabf30f64dc85a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ba0a8-0a6c-4705-a73b-7be09b889610",
   "metadata": {},
   "source": [
    "Some users might have payment registered in an organization. This allows you to connect to an organization (that you are a member of) with a payment method.\n",
    "\n",
    "Leave it blank is you want to use your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cdbd73-5923-4ae9-9940-b6be935f70fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your Hugging Face ðŸ¤— username or organization? (with an added payment method) Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "who = whoami()\n",
    "organization = getpass(prompt=\"What is your Hugging Face ðŸ¤— username or organization? (with an added payment method)\")\n",
    "\n",
    "namespace = organization or who['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972a719-2aed-4d2e-a24f-fae7776d5fa4",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27835fa4-3a4f-44b1-a02a-5e31584a1bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4041cedd3b3f4f8db3e29ec102f46a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'score', 'date_utc', 'title', 'flair', 'poster', 'permalink', 'new', 'updated'],\n",
       "    num_rows: 10042\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_IN)\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8846087e-4d0d-4c0e-8aeb-ea95d9e97126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " {'id': '10004zw',\n",
       "  'content': '[removed]',\n",
       "  'score': 1,\n",
       "  'date_utc': Timestamp('2022-12-31 18:16:22'),\n",
       "  'title': 'To All BORU contributors, Thank you :)',\n",
       "  'flair': 'CONCLUDED',\n",
       "  'poster': 'IsItAcOnSeQuEnCe',\n",
       "  'permalink': '/r/BestofRedditorUpdates/comments/10004zw/to_all_boru_contributors_thank_you/',\n",
       "  'new': False,\n",
       "  'updated': False})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = dataset['train'].to_pandas().to_dict('records')[:ROW_COUNT]\n",
    "len(documents), documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93096cbc-81c6-4137-a283-6afb0f48fbb9",
   "metadata": {},
   "source": [
    "# Inference Endpoints\n",
    "## Create Inference Endpoint\n",
    "We are going to use the [API](https://huggingface.co/docs/inference-endpoints/api_reference) to create an [Inference Endpoint](https://huggingface.co/inference-endpoints). This should provide a few main benefits:\n",
    "- It's convenient (No clicking)\n",
    "- It's repeatable (We have the code to run it easily)\n",
    "- It's cheaper (No time spent waiting for it to load, and automatically shut it down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e59de46-26b7-4bb9-bbad-8bba9931bde7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    endpoint = create_inference_endpoint(\n",
    "        ENDPOINT_NAME,\n",
    "        repository=\"jinaai/jina-embeddings-v2-base-en\",\n",
    "        framework=\"pytorch\",\n",
    "        accelerator=\"gpu\",\n",
    "        instance_size=INSTANCE_SIZE,\n",
    "        instance_type=INSTANCE_TYPE,\n",
    "        region=REGION,\n",
    "        vendor=VENDOR,\n",
    "        revision=\"7302ac470bed880590f9344bfeee32ff8722d0e5\",\n",
    "        task=\"sentence-embeddings\",\n",
    "        namespace=namespace,\n",
    "        custom_image={\n",
    "            \"health_route\": \"/health\",\n",
    "            \"env\": {\n",
    "                \"MAX_BATCH_TOKENS\": str(MAX_WORKERS * 2048),\n",
    "                \"MAX_CONCURRENT_REQUESTS\": \"512\",\n",
    "                \"MODEL_ID\": \"/repository\"\n",
    "            },\n",
    "            \"url\": \"ghcr.io/huggingface/text-embeddings-inference:0.5.0\",\n",
    "        },\n",
    "        type=\"protected\",\n",
    "    )\n",
    "except:\n",
    "    endpoint = [ie for ie in list_inference_endpoints(namespace=namespace) if ie.name == ENDPOINT_NAME][0]\n",
    "    print('Loaded endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c97dc-34e8-49e9-b60e-f5b7366294c0",
   "metadata": {},
   "source": [
    "There are a few design choices here:\n",
    "- I'm using the `g5.2xlarge` since it is big and `jina-embeddings-v2` are memory hungry (remember the 8k context length). \n",
    "- I didnt alter the default `MAX_BATCH_TOKENS` or `MAX_CONCURRENT_REQUESTS`\n",
    "    - You should consider this if you are making this production ready\n",
    "    - You will need to restrict these to match the HW you are running on\n",
    "- As mentioned before, I chose the repo and the corresponding revision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d173b2-8980-4554-9039-c62843d3fc7d",
   "metadata": {},
   "source": [
    "## Wait until its running\n",
    "Here we use `tqdm` as a pretty way of displaying our status. It took about ~30s for this model to get the Inference Endpoint running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3a8bd2-753c-49a8-9452-899578beddc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.1 ms, sys: 15.7 ms, total: 63.8 ms\n",
      "Wall time: 52.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InferenceEndpoint(name='boru-jina-embeddings-demo-ie', namespace='HF-test-lab', repository='jinaai/jina-embeddings-v2-base-en', status='running', url='https://k7l1xeok1jwnpbx5.us-east-1.aws.endpoints.huggingface.cloud')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "endpoint.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8aa66a9-3c8a-4040-9465-382c744f36cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb6e3cbe65a439cb8a91b4aaf7f1c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Waiting for status to change: 0s [00:00, ?s/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is 'running'.\n"
     ]
    }
   ],
   "source": [
    "with tqdm(desc=\"Waiting for status to change\", unit=\"s\") as pbar:\n",
    "    while True:\n",
    "        endpoint.fetch()\n",
    "        current_status = endpoint.status\n",
    "\n",
    "        if current_status == 'running':\n",
    "            print(\"Status is 'running'.\")\n",
    "            break\n",
    "\n",
    "        pbar.set_description(f\"Status: {current_status}\")\n",
    "        time.sleep(2)\n",
    "        pbar.update(1)\n",
    "\n",
    "embedding_url = endpoint.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fa066-e4d0-4a65-a82d-cf17db4af8d8",
   "metadata": {},
   "source": [
    "I found that even though the status is running, I want to get a test message to run first before running our batch in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906645e-60de-4eb6-b8b6-3ec98a9d9b00",
   "metadata": {},
   "source": [
    "When we use `endpoint.client.post` we get a bytes string back. This is a little tedious because we need to convert this to an `np.array`, but its just a couple quick lines in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09253d5-70ff-4d0e-8888-0022ce0adf7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05630935, -0.03560849,  0.02789049,  0.02792823, -0.02800371,\n",
       "       -0.01530391, -0.01863454, -0.0077982 ,  0.05374297,  0.03672185,\n",
       "       -0.06114018, -0.06880157, -0.0093503 , -0.03174005, -0.03206085,\n",
       "        0.0610647 ,  0.02243694,  0.03217408,  0.04181686,  0.00248854])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = endpoint.client.post(json={\"inputs\": 'This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music!', 'truncate': True}, task=\"feature-extraction\")\n",
    "response = np.array(json.loads(response.decode()))\n",
    "response[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d024788-6e6e-4a8d-b192-36ee3dacca13",
   "metadata": {},
   "source": [
    "Its possible to have inputs that exceed the context. In those scenarios its up to you have to handle them. In my case Id like to truncate rather than have an error. Let's test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a1cd15-dda3-4cfa-8bda-788d8c1b9e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the embedding_input is: 300000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03088215, -0.0351537 ,  0.05749275,  0.00983467,  0.02108356,\n",
       "        0.04539965,  0.06107162, -0.02536954,  0.03887688,  0.01998681,\n",
       "       -0.05391388,  0.01529677, -0.1279156 ,  0.01653782, -0.01940958,\n",
       "        0.0367411 ,  0.0031748 ,  0.04716022, -0.00713609, -0.00155313])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_input = 'This input will get multiplied' * 10000\n",
    "print(f'The length of the embedding_input is: {len(embedding_input)}')\n",
    "response = endpoint.client.post(json={\"inputs\": embedding_input, 'truncate': True}, task=\"feature-extraction\")\n",
    "response = np.array(json.loads(response.decode()))\n",
    "response[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7186126-ef6a-47d0-b158-112810649cd9",
   "metadata": {},
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadfd68-6d46-4ce8-a165-bfeb43b1f114",
   "metadata": {},
   "source": [
    "Here I send a document, update it with the embedding, and return it. This happens in parallel with `MAX_WORKERS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3193fb-3def-42a8-968e-c63f2b864ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def request(document, semaphore):\n",
    "    # Semaphore guard\n",
    "    async with semaphore:\n",
    "        result = await endpoint.async_client.post(json={\"inputs\": document['content'], 'truncate': True}, task=\"feature-extraction\")\n",
    "        result = np.array(json.loads(result.decode()))\n",
    "        document['embedding'] = result[0]  # Assuming the API's output can be directly assigned\n",
    "        return document\n",
    "\n",
    "async def main(documents):\n",
    "    # Semaphore to limit concurrent requests. Adjust the number as needed.\n",
    "    semaphore = asyncio.BoundedSemaphore(MAX_WORKERS)\n",
    "\n",
    "    # Creating a list of tasks\n",
    "    tasks = [request(document, semaphore) for document in documents]\n",
    "    \n",
    "    # Using tqdm to show progress. It's been integrated into the async loop.\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(documents)):\n",
    "        await f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec4983af-65eb-4841-808a-3738fb4d682d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a2affdee8d46f3b0c1f691eaac4b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings = 100 documents = 100\n",
      "0 min 21.33 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "# Get embeddings\n",
    "await main(documents)\n",
    "\n",
    "# Make sure we got it all\n",
    "count = 0\n",
    "for document in documents:\n",
    "    if 'embedding' in document.keys() and len(document['embedding']) == 768:\n",
    "        count += 1\n",
    "print(f'Embeddings = {count} documents = {len(documents)}')\n",
    "\n",
    "            \n",
    "# Print elapsed time\n",
    "elapsed_time = time.perf_counter() - start\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(f\"{int(minutes)} min {seconds:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab97c7b-7bac-4bf5-9752-b528294dadc7",
   "metadata": {},
   "source": [
    "## Pause Inference Endpoint\n",
    "Now that we have finished, lets pause the endpoint so we don't incur any extra charges, this will also allow us to analyze the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "540a0978-7670-4ce3-95c1-3823cc113b85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: paused\n"
     ]
    }
   ],
   "source": [
    "endpoint = endpoint.pause()\n",
    "\n",
    "print(f\"Endpoint Status: {endpoint.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad65b7-3da2-4113-9b95-8fb4e21ae793",
   "metadata": {},
   "source": [
    "# Push updated dataset to Hub\n",
    "We now have our documents updated with the embeddings we wanted. First we need to convert it back to a `Dataset` format. I find its easiest to go from list of dicts -> `pd.DataFrame` -> `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb993f8-d624-4192-9626-8e9ed9888a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(documents)\n",
    "dd = DatasetDict({'train': Dataset.from_pandas(df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129760c8-cae1-4b1e-8216-f5152df8c536",
   "metadata": {},
   "source": [
    "I'm uploading it to the user's account by default but feel free to push to wherever you want by setting the user in the `repo_id` or in the config by setting `DATASET_OUT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f48e7c55-d5b7-4ed6-8516-272ae38716b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3af2e864770481db5adc3968500b5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e063c42d8f4490c939bc64e626b507a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/823 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd.push_to_hub(repo_id=DATASET_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85ea2244-a4c6-4f04-b187-965a2fc356a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is at https://huggingface.co/datasets/derek-thomas/processed-subset-bestofredditorupdates\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset is at https://huggingface.co/datasets/{who[\"name\"]}/{DATASET_OUT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41abea64-379d-49de-8d9a-355c2f4ce1ac",
   "metadata": {},
   "source": [
    "# Analyze Usage\n",
    "1. Go to your `dashboard_url` printed below\n",
    "1. Click on the Usage & Cost tab\n",
    "1. See how much you have spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16815445-3079-43da-b14e-b54176a07a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.endpoints.huggingface.co/HF-test-lab/endpoints/boru-jina-embeddings-demo-ie\n"
     ]
    }
   ],
   "source": [
    "dashboard_url = f'https://ui.endpoints.huggingface.co/{namespace}/endpoints/{ENDPOINT_NAME}'\n",
    "print(dashboard_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81096c6f-d12f-4781-84ec-9066cfa465b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hit enter to continue with the notebook \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input(\"Hit enter to continue with the notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d524e-9aa6-4a6f-a275-8a552e289818",
   "metadata": {},
   "source": [
    "We can see that it only took `$0.04` to pay for this!\n",
    "\n",
    "![Cost](../media/automatic_embedding_tei_inference_endpoints.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953d5be-2494-4ff8-be42-9daf00c99c41",
   "metadata": {},
   "source": [
    "# Delete Endpoint\n",
    "We should see a `200` if everything went correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c310c0f3-6f12-4d5c-838b-3a4c1f2e54ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint deleted successfully\n"
     ]
    }
   ],
   "source": [
    "endpoint = endpoint.delete()\n",
    "\n",
    "if not endpoint:\n",
    "    print('Endpoint deleted successfully')\n",
    "else:\n",
    "    print('Delete Endpoint in manually') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1b1c3-16c3-403a-9472-a97e730826d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
