{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation\n",
    "\n",
    "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    "For an introduction to RAG, you can check [this other cookbook](https://github.com/huggingface/cookbook/blob/main/notebooks/RAG_zephyr_langchain.ipynb)!\n",
    "\n",
    "RAG systems are complex, with many moving parts: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
    "\n",
    "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
    "So let's see how to evaluate our RAG system.\n",
    "\n",
    "### Evaluating RAG performance\n",
    "\n",
    "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
    "\n",
    "For our evaluation pipeline, we will need:\n",
    "- an evaluation dataset\n",
    "- an evaluator to compute the accuracy of our system.\n",
    "\n",
    "‚û°Ô∏è It turns out, we can use LLMs to help us all along the way!\n",
    "- the evaluation dataset will be synthetically generated by an LLM ü§ñ, and questions will be filtered out by other LLMs ü§ñ\n",
    "- the evaluation will then be run on this synthetic dataset by a LLM-as-a-judge agent ü§ñ.\n",
    "\n",
    "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers faiss-gpu openpyxl openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from datasets import Dataset\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82a5526012a45268d6ce4a165c7f7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e274529713444ee2bc13c7dcd2f61e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e2cf1d678640f2a8a38b7e1c5bba3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build a synthetic dataset for evaluation\n",
    "We first build a synthetic dataset of questions and associated contexts.\n",
    "\n",
    "The idea is to randomly get elements from our knowledge base, and ask a LLM to generate questions based on these documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0fb35ee5dd4122bc582ed1954ef302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup agents for question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/ml2/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "QA_generation_prompt = ChatPromptTemplate.from_template(QA_generation_prompt)\n",
    "QA_generation_agent = QA_generation_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 QA couples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3c0cc94bc14c2aba9dcd077f882e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 10\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "outputs = []\n",
    "for context in tqdm(random.sample(langchain_docs, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = QA_generation_agent.invoke({\"context\": context.page_content}).content\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[1]\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@gradio/button\\n\\n## 0.2.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\\n  - @gradio/client@0.9.3\\n  - @gradio/upload@0.5.6\\n\\n## 0.2.12\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\\n  - @gradio/client@0.9.2\\n  - @gradio/upload@0.5.5\\n\\n## 0.2.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\\n  - @gradio/upload@0.5.4\\n  - @gradio/client@0.9.1\\n\\n## 0.2.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098da7d88a539aaaf5ffe88), [`d76bcaa`](https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70), [`67ddd40`](https://github.com/gradio-app/gradio/commit/67ddd40b4b70d3a37cb1637c33620f8d197dbee0)]:\\n  - @gradio/upload@0.5.3\\n  - @gradio/client@0.9.0\\n\\n## 0.2.9\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/upload@0.5.2\\n\\n## 0.2.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\\n  - @gradio/client@0.8.2\\n  - @gradio/upload@0.5.1\\n\\n## 0.2.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\\n  - @gradio/upload@0.5.0\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gradio/commit/2f805a7dd3d2b64b098f659dadd5d01258290521)]:\\n  - @gradio/upload@0.4.2\\n\\n## 0.2.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gradio/commit/324867f63c920113d89a565892aa596cf8b1e486)]:\\n  - @gradio/client@0.8.1\\n  - @gradio/upload@0.4.1\\n\\n## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gradio/commit/854b482f598e0dc47673846631643c079576da9c), [`f1409f95e`](https://github.com/gradio-app/gradio/commit/f1409f95ed39c5565bed6a601e41f94e30196a57)]:\\n  - @gradio/upload@0.4.0\\n  - @gradio/client@0.8.0\\n\\n## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gradio/commit/bca6c2c80f7e5062427019de45c282238388af95), [`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780)]:\\n  - @gradio/client@0.7.2\\n  - @gradio/upload@0.3.3\\n\\n## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`aaa55ce85`](https://github.com/gradio-app/gradio/commit/aaa55ce85e12f95aba9299445e9c5e59824da18e)]:\\n  - @gradio/upload@0.3.2\\n\\n## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gradio/commit/2ba14b284f908aa13859f4337167a157075a68eb)]:\\n  - @gradio/client@0.7.1\\n  - @gradio/upload@0.3.1\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Clean root url. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Custom components. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40`](https://github.com/gradio-app/gradio/commit/e4f7b4b409323b01aa01b39e15ce6139e29aa073) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-app/gradio/commit/667802a6cdbfb2ce454a3be5a78e0990b194548a) - JS Component Documentation. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd`](https://github.com/gradio-app/gradio/commit/90318b1dd118ae08a695a50e7c556226234ab6dc) - swap `mode` on the frontned to `interactive` to match the backend. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 0.2.0-beta.6\\n\\n### Fixes\\n\\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 0.2.0-beta.5\\n\\n### Features\\n\\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n\\n## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/upload@0.3.3\\n\\n## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/utils@0.1.2\\n  - @gradio/upload@0.3.2\\n\\n## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/upload@0.3.1\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5554](https://github.com/gradio-app/gradio/pull/5554) [`75ddeb390`](https://github.com/gradio-app/gradio/commit/75ddeb390d665d4484667390a97442081b49a423) - Accessibility Improvements. Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/utils@0.1.1\\n  - @gradio/upload@0.2.1\\n\\n## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db), [`79d8f9d8`](https://github.com/gradio-app/gradio/commit/79d8f9d891901683c5a1b7486efb44eab2478c96)]:\\n  - @gradio/utils@0.1.0\\n  - @gradio/upload@0.2.0\\n\\n## 0.1.1\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279) [`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))\\n\\n##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling.\\n\\n##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large applications.\\n\\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\\n- Corrected an issue that was causing markdown to re-render infinitely.\\n- Ensured that the `gr.3DModel` does re-render prematurely.\\n\\nThanks [@pngwn](https://github.com/pngwn)!\\n\\n### Fixes\\n\\n- [#5285](https://github.com/gradio-app/gradio/pull/5285) [`cdfd4217`](https://github.com/gradio-app/gradio/commit/cdfd42174a9c777eaee9c1209bf8e90d8c7791f2) - Tweaks to `icon` parameter in `gr.Button()`. Thanks [@abidlabs](https://github.com/abidlabs)!\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5080](https://github.com/gradio-app/gradio/pull/5080) [`37caa2e0`](https://github.com/gradio-app/gradio/commit/37caa2e0fe95d6cab8beb174580fb557904f137f) - Add icon and link params to `gr.Button`. Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/utils@0.0.2\\n</td>\n",
       "      <td>What was the version number of @gradio/button when improved markdown support and startup performance enhancements were introduced?\\n</td>\n",
       "      <td>0.1.1</td>\n",
       "      <td>gradio-app/gradio/blob/main/js/button/CHANGELOG.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n--&gt;\\n\\n# BARTpho\\n\\n## Overview\\n\\nThe BARTpho model was proposed in [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\\n\\nThe abstract from the paper is the following:\\n\\n*We present BARTpho with two versions -- BARTpho_word and BARTpho_syllable -- the first public large-scale monolingual\\nsequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the \"large\" architecture and pre-training\\nscheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments\\non a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho\\noutperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future\\nresearch and applications of generative Vietnamese NLP tasks.*\\n\\nThis model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The original code can be found [here](https://github.com/VinAIResearch/BARTpho).\\n\\n## Usage example\\n\\n```python\\n&gt;&gt;&gt; import torch\\n&gt;&gt;&gt; from transformers import AutoModel, AutoTokenizer\\n\\n&gt;&gt;&gt; bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\\n\\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\\n\\n&gt;&gt;&gt; line = \"Ch√∫ng t√¥i l√† nh·ªØng nghi√™n c·ª©u vi√™n.\"\\n\\n&gt;&gt;&gt; input_ids = tokenizer(line, return_tensors=\"pt\")\\n\\n&gt;&gt;&gt; with torch.no_grad():\\n...     features = bartpho(**input_ids)  # Models outputs are now tuples\\n\\n&gt;&gt;&gt; # With TensorFlow 2.0+:\\n&gt;&gt;&gt; from transformers import TFAutoModel\\n\\n&gt;&gt;&gt; bartpho = TFAutoModel.from_pretrained(\"vinai/bartpho-syllable\")\\n&gt;&gt;&gt; input_ids = tokenizer(line, return_tensors=\"tf\")\\n&gt;&gt;&gt; features = bartpho(**input_ids)\\n```\\n\\n## Usage tips\\n\\n- Following mBART, BARTpho uses the \"large\" architecture of BART with an additional layer-normalization layer on top of\\n  both the encoder and decoder. Thus, usage examples in the [documentation of BART](bart), when adapting to use\\n  with BARTpho, should be adjusted by replacing the BART-specialized classes with the mBART-specialized counterparts.\\n  For example:\\n\\n```python\\n&gt;&gt;&gt; from transformers import MBartForConditionalGeneration\\n\\n&gt;&gt;&gt; bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\\n&gt;&gt;&gt; TXT = \"Ch√∫ng t√¥i l√† &lt;mask&gt; nghi√™n c·ª©u vi√™n.\"\\n&gt;&gt;&gt; input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\\n&gt;&gt;&gt; logits = bartpho(input_ids).logits\\n&gt;&gt;&gt; masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\\n&gt;&gt;&gt; probs = logits[0, masked_index].softmax(dim=0)\\n&gt;&gt;&gt; values, predictions = probs.topk(5)\\n&gt;&gt;&gt; print(tokenizer.decode(predictions).split())\\n```\\n\\n- This implementation is only for tokenization: \"monolingual_vocab_file\" consists of Vietnamese-specialized types\\n  extracted from the pre-trained SentencePiece model \"vocab_file\" that is available from the multilingual XLM-RoBERTa.\\n  Other languages, if employing this pre-trained multilingual SentencePiece model \"vocab_file\" for subword\\n  segmentation, can reuse BartphoTokenizer with their own language-specialized \"monolingual_vocab_file\".\\n\\n## BartphoTokenizer\\n\\n[[autodoc]] BartphoTokenizer\\n</td>\n",
       "      <td>Who proposed the BARTpho model?\\n</td>\n",
       "      <td>Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/bartpho.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--\\n# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1\\n# Doc / guide: https://huggingface.co/docs/hub/datasets-cards\\n{{ card_data }}\\n---\\n\\n# Dataset Card for {{ pretty_name | default(\"Dataset Name\", true) }}\\n\\n&lt;!-- Provide a quick summary of the dataset. --&gt;\\n\\n{{ dataset_summary | default(\"\", true) }}\\n\\n## Dataset Details\\n\\n### Dataset Description\\n\\n&lt;!-- Provide a longer summary of what this dataset is. --&gt;\\n\\n{{ dataset_description | default(\"\", true) }}\\n\\n- **Curated by:** {{ curators | default(\"[More Information Needed]\", true)}}\\n- **Funded by [optional]:** {{ funded_by | default(\"[More Information Needed]\", true)}}\\n- **Shared by [optional]:** {{ shared_by | default(\"[More Information Needed]\", true)}}\\n- **Language(s) (NLP):** {{ language | default(\"[More Information Needed]\", true)}}\\n- **License:** {{ license | default(\"[More Information Needed]\", true)}}\\n\\n### Dataset Sources [optional]\\n\\n&lt;!-- Provide the basic links for the dataset. --&gt;\\n\\n- **Repository:** {{ repo | default(\"[More Information Needed]\", true)}}\\n- **Paper [optional]:** {{ paper | default(\"[More Information Needed]\", true)}}\\n- **Demo [optional]:** {{ demo | default(\"[More Information Needed]\", true)}}\\n\\n## Uses\\n\\n&lt;!-- Address questions around how the dataset is intended to be used. --&gt;\\n\\n### Direct Use\\n\\n&lt;!-- This section describes suitable use cases for the dataset. --&gt;\\n\\n{{ direct_use | default(\"[More Information Needed]\", true)}}\\n\\n### Out-of-Scope Use\\n\\n&lt;!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. --&gt;\\n\\n{{ out_of_scope_use | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Structure\\n\\n&lt;!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. --&gt;\\n\\n{{ dataset_structure | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Creation\\n\\n### Curation Rationale\\n\\n&lt;!-- Motivation for the creation of this dataset. --&gt;\\n\\n{{ curation_rationale_section | default(\"[More Information Needed]\", true)}}\\n\\n### Source Data\\n\\n&lt;!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). --&gt;\\n\\n#### Data Collection and Processing\\n\\n&lt;!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. --&gt;\\n\\n{{ data_collection_and_processing_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Who are the source data producers?\\n\\n&lt;!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. --&gt;\\n\\n{{ source_data_producers_section | default(\"[More Information Needed]\", true)}}\\n\\n### Annotations [optional]\\n\\n&lt;!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. --&gt;\\n\\n#### Annotation process\\n\\n&lt;!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. --&gt;\\n\\n{{ annotation_process_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Who are the annotators?\\n\\n&lt;!-- This section describes the people or systems who created the annotations. --&gt;\\n\\n{{ who_are_annotators_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Personal and Sensitive Information\\n\\n&lt;!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. --&gt;\\n\\n{{ personal_and_sensitive_information | default(\"[More Information Needed]\", true)}}\\n\\n## Bias, Risks, and Limitations\\n\\n&lt;!-- This section is meant to convey both technical and sociotechnical limitations. --&gt;\\n\\n{{ bias_risks_limitations | default(\"[More Information Needed]\", true)}}\\n\\n### Recommendations\\n\\n&lt;!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. --&gt;\\n\\n{{ bias_recommendations | default(\"Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.\", true)}}\\n\\n## Citation [optional]\\n\\n&lt;!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. --&gt;\\n\\n**BibTeX:**\\n\\n{{ citation_bibtex | default(\"[More Information Needed]\", true)}}\\n\\n**APA:**\\n\\n{{ citation_apa | default(\"[More Information Needed]\", true)}}\\n\\n## Glossary [optional]\\n\\n&lt;!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. --&gt;\\n\\n{{ glossary | default(\"[More Information Needed]\", true)}}\\n\\n## More Information [optional]\\n\\n{{ more_information | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Card Authors [optional]\\n\\n{{ dataset_card_authors | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Card Contact\\n\\n{{ dataset_card_contact | default(\"[More Information Needed]\", true)}}</td>\n",
       "      <td>What section of the dataset card provides a description of how the dataset is intended to be used?\\n</td>\n",
       "      <td>Direct Use</td>\n",
       "      <td>huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n--&gt;\\n\\n# GPTSAN-japanese\\n\\n## Overview\\n\\nThe GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto (tanreinama).\\n\\nGPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM\\nin the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can\\nfine-tune for translation or summarization.\\n\\n### Usage example\\n\\nThe `generate()` method can be used to generate text using GPTSAN-Japanese model.\\n\\n```python\\n&gt;&gt;&gt; from transformers import AutoModel, AutoTokenizer\\n&gt;&gt;&gt; import torch\\n\\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\\n&gt;&gt;&gt; model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").cuda()\\n&gt;&gt;&gt; x_tok = tokenizer(\"„ÅØ„ÄÅ\", prefix_text=\"ÁπîÁî∞‰ø°Èï∑\", return_tensors=\"pt\")\\n&gt;&gt;&gt; torch.manual_seed(0)\\n&gt;&gt;&gt; gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)\\n&gt;&gt;&gt; tokenizer.decode(gen_tok[0])\\n'ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅ2004Âπ¥„Å´„ÄéÊà¶ÂõΩBASARA„Äè„ÅÆ„Åü„ÇÅ„Å´„ÄÅË±äËá£ÁßÄÂêâ'\\n```\\n\\n## GPTSAN Features\\n\\nGPTSAN has some unique features. It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models.\\nThe Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text.\\nGPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAN repository for details.\\n\\n### Prefix-LM Model\\n\\nGPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The original GPTSAN repository calls it `hybrid`)\\nIn GPTSAN, the `Prefix` part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length.\\nArbitrary lengths can also be specified differently for each batch.\\nThis length applies to the text entered in `prefix_text` for the tokenizer.\\nThe tokenizer returns the mask of the `Prefix` part of Prefix-LM as `token_type_ids`.\\nThe model treats the part where `token_type_ids` is 1 as a `Prefix` part, that is, the input can refer to both tokens before and after.\\n\\n## Usage tips\\n\\nSpecifying the Prefix part is done with a mask passed to self-attention.\\nWhen token_type_ids=None or all zero, it is equivalent to regular causal mask\\n\\nfor example:\\n\\n&gt;&gt;&gt; x_token = tokenizer(\"ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥\")\\ninput_ids:      | SOT | SEG | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ |\\ntoken_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |\\nprefix_lm_mask:\\nSOT | 1 0 0 0 0 0 |\\nSEG | 1 1 0 0 0 0 |\\nÔΩ±   | 1 1 1 0 0 0 |\\nÔΩ≤   | 1 1 1 1 0 0 |\\nÔΩ≥   | 1 1 1 1 1 0 |\\nÔΩ¥   | 1 1 1 1 1 1 |\\n\\n&gt;&gt;&gt; x_token = tokenizer(\"\", prefix_text=\"ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥\")\\ninput_ids:      | SOT | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ | SEG |\\ntoken_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |\\nprefix_lm_mask:\\nSOT | 1 1 1 1 1 0 |\\nÔΩ±   | 1 1 1 1 1 0 |\\nÔΩ≤   | 1 1 1 1 1 0 |\\nÔΩ≥   | 1 1 1 1 1 0 |\\nÔΩ¥   | 1 1 1 1 1 0 |\\nSEG | 1 1 1 1 1 1 |\\n\\n&gt;&gt;&gt; x_token = tokenizer(\"ÔΩ≥ÔΩ¥\", prefix_text=\"ÔΩ±ÔΩ≤\")\\ninput_ids:      | SOT | ÔΩ± | ÔΩ≤ | SEG | ÔΩ≥ | ÔΩ¥ |\\ntoken_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |\\nprefix_lm_mask:\\nSOT | 1 1 1 0 0 0 |\\nÔΩ±   | 1 1 1 0 0 0 |\\nÔΩ≤   | 1 1 1 0 0 0 |\\nSEG | 1 1 1 1 0 0 |\\nÔΩ≥   | 1 1 1 1 1 0 |\\nÔΩ¥   | 1 1 1 1 1 1 |\\n\\n### Spout Vector\\n\\nA Spout Vector is a special vector for controlling text generation.\\nThis vector is treated as the first embedding in self-attention to bring extraneous attention to the generated tokens.\\nIn the pre-trained model published from `Tanrei/GPTSAN-japanese`, the Spout Vector is a 128-dimensional vector that passes through 8 fully connected layers in the model and is projected into the space acting as external attention.\\nThe Spout Vector projected by the fully connected layer is split to be passed to all self-attentions.\\n\\n## GPTSanJapaneseConfig\\n\\n[[autodoc]] GPTSanJapaneseConfig\\n\\n## GPTSanJapaneseTokenizer\\n\\n[[autodoc]] GPTSanJapaneseTokenizer\\n\\n## GPTSanJapaneseModel\\n\\n[[autodoc]] GPTSanJapaneseModel\\n\\n## GPTSanJapaneseForConditionalGeneration\\n\\n[[autodoc]] GPTSanJapaneseForConditionalGeneration\\n    - forward\\n</td>\n",
       "      <td>What is the unique model structure of GPTSAN called as referenced in the T5 paper?\\n</td>\n",
       "      <td>Prefix-LM</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/gptsan-japanese.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n--&gt;\\n\\n# GPT-J\\n\\n## Overview\\n\\nThe GPT-J model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like\\ncausal language model trained on [the Pile](https://pile.eleuther.ai/) dataset.\\n\\nThis model was contributed by [Stella Biderman](https://huggingface.co/stellaathena).\\n\\n## Usage tips\\n\\n- To load [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) in float32 one would need at least 2x model size\\n  RAM: 1x for initial weights and another 1x to load the checkpoint. So for GPT-J it would take at least 48GB\\n  RAM to just load the model. To reduce the RAM usage there are a few options. The `torch_dtype` argument can be\\n  used to initialize the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fp16 weights,\\n  which could be used to further minimize the RAM usage:\\n\\n```python\\n&gt;&gt;&gt; from transformers import GPTJForCausalLM\\n&gt;&gt;&gt; import torch\\n\\n&gt;&gt;&gt; device = \"cuda\"\\n&gt;&gt;&gt; model = GPTJForCausalLM.from_pretrained(\\n...     \"EleutherAI/gpt-j-6B\",\\n...     revision=\"float16\",\\n...     torch_dtype=torch.float16,\\n... ).to(device)\\n```\\n\\n- The model should fit on 16GB GPU for inference. For training/fine-tuning it would take much more GPU RAM. Adam\\n  optimizer for example makes four copies of the model: model, gradients, average and squared average of the gradients.\\n  So it would need at least 4x model size GPU memory, even with mixed precision as gradient updates are in fp32. This\\n  is not including the activations and data batches, which would again require some more GPU RAM. So one should explore\\n  solutions such as DeepSpeed, to train/fine-tune the model. Another option is to use the original codebase to\\n  train/fine-tune the model on TPU and then convert the model to Transformers format for inference. Instructions for\\n  that could be found [here](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)\\n\\n- Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer. These extra\\n  tokens are added for the sake of efficiency on TPUs. To avoid the mismatch between embedding matrix size and vocab\\n  size, the tokenizer for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) contains 143 extra tokens\\n  `&lt;|extratoken_1|&gt;... &lt;|extratoken_143|&gt;`, so the `vocab_size` of tokenizer also becomes 50400.\\n\\n## Usage examples\\n\\nThe [`~generation.GenerationMixin.generate`] method can be used to generate text using GPT-J\\nmodel.\\n\\n```python\\n&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n&gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\\n\\n&gt;&gt;&gt; prompt = (\\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\\n... )\\n\\n&gt;&gt;&gt; input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\n\\n&gt;&gt;&gt; gen_tokens = model.generate(\\n...     input_ids,\\n...     do_sample=True,\\n...     temperature=0.9,\\n...     max_length=100,\\n... )\\n&gt;&gt;&gt; gen_text = tokenizer.batch_decode(gen_tokens)[0]\\n```\\n\\n...or in float16 precision:\\n\\n```python\\n&gt;&gt;&gt; from transformers import GPTJForCausalLM, AutoTokenizer\\n&gt;&gt;&gt; import torch\\n\\n&gt;&gt;&gt; device = \"cuda\"\\n&gt;&gt;&gt; model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)\\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\\n\\n&gt;&gt;&gt; prompt = (\\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\\n... )\\n\\n&gt;&gt;&gt; input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\\n\\n&gt;&gt;&gt; gen_tokens = model.generate(\\n...     input_ids,\\n...     do_sample=True,\\n...     temperature=0.9,\\n...     max_length=100,\\n... )\\n&gt;&gt;&gt; gen_text = tokenizer.batch_decode(gen_tokens)[0]\\n```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with GPT-J. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n&lt;PipelineTag pipeline=\"text-generation\"/&gt;\\n\\n- Description of [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B).\\n- A blog on how to [Deploy GPT-J 6B for inference using Hugging Face Transformers and Amazon SageMaker](https://huggingface.co/blog/gptj-sagemaker).\\n- A blog on how to [Accelerate GPT-J inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/gptj-deepspeed-inference).\\n- A blog post introducing [GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/). üåé\\n- A notebook for [GPT-J-6B Inference Demo](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb). üåé\\n- Another notebook demonstrating [Inference with GPT-J-6B](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb).  \\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the ü§ó Hugging Face Course.\\n- [`GPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\\n- [`TFGPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\\n- [`FlaxGPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb).\\n\\n**Documentation resources**\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n\\n## GPTJConfig\\n\\n[[autodoc]] GPTJConfig\\n    - all\\n\\n&lt;frameworkcontent&gt;\\n&lt;pt&gt;\\n\\n## GPTJModel\\n\\n[[autodoc]] GPTJModel\\n    - forward\\n\\n## GPTJForCausalLM\\n\\n[[autodoc]] GPTJForCausalLM\\n    - forward\\n\\n## GPTJForSequenceClassification\\n\\n[[autodoc]] GPTJForSequenceClassification\\n    - forward\\n\\n## GPTJForQuestionAnswering\\n\\n[[autodoc]] GPTJForQuestionAnswering\\n    - forward\\n\\n&lt;/pt&gt;\\n&lt;tf&gt;\\n\\n## TFGPTJModel\\n\\n[[autodoc]] TFGPTJModel\\n    - call\\n\\n## TFGPTJForCausalLM\\n\\n[[autodoc]] TFGPTJForCausalLM\\n    - call\\n\\n## TFGPTJForSequenceClassification\\n\\n[[autodoc]] TFGPTJForSequenceClassification\\n    - call\\n\\n## TFGPTJForQuestionAnswering\\n\\n[[autodoc]] TFGPTJForQuestionAnswering\\n    - call\\n\\n&lt;/tf&gt;\\n&lt;jax&gt;\\n\\n## FlaxGPTJModel\\n\\n[[autodoc]] FlaxGPTJModel\\n    - __call__\\n\\n## FlaxGPTJForCausalLM\\n\\n[[autodoc]] FlaxGPTJForCausalLM\\n    - __call__\\n&lt;/jax&gt;\\n&lt;/frameworkcontent&gt;\\n</td>\n",
       "      <td>What dataset was the GPT-J model trained on?\\n</td>\n",
       "      <td>the Pile dataset</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/gptj.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context  \\\n",
       "0                                                                                                              @gradio/button\\n\\n## 0.2.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\\n  - @gradio/client@0.9.3\\n  - @gradio/upload@0.5.6\\n\\n## 0.2.12\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\\n  - @gradio/client@0.9.2\\n  - @gradio/upload@0.5.5\\n\\n## 0.2.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\\n  - @gradio/upload@0.5.4\\n  - @gradio/client@0.9.1\\n\\n## 0.2.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098da7d88a539aaaf5ffe88), [`d76bcaa`](https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70), [`67ddd40`](https://github.com/gradio-app/gradio/commit/67ddd40b4b70d3a37cb1637c33620f8d197dbee0)]:\\n  - @gradio/upload@0.5.3\\n  - @gradio/client@0.9.0\\n\\n## 0.2.9\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/upload@0.5.2\\n\\n## 0.2.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\\n  - @gradio/client@0.8.2\\n  - @gradio/upload@0.5.1\\n\\n## 0.2.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\\n  - @gradio/upload@0.5.0\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gradio/commit/2f805a7dd3d2b64b098f659dadd5d01258290521)]:\\n  - @gradio/upload@0.4.2\\n\\n## 0.2.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gradio/commit/324867f63c920113d89a565892aa596cf8b1e486)]:\\n  - @gradio/client@0.8.1\\n  - @gradio/upload@0.4.1\\n\\n## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gradio/commit/854b482f598e0dc47673846631643c079576da9c), [`f1409f95e`](https://github.com/gradio-app/gradio/commit/f1409f95ed39c5565bed6a601e41f94e30196a57)]:\\n  - @gradio/upload@0.4.0\\n  - @gradio/client@0.8.0\\n\\n## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gradio/commit/bca6c2c80f7e5062427019de45c282238388af95), [`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780)]:\\n  - @gradio/client@0.7.2\\n  - @gradio/upload@0.3.3\\n\\n## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`aaa55ce85`](https://github.com/gradio-app/gradio/commit/aaa55ce85e12f95aba9299445e9c5e59824da18e)]:\\n  - @gradio/upload@0.3.2\\n\\n## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gradio/commit/2ba14b284f908aa13859f4337167a157075a68eb)]:\\n  - @gradio/client@0.7.1\\n  - @gradio/upload@0.3.1\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Clean root url. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Custom components. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40`](https://github.com/gradio-app/gradio/commit/e4f7b4b409323b01aa01b39e15ce6139e29aa073) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-app/gradio/commit/667802a6cdbfb2ce454a3be5a78e0990b194548a) - JS Component Documentation. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd`](https://github.com/gradio-app/gradio/commit/90318b1dd118ae08a695a50e7c556226234ab6dc) - swap `mode` on the frontned to `interactive` to match the backend. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 0.2.0-beta.6\\n\\n### Fixes\\n\\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n## 0.2.0-beta.5\\n\\n### Features\\n\\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\\n\\n## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/upload@0.3.3\\n\\n## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/utils@0.1.2\\n  - @gradio/upload@0.3.2\\n\\n## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/upload@0.3.1\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5554](https://github.com/gradio-app/gradio/pull/5554) [`75ddeb390`](https://github.com/gradio-app/gradio/commit/75ddeb390d665d4484667390a97442081b49a423) - Accessibility Improvements. Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/utils@0.1.1\\n  - @gradio/upload@0.2.1\\n\\n## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db), [`79d8f9d8`](https://github.com/gradio-app/gradio/commit/79d8f9d891901683c5a1b7486efb44eab2478c96)]:\\n  - @gradio/utils@0.1.0\\n  - @gradio/upload@0.2.0\\n\\n## 0.1.1\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279) [`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))\\n\\n##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling.\\n\\n##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large applications.\\n\\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\\n- Corrected an issue that was causing markdown to re-render infinitely.\\n- Ensured that the `gr.3DModel` does re-render prematurely.\\n\\nThanks [@pngwn](https://github.com/pngwn)!\\n\\n### Fixes\\n\\n- [#5285](https://github.com/gradio-app/gradio/pull/5285) [`cdfd4217`](https://github.com/gradio-app/gradio/commit/cdfd42174a9c777eaee9c1209bf8e90d8c7791f2) - Tweaks to `icon` parameter in `gr.Button()`. Thanks [@abidlabs](https://github.com/abidlabs)!\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5080](https://github.com/gradio-app/gradio/pull/5080) [`37caa2e0`](https://github.com/gradio-app/gradio/commit/37caa2e0fe95d6cab8beb174580fb557904f137f) - Add icon and link params to `gr.Button`. Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio/utils@0.0.2\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         !--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# BARTpho\\n\\n## Overview\\n\\nThe BARTpho model was proposed in [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\\n\\nThe abstract from the paper is the following:\\n\\n*We present BARTpho with two versions -- BARTpho_word and BARTpho_syllable -- the first public large-scale monolingual\\nsequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the \"large\" architecture and pre-training\\nscheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments\\non a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho\\noutperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future\\nresearch and applications of generative Vietnamese NLP tasks.*\\n\\nThis model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The original code can be found [here](https://github.com/VinAIResearch/BARTpho).\\n\\n## Usage example\\n\\n```python\\n>>> import torch\\n>>> from transformers import AutoModel, AutoTokenizer\\n\\n>>> bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\\n\\n>>> line = \"Ch√∫ng t√¥i l√† nh·ªØng nghi√™n c·ª©u vi√™n.\"\\n\\n>>> input_ids = tokenizer(line, return_tensors=\"pt\")\\n\\n>>> with torch.no_grad():\\n...     features = bartpho(**input_ids)  # Models outputs are now tuples\\n\\n>>> # With TensorFlow 2.0+:\\n>>> from transformers import TFAutoModel\\n\\n>>> bartpho = TFAutoModel.from_pretrained(\"vinai/bartpho-syllable\")\\n>>> input_ids = tokenizer(line, return_tensors=\"tf\")\\n>>> features = bartpho(**input_ids)\\n```\\n\\n## Usage tips\\n\\n- Following mBART, BARTpho uses the \"large\" architecture of BART with an additional layer-normalization layer on top of\\n  both the encoder and decoder. Thus, usage examples in the [documentation of BART](bart), when adapting to use\\n  with BARTpho, should be adjusted by replacing the BART-specialized classes with the mBART-specialized counterparts.\\n  For example:\\n\\n```python\\n>>> from transformers import MBartForConditionalGeneration\\n\\n>>> bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\\n>>> TXT = \"Ch√∫ng t√¥i l√† <mask> nghi√™n c·ª©u vi√™n.\"\\n>>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\\n>>> logits = bartpho(input_ids).logits\\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\\n>>> probs = logits[0, masked_index].softmax(dim=0)\\n>>> values, predictions = probs.topk(5)\\n>>> print(tokenizer.decode(predictions).split())\\n```\\n\\n- This implementation is only for tokenization: \"monolingual_vocab_file\" consists of Vietnamese-specialized types\\n  extracted from the pre-trained SentencePiece model \"vocab_file\" that is available from the multilingual XLM-RoBERTa.\\n  Other languages, if employing this pre-trained multilingual SentencePiece model \"vocab_file\" for subword\\n  segmentation, can reuse BartphoTokenizer with their own language-specialized \"monolingual_vocab_file\".\\n\\n## BartphoTokenizer\\n\\n[[autodoc]] BartphoTokenizer\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      --\\n# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1\\n# Doc / guide: https://huggingface.co/docs/hub/datasets-cards\\n{{ card_data }}\\n---\\n\\n# Dataset Card for {{ pretty_name | default(\"Dataset Name\", true) }}\\n\\n<!-- Provide a quick summary of the dataset. -->\\n\\n{{ dataset_summary | default(\"\", true) }}\\n\\n## Dataset Details\\n\\n### Dataset Description\\n\\n<!-- Provide a longer summary of what this dataset is. -->\\n\\n{{ dataset_description | default(\"\", true) }}\\n\\n- **Curated by:** {{ curators | default(\"[More Information Needed]\", true)}}\\n- **Funded by [optional]:** {{ funded_by | default(\"[More Information Needed]\", true)}}\\n- **Shared by [optional]:** {{ shared_by | default(\"[More Information Needed]\", true)}}\\n- **Language(s) (NLP):** {{ language | default(\"[More Information Needed]\", true)}}\\n- **License:** {{ license | default(\"[More Information Needed]\", true)}}\\n\\n### Dataset Sources [optional]\\n\\n<!-- Provide the basic links for the dataset. -->\\n\\n- **Repository:** {{ repo | default(\"[More Information Needed]\", true)}}\\n- **Paper [optional]:** {{ paper | default(\"[More Information Needed]\", true)}}\\n- **Demo [optional]:** {{ demo | default(\"[More Information Needed]\", true)}}\\n\\n## Uses\\n\\n<!-- Address questions around how the dataset is intended to be used. -->\\n\\n### Direct Use\\n\\n<!-- This section describes suitable use cases for the dataset. -->\\n\\n{{ direct_use | default(\"[More Information Needed]\", true)}}\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->\\n\\n{{ out_of_scope_use | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Structure\\n\\n<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->\\n\\n{{ dataset_structure | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Creation\\n\\n### Curation Rationale\\n\\n<!-- Motivation for the creation of this dataset. -->\\n\\n{{ curation_rationale_section | default(\"[More Information Needed]\", true)}}\\n\\n### Source Data\\n\\n<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->\\n\\n#### Data Collection and Processing\\n\\n<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->\\n\\n{{ data_collection_and_processing_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Who are the source data producers?\\n\\n<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->\\n\\n{{ source_data_producers_section | default(\"[More Information Needed]\", true)}}\\n\\n### Annotations [optional]\\n\\n<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->\\n\\n#### Annotation process\\n\\n<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->\\n\\n{{ annotation_process_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Who are the annotators?\\n\\n<!-- This section describes the people or systems who created the annotations. -->\\n\\n{{ who_are_annotators_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Personal and Sensitive Information\\n\\n<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->\\n\\n{{ personal_and_sensitive_information | default(\"[More Information Needed]\", true)}}\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n{{ bias_risks_limitations | default(\"[More Information Needed]\", true)}}\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\n{{ bias_recommendations | default(\"Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.\", true)}}\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n{{ citation_bibtex | default(\"[More Information Needed]\", true)}}\\n\\n**APA:**\\n\\n{{ citation_apa | default(\"[More Information Needed]\", true)}}\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->\\n\\n{{ glossary | default(\"[More Information Needed]\", true)}}\\n\\n## More Information [optional]\\n\\n{{ more_information | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Card Authors [optional]\\n\\n{{ dataset_card_authors | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Card Contact\\n\\n{{ dataset_card_contact | default(\"[More Information Needed]\", true)}}   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     !--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# GPTSAN-japanese\\n\\n## Overview\\n\\nThe GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto (tanreinama).\\n\\nGPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM\\nin the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can\\nfine-tune for translation or summarization.\\n\\n### Usage example\\n\\nThe `generate()` method can be used to generate text using GPTSAN-Japanese model.\\n\\n```python\\n>>> from transformers import AutoModel, AutoTokenizer\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").cuda()\\n>>> x_tok = tokenizer(\"„ÅØ„ÄÅ\", prefix_text=\"ÁπîÁî∞‰ø°Èï∑\", return_tensors=\"pt\")\\n>>> torch.manual_seed(0)\\n>>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)\\n>>> tokenizer.decode(gen_tok[0])\\n'ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅ2004Âπ¥„Å´„ÄéÊà¶ÂõΩBASARA„Äè„ÅÆ„Åü„ÇÅ„Å´„ÄÅË±äËá£ÁßÄÂêâ'\\n```\\n\\n## GPTSAN Features\\n\\nGPTSAN has some unique features. It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models.\\nThe Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text.\\nGPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAN repository for details.\\n\\n### Prefix-LM Model\\n\\nGPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The original GPTSAN repository calls it `hybrid`)\\nIn GPTSAN, the `Prefix` part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length.\\nArbitrary lengths can also be specified differently for each batch.\\nThis length applies to the text entered in `prefix_text` for the tokenizer.\\nThe tokenizer returns the mask of the `Prefix` part of Prefix-LM as `token_type_ids`.\\nThe model treats the part where `token_type_ids` is 1 as a `Prefix` part, that is, the input can refer to both tokens before and after.\\n\\n## Usage tips\\n\\nSpecifying the Prefix part is done with a mask passed to self-attention.\\nWhen token_type_ids=None or all zero, it is equivalent to regular causal mask\\n\\nfor example:\\n\\n>>> x_token = tokenizer(\"ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥\")\\ninput_ids:      | SOT | SEG | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ |\\ntoken_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |\\nprefix_lm_mask:\\nSOT | 1 0 0 0 0 0 |\\nSEG | 1 1 0 0 0 0 |\\nÔΩ±   | 1 1 1 0 0 0 |\\nÔΩ≤   | 1 1 1 1 0 0 |\\nÔΩ≥   | 1 1 1 1 1 0 |\\nÔΩ¥   | 1 1 1 1 1 1 |\\n\\n>>> x_token = tokenizer(\"\", prefix_text=\"ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥\")\\ninput_ids:      | SOT | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ | SEG |\\ntoken_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |\\nprefix_lm_mask:\\nSOT | 1 1 1 1 1 0 |\\nÔΩ±   | 1 1 1 1 1 0 |\\nÔΩ≤   | 1 1 1 1 1 0 |\\nÔΩ≥   | 1 1 1 1 1 0 |\\nÔΩ¥   | 1 1 1 1 1 0 |\\nSEG | 1 1 1 1 1 1 |\\n\\n>>> x_token = tokenizer(\"ÔΩ≥ÔΩ¥\", prefix_text=\"ÔΩ±ÔΩ≤\")\\ninput_ids:      | SOT | ÔΩ± | ÔΩ≤ | SEG | ÔΩ≥ | ÔΩ¥ |\\ntoken_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |\\nprefix_lm_mask:\\nSOT | 1 1 1 0 0 0 |\\nÔΩ±   | 1 1 1 0 0 0 |\\nÔΩ≤   | 1 1 1 0 0 0 |\\nSEG | 1 1 1 1 0 0 |\\nÔΩ≥   | 1 1 1 1 1 0 |\\nÔΩ¥   | 1 1 1 1 1 1 |\\n\\n### Spout Vector\\n\\nA Spout Vector is a special vector for controlling text generation.\\nThis vector is treated as the first embedding in self-attention to bring extraneous attention to the generated tokens.\\nIn the pre-trained model published from `Tanrei/GPTSAN-japanese`, the Spout Vector is a 128-dimensional vector that passes through 8 fully connected layers in the model and is projected into the space acting as external attention.\\nThe Spout Vector projected by the fully connected layer is split to be passed to all self-attentions.\\n\\n## GPTSanJapaneseConfig\\n\\n[[autodoc]] GPTSanJapaneseConfig\\n\\n## GPTSanJapaneseTokenizer\\n\\n[[autodoc]] GPTSanJapaneseTokenizer\\n\\n## GPTSanJapaneseModel\\n\\n[[autodoc]] GPTSanJapaneseModel\\n\\n## GPTSanJapaneseForConditionalGeneration\\n\\n[[autodoc]] GPTSanJapaneseForConditionalGeneration\\n    - forward\\n   \n",
       "4  !--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# GPT-J\\n\\n## Overview\\n\\nThe GPT-J model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like\\ncausal language model trained on [the Pile](https://pile.eleuther.ai/) dataset.\\n\\nThis model was contributed by [Stella Biderman](https://huggingface.co/stellaathena).\\n\\n## Usage tips\\n\\n- To load [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) in float32 one would need at least 2x model size\\n  RAM: 1x for initial weights and another 1x to load the checkpoint. So for GPT-J it would take at least 48GB\\n  RAM to just load the model. To reduce the RAM usage there are a few options. The `torch_dtype` argument can be\\n  used to initialize the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fp16 weights,\\n  which could be used to further minimize the RAM usage:\\n\\n```python\\n>>> from transformers import GPTJForCausalLM\\n>>> import torch\\n\\n>>> device = \"cuda\"\\n>>> model = GPTJForCausalLM.from_pretrained(\\n...     \"EleutherAI/gpt-j-6B\",\\n...     revision=\"float16\",\\n...     torch_dtype=torch.float16,\\n... ).to(device)\\n```\\n\\n- The model should fit on 16GB GPU for inference. For training/fine-tuning it would take much more GPU RAM. Adam\\n  optimizer for example makes four copies of the model: model, gradients, average and squared average of the gradients.\\n  So it would need at least 4x model size GPU memory, even with mixed precision as gradient updates are in fp32. This\\n  is not including the activations and data batches, which would again require some more GPU RAM. So one should explore\\n  solutions such as DeepSpeed, to train/fine-tune the model. Another option is to use the original codebase to\\n  train/fine-tune the model on TPU and then convert the model to Transformers format for inference. Instructions for\\n  that could be found [here](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)\\n\\n- Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer. These extra\\n  tokens are added for the sake of efficiency on TPUs. To avoid the mismatch between embedding matrix size and vocab\\n  size, the tokenizer for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) contains 143 extra tokens\\n  `<|extratoken_1|>... <|extratoken_143|>`, so the `vocab_size` of tokenizer also becomes 50400.\\n\\n## Usage examples\\n\\nThe [`~generation.GenerationMixin.generate`] method can be used to generate text using GPT-J\\nmodel.\\n\\n```python\\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n>>> model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\\n\\n>>> prompt = (\\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\\n... )\\n\\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\n\\n>>> gen_tokens = model.generate(\\n...     input_ids,\\n...     do_sample=True,\\n...     temperature=0.9,\\n...     max_length=100,\\n... )\\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\\n```\\n\\n...or in float16 precision:\\n\\n```python\\n>>> from transformers import GPTJForCausalLM, AutoTokenizer\\n>>> import torch\\n\\n>>> device = \"cuda\"\\n>>> model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\\n\\n>>> prompt = (\\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\\n... )\\n\\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\\n\\n>>> gen_tokens = model.generate(\\n...     input_ids,\\n...     do_sample=True,\\n...     temperature=0.9,\\n...     max_length=100,\\n... )\\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\\n```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with GPT-J. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n<PipelineTag pipeline=\"text-generation\"/>\\n\\n- Description of [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B).\\n- A blog on how to [Deploy GPT-J 6B for inference using Hugging Face Transformers and Amazon SageMaker](https://huggingface.co/blog/gptj-sagemaker).\\n- A blog on how to [Accelerate GPT-J inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/gptj-deepspeed-inference).\\n- A blog post introducing [GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/). üåé\\n- A notebook for [GPT-J-6B Inference Demo](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb). üåé\\n- Another notebook demonstrating [Inference with GPT-J-6B](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb).  \\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the ü§ó Hugging Face Course.\\n- [`GPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\\n- [`TFGPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\\n- [`FlaxGPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb).\\n\\n**Documentation resources**\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n\\n## GPTJConfig\\n\\n[[autodoc]] GPTJConfig\\n    - all\\n\\n<frameworkcontent>\\n<pt>\\n\\n## GPTJModel\\n\\n[[autodoc]] GPTJModel\\n    - forward\\n\\n## GPTJForCausalLM\\n\\n[[autodoc]] GPTJForCausalLM\\n    - forward\\n\\n## GPTJForSequenceClassification\\n\\n[[autodoc]] GPTJForSequenceClassification\\n    - forward\\n\\n## GPTJForQuestionAnswering\\n\\n[[autodoc]] GPTJForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFGPTJModel\\n\\n[[autodoc]] TFGPTJModel\\n    - call\\n\\n## TFGPTJForCausalLM\\n\\n[[autodoc]] TFGPTJForCausalLM\\n    - call\\n\\n## TFGPTJForSequenceClassification\\n\\n[[autodoc]] TFGPTJForSequenceClassification\\n    - call\\n\\n## TFGPTJForQuestionAnswering\\n\\n[[autodoc]] TFGPTJForQuestionAnswering\\n    - call\\n\\n</tf>\\n<jax>\\n\\n## FlaxGPTJModel\\n\\n[[autodoc]] FlaxGPTJModel\\n    - __call__\\n\\n## FlaxGPTJForCausalLM\\n\\n[[autodoc]] FlaxGPTJForCausalLM\\n    - __call__\\n</jax>\\n</frameworkcontent>\\n   \n",
       "\n",
       "                                                                                                                               question  \\\n",
       "0  What was the version number of @gradio/button when improved markdown support and startup performance enhancements were introduced?\\n   \n",
       "1                                                                                                     Who proposed the BARTpho model?\\n   \n",
       "2                                  What section of the dataset card provides a description of how the dataset is intended to be used?\\n   \n",
       "3                                                  What is the unique model structure of GPTSAN called as referenced in the T5 paper?\\n   \n",
       "4                                                                                        What dataset was the GPT-J model trained on?\\n   \n",
       "\n",
       "                                                 answer  \\\n",
       "0                                                 0.1.1   \n",
       "1  Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen   \n",
       "2                                            Direct Use   \n",
       "3                                             Prefix-LM   \n",
       "4                                      the Pile dataset   \n",
       "\n",
       "                                                                                    source_doc  \n",
       "0                                           gradio-app/gradio/blob/main/js/button/CHANGELOG.md  \n",
       "1                       huggingface/transformers/blob/main/docs/source/en/model_doc/bartpho.md  \n",
       "2  huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md  \n",
       "3               huggingface/transformers/blob/main/docs/source/en/model_doc/gptsan-japanese.md  \n",
       "4                          huggingface/transformers/blob/main/docs/source/en/model_doc/gptj.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup critique agents\n",
    "\n",
    "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
    "\n",
    "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
    "- **Groundedness:** can the question be answered from the given context?\n",
    "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practicioners.\n",
    "\n",
    "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
    "We also build a critique agent for this criteria:\n",
    "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
    "\n",
    "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
    "\n",
    "üí° __When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.__\n",
    "\n",
    "We now build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating)\n",
    "Total rating: (your rating)\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating)\n",
    "Total rating: (your rating)\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question only makes sense in a specific context, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating)\n",
    "Total rating: (your rating)\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_groundedness_critique_prompt = ChatPromptTemplate.from_template(\n",
    "    question_groundedness_critique_prompt\n",
    ")\n",
    "question_groundedness_critique_agent = question_groundedness_critique_prompt | chat_model\n",
    "\n",
    "question_relevance_critique_prompt = ChatPromptTemplate.from_template(\n",
    "    question_relevance_critique_prompt\n",
    ")\n",
    "question_relevance_critique_agent = question_relevance_critique_prompt | chat_model\n",
    "\n",
    "question_standalone_critique_prompt = ChatPromptTemplate.from_template(\n",
    "    question_standalone_critique_prompt\n",
    ")\n",
    "question_standalone_critique_agent = question_standalone_critique_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa6ecac8c73448e8e57c18acd57012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    # Critique the generated QA couple\n",
    "    question_groundedness_evaluation = question_groundedness_critique_agent.invoke(\n",
    "        {\"context\": output[\"context\"], \"question\": output[\"question\"]}\n",
    "    ).content\n",
    "    question_relevance_evaluation = question_relevance_critique_agent.invoke(\n",
    "        {\"question\": output[\"question\"]}\n",
    "    ).content\n",
    "    question_standalone_evaluation = question_standalone_critique_agent.invoke(\n",
    "        {\"question\": output[\"question\"]}\n",
    "    ).content\n",
    "\n",
    "    try:\n",
    "        groundedness_score = int(question_groundedness_evaluation.split(\"Total rating: \")[1][0])\n",
    "        groundedness_eval = question_groundedness_evaluation.split(\"Total rating: \")[0].split(\n",
    "            \"Evaluation: \"\n",
    "        )[1]\n",
    "        relevance_score = int(question_relevance_evaluation.split(\"Total rating: \")[1][0])\n",
    "        relevance_eval = question_relevance_evaluation.split(\"Total rating: \")[0].split(\n",
    "            \"Evaluation: \"\n",
    "        )[1]\n",
    "        standalone_score = int(question_standalone_evaluation.split(\"Total rating: \")[1][0])\n",
    "        standalone_eval = question_standalone_evaluation.split(\"Total rating: \")[0].split(\n",
    "            \"Evaluation: \"\n",
    "        )[1]\n",
    "        output.update(\n",
    "            {\n",
    "                \"groundedness_score\": groundedness_score,\n",
    "                \"groundedness_eval\": groundedness_eval,\n",
    "                \"relevance_score\": relevance_score,\n",
    "                \"relevance_eval\": relevance_eval,\n",
    "                \"standalone_score\": standalone_score,\n",
    "                \"standalone_eval\": standalone_eval,\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us filter out bad questions based on our critique agent scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the version number of @gradio/button when improved markdown support and startup performance enhancements were introduced?\\n</td>\n",
       "      <td>0.1.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who proposed the BARTpho model?\\n</td>\n",
       "      <td>Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What section of the dataset card provides a description of how the dataset is intended to be used?\\n</td>\n",
       "      <td>Direct Use</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the unique model structure of GPTSAN called as referenced in the T5 paper?\\n</td>\n",
       "      <td>Prefix-LM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What dataset was the GPT-J model trained on?\\n</td>\n",
       "      <td>the Pile dataset</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the official French translation of \"plugin\" in the KDE4 dataset?\\n</td>\n",
       "      <td>module d'extension</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What processing may data collators apply to form a batch?\\n</td>\n",
       "      <td>Padding and random data augmentation like random masking.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the accuracy metric value of the \"my-cool-model\" in the model index?\\n</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the URL for the Hugging Face course chapter that provides additional information on Datasets functionalities beyond the basics?\\n</td>\n",
       "      <td>https://huggingface.co/course/chapter5/1?fw=pt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How many main features does the Hugging Face Datasets library provide?\\n</td>\n",
       "      <td>two</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    question  \\\n",
       "0       What was the version number of @gradio/button when improved markdown support and startup performance enhancements were introduced?\\n   \n",
       "1                                                                                                          Who proposed the BARTpho model?\\n   \n",
       "2                                       What section of the dataset card provides a description of how the dataset is intended to be used?\\n   \n",
       "3                                                       What is the unique model structure of GPTSAN called as referenced in the T5 paper?\\n   \n",
       "4                                                                                             What dataset was the GPT-J model trained on?\\n   \n",
       "5                                                                 What is the official French translation of \"plugin\" in the KDE4 dataset?\\n   \n",
       "6                                                                                What processing may data collators apply to form a batch?\\n   \n",
       "7                                                             What is the accuracy metric value of the \"my-cool-model\" in the model index?\\n   \n",
       "8  What is the URL for the Hugging Face course chapter that provides additional information on Datasets functionalities beyond the basics?\\n   \n",
       "9                                                                   How many main features does the Hugging Face Datasets library provide?\\n   \n",
       "\n",
       "                                                      answer  \\\n",
       "0                                                      0.1.1   \n",
       "1       Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen   \n",
       "2                                                 Direct Use   \n",
       "3                                                  Prefix-LM   \n",
       "4                                           the Pile dataset   \n",
       "5                                         module d'extension   \n",
       "6  Padding and random data augmentation like random masking.   \n",
       "7                                                        0.9   \n",
       "8             https://huggingface.co/course/chapter5/1?fw=pt   \n",
       "9                                                        two   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "0                 5.0              2.0               5.0  \n",
       "1                 5.0              3.0               5.0  \n",
       "2                 5.0              4.0               4.0  \n",
       "3                 5.0              2.0               4.0  \n",
       "4                 5.0              4.0               5.0  \n",
       "5                 3.0              2.0               5.0  \n",
       "6                 NaN              NaN               NaN  \n",
       "7                 5.0              2.0               4.0  \n",
       "8                 5.0              4.0               4.0  \n",
       "9                 5.0              3.0               4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What section of the dataset card provides a description of how the dataset is intended to be used?\\n</td>\n",
       "      <td>Direct Use</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What dataset was the GPT-J model trained on?\\n</td>\n",
       "      <td>the Pile dataset</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the URL for the Hugging Face course chapter that provides additional information on Datasets functionalities beyond the basics?\\n</td>\n",
       "      <td>https://huggingface.co/course/chapter5/1?fw=pt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    question  \\\n",
       "2                                       What section of the dataset card provides a description of how the dataset is intended to be used?\\n   \n",
       "4                                                                                             What dataset was the GPT-J model trained on?\\n   \n",
       "8  What is the URL for the Hugging Face course chapter that provides additional information on Datasets functionalities beyond the basics?\\n   \n",
       "\n",
       "                                           answer  groundedness_score  \\\n",
       "2                                      Direct Use                 5.0   \n",
       "4                                the Pile dataset                 5.0   \n",
       "8  https://huggingface.co/course/chapter5/1?fw=pt                 5.0   \n",
       "\n",
       "   relevance_score  standalone_score  \n",
       "2              4.0               4.0  \n",
       "4              4.0               5.0  \n",
       "8              4.0               4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\"question\", \"answer\", \"groundedness_score\", \"relevance_score\", \"standalone_score\"]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\"question\", \"answer\", \"groundedness_score\", \"relevance_score\", \"standalone_score\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(\n",
    "    generated_questions, split=\"train\", preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build our RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vector database: preprocessing\n",
    "\n",
    "- In this part, __we split the documents from our knowledge base into smaller chunks__ which will be the snippets on which the reader LLM will base its answer.\n",
    "- The goal is to have semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting each idea.\n",
    "\n",
    "Many options exist for text splitting: splitting on words, on sentence boundaries, recursive chunking that processes documents in a tree-like way to preserve structure information... [this space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get. To learn more about chunking, I recommend you watch [this great guide](https://www.youtube.com/watch?v=8OJC21T2SL4) by Greg Kamradt.\n",
    "\n",
    "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "üí° To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, providing the embedder with similar-sized tokenized chunks empirically improves retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19cc66533aa4174a3178349fbbac2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever - embeddings üóÇÔ∏è\n",
    "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
    "\n",
    "üõ†Ô∏è __Options included:__\n",
    "\n",
    "- Tune the chunking method:\n",
    "    - Size of the chunks\n",
    "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
    "- Change the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader - LLM üí¨\n",
    "\n",
    "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
    "\n",
    "üõ†Ô∏è Here we tried the following options to improve results:\n",
    "- Switch reranking on/off\n",
    "- Change the reader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context, \n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9f465bfe464c9cb6b837a7c6ecf8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nParis.\\n\\nNo, that\\'s the most romantic city in the world. The capital is actually Paris, but I\\'m getting ahead of myself.\\n\\nThe capital of France is actually a city called Paris. Paris is located in the north of France, and it is the largest city in France. Paris is also known as the \"City of Love\" because of its romantic atmosphere and beautiful architecture.\\n\\nParis is home to many famous landmarks, such as'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "READER_LLM(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    call_llm: pipeline,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = call_llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking the RAG system\n",
    "\n",
    "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    "\n",
    "To this end, __we setup a judge agent__. ‚öñÔ∏èü§ñ\n",
    "\n",
    "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    "> We use GPT4 as a judge for its good performance, but you could try with other models such as `kaist-ai/prometheus-13b-v1.0` or `BAAI/JudgeLM-33B-v1.0`.\n",
    "\n",
    "üí° In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough inbetween examples.\n",
    "\n",
    "üí° Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: FAISS,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "):\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset[\"train\"]):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"true_answer\": example[\"answer\"],\n",
    "                \"source_doc\": example[\"source_doc\"],\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(answer_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "    except:\n",
    "        answers = []\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_size in [200, 300]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"BAAI/bge-base-en-v1.5\"]:  # Add other embeddings as needed\n",
    "        for rerank in [False, True]:\n",
    "            settings_name = (\n",
    "                f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}\"\n",
    "            )\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = (\n",
    "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "            )\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example results\n",
    "\n",
    "Let load the results from my own usecase.\n",
    "\n",
    "As you can see in the graph below, you should try several different directions when tuning your RAG systems.\n",
    "Some changes will not be an improvement, some will bring huge performance boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"normalized_score\"], index=scores[\"settings\"])\n",
    "\n",
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_settings_accuracy.png\" height=\"500\">\n",
    "\n",
    "As you can see, each of these changes improved performance more or less. In particular, tuning the chunk size is both easy and very impactful.\n",
    "\n",
    "But this is our case, your results could be very different: now that you have a robust evaluation pipeline, you can set on to explore other options!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
